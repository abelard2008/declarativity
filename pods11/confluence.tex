\section{Confluence}
\label{sec:confluence}
Although there is much discussion among software developers about distributed ``consistency,'' the term is used in a number of different ways.  One common usage refers narrowly to the consistency of data replicas across machines; this is a topic we cover in Section~\ref{sec:consistency}.  More generally, we might ask for a consistent interpretation of the logical consequences of a program on a given input, independent of nondeterminism arising from distribution. 
This corresponds to the Church-Rosser notion of confluence, which we adapt to our setting in this section.  Note that confluence implies another useful notion of consistency: that distributed evaluation is equivalent to sequential evaluation.  When this property can be proven, complications of parallelism can be safely ignored by the programmer.  In the last PODS conference, the ``CALM'' conjecture was presented to link this notion of consistency to monotonicity properties~\cite{declarative-imperative}; we formalize that connection here. 

Recall that nondeterminism in \lang only arises due to \dedalus{choice} in asynchronous rules, 
%which only occurs in asynchronous rules to model network nondeterminism.  
which model temporal nondeterminism in unreliable networks.
Model-theoretically, a nondeterministic result is manifest in multiple ultimate models.

\begin{definition}
A \lang program is {\em confluent} (has a deterministic output) if, for every EDB, it has a unique ultimate model.  A program that is not confluent is {\em diffluent}.
\end{definition}

Unfortunately, confluence is an undecidable property of \lang programs:

\begin{lemma}
\label{lem:confluence-undecidable}
Confluence of a \lang program is undecidable.
\end{lemma}
\begin{proof}
We reduce the undecidable problem of determining whether a two-counter machine accepts any input to testing confluence of a \lang program.  See appendix for the full proof.
\end{proof}

Given undecidability, we will first identify classes of programs that have a unique ultimate model.  However, in general, a \lang program may encode many ultimate models.  We will show that many \lang programs have a ``natural'' ultimate model that is preferable to all other ultimate models, and corresponds to intuition.  We show how to induce this ultimate model by rewriting the program.

%Fortunately, as we will show, there is a rich class of programs that can be statically detected as confluent.  Furthermore, many other programs have a ``natural'' ultimate model
%that corresponds to intuition; we show how to induce this ultimate model by augmenting the program.
%We will first consider programs that we can statically check as having a unique ultimate model.
%However, a \lang program may encode many ultimate models, so there is a question as to which ultimate model is preferable.
%\lang programs without negation are trivially confluent, as they are certain to have
%only one stable model.  The same is true of both negation-free and temporally stratified \lang programs that do not use the  {\em choice} construct.
%Cast in operational terms this is
%unsurprising: programs with no sources of nondeterminism have deterministic executions.

\eat{
\begin{example}
\label{ex:nonconfluent}
A diffluent \lang program.

\begin{Dedalus}
p(x)@async <- p_edb(X);
q(X)@next <- p(X), !q(_);
q(X)@next <- q(X);
\end{Dedalus}
\end{example}

In the above program, assume the EDB contains facts \dedalus{p\_edb(1)} and \dedalus{p\_edb(2)}.  If the nondeterministic choice of \dedalus{async} chooses an earlier timestamp for \dedalus{p(1)} than \dedalus{p(2)}, then the ultimate model will consist of the single fact \dedalus{q(1)}.  Similarly, if \dedalus{p(2)} is assigned an earlier timestamp than \dedalus{p(1)}, then the ultimate model will consist of the single fact \dedalus{q(2)}.  If \dedalus{p(1)} and \dedalus{p(2)} are assigned the same timestamp, then the ultimate model will consist of the two facts \dedalus{q(1)} and \dedalus{q(2)}.  Since the program does not have a unique ultimate model for this EDB, it is diffluent.
}

\subsection{Sources of Diffluence}

%Any \lang programs that use asynchronous rules have an infinite
%number of stable models, but as we will see, in many cases these correspond to a
%unique ultimate model.
%In general, however, \lang programs are not necessarily confluent even in the absence of negation.
First, we provide some intuition for the sources of diffluence in \lang programs.  
%In Example~\ref{ex:nonconfluent}, we saw a \lang program that was difluent because the ultimate model consisted of the first batch of messages in the \dedalus{p} predicate.

%\begin{definition}
%If removing negation 
%\jmh{Is that notion well-defined?  Just drop the exclamation points?  Be clear since this is a %surprising, semantics-changing thing to do.} 
%rom a \lang program results in fewer ultimate models, we say the program has {\em %nondeterministic world closing}.
%\end{definition}

In Datalog programs with negation, the standard stratified interpretation assumes that predicates in a lower stratum are evaluated to fixpoint and ``closed'' before their contents are considered in higher strata.  In \lang, even with a stratified interpretation, the contents of a negated predicate at a given timestep $t$ may be a subset of its contents at $t+1$.  In essence, in \lang the closed-world assumption may be ``temporarily'' applied to sets whose contents are still evolving over time. The evolution of such sets and their consequences can lead to nondeterministic ultimate models.

Consider a \lang program $P'$ derived from another \lang program $P$ by 
%\paa{not dropping?}
dropping all negated subgoals 
%to positive subgoals (i.e., removing the $!$ symbol from 
%$p$) 
and removing any occurrences of the {\em count} aggregate function (i.e., replacing 
a clause like \dedalus{count<A>} with \dedalus{A}).  If $P'$ has fewer ultimate models than $P$, we say that $P$ has {\em nondeterministic
world closing}.  Nondeterministic world closing occurs when programs apply the closed world assumption to potentially incomplete sets.

\begin{example}
\label{ex:nonconfluent2}
A \lang program whose diffluence is caused by nondeterministic world closing.

\begin{Dedalus}
q()@async <- q_edb();
r()@async <- r_edb();
p() <- q(), !r();
p()@next <- p();
q()@next <- q();
r()@next <- r();
\end{Dedalus}

Assume an EDB of \dedalus{q\_edb(), r\_edb()}.  Any stable model where \dedalus{q()} has a lower timestamp than \dedalus{r()} yields an ultimate model containing \dedalus{p()}.  Otherwise, the ultimate model does not contain \dedalus{p()}.   We will see later how, using intuition from stratification to deterministically close worlds, we can induce confluence with an ultimate model that corresponds to intuition.
\end{example}


%\begin{definition}
%If a negation-free \lang program is diffluent, we say the program has {\em nondeterministic %coincidence}.

%\paa{isn't this backwards?  you are really saying, if a neg-free program is diffluent, it must be
%the case that it has nondeterministic coincidence, because we're pretty sure (I think I mostly %proved it...) that's the only other thing that can cause diffluence... but for this definition,
%don't we just want to define ``unguarded asynchrony'' here?}
%\jmh{I agree that this seems like a lemma, not a definition.}
%\end{definition}


Nondeterministic world closing is not a necessary condition for diffluence. 
A \lang program may have multiple ultimate models if it is possible for a join 
to succeed in one execution (stable model) and fail in another.  We say that such 
programs have {\em nondeterministic coincidence}.

\begin{example}
\label{ex:diffluent-noneg}
A \lang program whose diffluence is caused by nondeterministic coincidence.

\begin{Dedalus}
q()@async <- q_edb();
r()@async <- r_edb();
p() <- q(), r();
p()@next <- p();
q()@next <- q();
\end{Dedalus}

%Example~\ref{ex:diffluent-noneg} may be obtained from Example~\ref{ex:nonconfluent2} by dropping negation in the third rule, and dropping the sixth rule. \jmh{Why do we care?}  
As before, assume an EDB of \dedalus{q\_edb(), r\_edb()}.  Any stable model where the timestamp of \dedalus{q()} is less than or equal to the timestamp of \dedalus{r()} yields an ultimate model containing \dedalus{p()}.  Otherwise, the ultimate model does not contain \dedalus{p()}.  However, this example becomes confluent if we admit the last rule of Example~\ref{ex:nonconfluent2}: \dedalus{r()@next <- r();}.
\end{example}

%The above example illustrates that even without negation, a \lang program may induce multiple ultimate models.

We will see that ruling out both nondeterministic world closing and nondeterministic coincidence yields confluence.

\begin{definition}
A \lang program is {\em negation-free} if the \dedalus{!} symbol does not appear in the program.
\end{definition}

\begin{definition}

A fact is {\em persistent} if it is henceforth true 
%(i.e., if $a \Rightarrow \Box a$) \jmh{cheating to borrow symbols from elsewhere.  use \lang notation or terminology.}
(i.e., if it is true at time \dedalus{t}, then it is true at all $\dedalus{s} > \dedalus{t}$.)
\end{definition}

Persistent facts are the only facts that appear in a program's ultimate models.
Persistence is a semantic notion, but in certain cases it is detectable from the program's 
syntax alone.  A rule of the form \dedalus{p(X)@next <- p(X);} clearly establishes any fact
in \dedalus{p} as a persistent fact for any EDB and execution in which the fact
is ever true. 
We call a predicate referenced by such a rule
a {\em simply persisted} predicate.  
%All EDB predicates are both simply persisted and {\em a priori} true.  
We call a predicate that is both simply persisted and  {\em a priori} true (i.e., asserted
at time 0) a {\em ground truth}.  Ground truths are analogous to the EDB in Datalog programs,
in which time is not explicitly modeled.
%\jmh{Sorta.  Though this makes non-time-0 EDB facts different, and I'm not sure why they're different from EDB in datalog.}
%overriding, because time 0 implies we dont' need to worry about it not being complete until some later time. we can always build a queue or some shit over time 0 edb facts and dequeue them over time. -wrm

%\jmh{This last sentence surprised me.  Are you saying that is implicit in the language, or you will require simple persistence rules over the edb?  I though the edb facts were only true at time 1.}

\begin{definition}
A \lang program has {\em guarded asynchrony} if all predicates 
%\jmh{we have async rules, not async preds} 
appearing in the heads of \dedalus{async} rules
are simply persisted.
\end{definition}



\begin{lemma}
\label{lem:guarding}
A negation-free \lang program with guarded asynchrony is confluent.
\end{lemma}
\begin{proof}
%Sketch: only possible way two ultimate models are different is if two facts (or predicates, e.g. 
%``!'' on facts) join in one trace, but not the other.  If the program has guarded asynchrony, then 
%it is impossible for a join to succeed in one trace and not another.

%\wrm{yeah, this is right, need to define terms though-- define ``ground atom'' as ``first time %something in the ultimate model is true before it is eventually always true'', change ``join'' to %``unification'', change ``eventually always succeed'' to ``eventually always satisfied'' or %something, define ``modulo time'' define ``identical tuple''.  i'll let you fix this. you can delete %my sketch above too}

%\paa{I think I addressed everything but "change join to unify," which I don't agree with}
Towards a proof by contradiction, consider a negation-free \lang program that 
induces more than one ultimate model.  There must be a ground atom $a$ for a predicate $p$
that is true in one but
not in another model, and $a$ must be persistent, or it would not be
in the ultimate model.  Consider a derivation of $a$: a finite tree of applications of
implication whose leaves are EDB atoms.  If none of the implications involve a nondeterministic
choice of timestamp via an {\em async} rule, then certainly 
$a$ occurs in all stable models of the program,
%there is only one stable model of the program
%\jmh{I don't buy this; we're scoped down to atom $a$ here and the rules that feed into it}, 
so there must be at least one {\em async} rule in $a$'s derivation, contributing an atom
$r$.  
If $p$ is derived directly from 
$r$ via a series of derivation steps without any joins, then every stable
model $m$ will have a tuple $a_m$ in $p$ that differs from $a$ only in its timestamp, 
and hence correspond to the same ultimate model.
Therefore, $a$ must have at least one join step (i.e., a rule with at least two subgoals)
in its derivation following $r$,
which succeeded in this stable model (producing $a$), but did not succeed in another.  
%Guarded joins always
%eventually succeed, \jmh{don't you need to define ``join success'' and prove that guarded asynchrony does what you say?} what it and 

Consider such  a join rule.  One of its subgoals $s$ corresponds to a derivation that depends
on the async-derived atom $r$.  If any predicate between $s$ and $r$ is simply persisted and 
the program is negation-free,
then $s$ is persistent.  Regardless of the nondeterministic choice of timestamp for $r$, there
is some timestamp $V_m$ in every stable model $m$ of the program such that $s$ is true
for all $W > V$.  Hence in all stable models, $s$ is ``eventually'' true.  The same argument holds
for all subgoals of the rule under consideration, and hence guarded asynchrony implies that
all joins will eventually succeed.
Hence $a$ must exist in all ultimate models.
\end{proof}


If either qualification is false, problems can result, as we have previously illustrated.


\subsection{Monotonic Properties}

% \paa{add some nice text here transitioning the discussion into distributed systems etc}
% 
In practice, many natural distributed programs are not negation-free~\cite{cidr11}, and the conditions of Lemma~\ref{lem:guarding} do not hold.  Still, there are cases where these programs contain monotonicities that can be used to guarantee confluence.  In this section we broaden our notion of monotonicity, enabling us to verify the confluence of a larger class of programs.

We begin by considering the following example of a Dedalus program with negation, which we can argue is both monotonic and confluent:
\begin{example}
\hspace{1in} %gets the code on a separate line

%Example of a logically monotonic Dedalus program:\\
\begin{Dedalus}
response(X)@async <- response_edb(X);
node(X)@next <- node(X);
response(X)@next <- response(X);
all_responded() <- !not_responded(_);
not_responded(X) <- node(X), !response(X);
all_responded()@next <- all_responded();
\end{Dedalus}
\end{example}

%\jmh{suddenly we're getting to the nub of our application domain.  needs more prose to set this up; possibly even a section break}
%The above program is very nearly monotonic \jmh{whatever that means}. 

If we assume that 
%the contents of \texttt{node}
%are fixed (e.g., it is EDB and given \emph{a priori}), 
\texttt{node} is a ground truth,
there is only one possible nonempty ultimate model, which contains \dedalus{all\_responded()}.
This fact is implied by the predicate \dedalus{!not\_responded(\_)}, which 
%\jmh{not sure what ``which'' refers to} 
is monotonic because it is {\em positive} (i.e. the negation depth is even). 
Note that the program is also confluent, because for any EDB, there is a unique ultimate model: either every node has an associated response, leading to a model with \dedalus{all\_responded()}, or there exists some node without an associated response, leading to an empty ultimate model.

Note however that if the set \texttt{node} is not fixed, the program is not monotonic, because 
\texttt{all\_responded} is defined with odd negation depth over \texttt{node}: adding a tuple
to \texttt{node} may cause a fact in \texttt{all\_responded} to become false.  If such inserts
are possible, careful maintenance of \texttt{node} is required to ensure confluence: we will
return to this subject in the following section when we introduce {\em coordination}.
%will require coordination 
%\jmh{we haven't defined coordination yet!}.  
This is analogous to the classical problem of dynamic membership in quorum systems: quantifying over all (or a majority
of) participants requires a static ``view'' of membership, and changing this view requires
explicit coordination among participants.



We can generalize Lemma~\ref{lem:guarding} to account for monotonic programs that 
contain syntactic negation or counting with
the notion of a {\em monotonic property}.




\begin{definition}
\label{def:mp}
A monotonic property is a formula which, if it is true at time \dedalus{T},  is true at any time \dedalus{U > T}.
\end{definition}

Intuitively, a monotonic property represents some knowledge that never becomes untrue as we acquire  more knowledge.  A persistent fact is a monotonic property, as is the negation of a
fact that will never be true.  
While we have defined monotonic properties semantically, we may provide a conservative, 
constructive definition based on the following cases:

%We may define monotonic properties constructively as follows. \jmh{You're not defining, you're %giving examples, right?}
%\paa{actually, it was my intention to give structural base cases and then
%use them in a proof by structural induction on Lemma~\ref{lem:property}.
%they aren't complete wrt the semantic definition of monotonic properties,
%but should be sufficient for a proof of the lemma.  but
%it looks like we're going to try a different strategy now (induction on
%derivation depth) ?}  \jmh{No, sorry, that was just me trying unsuccessfully to make sense of 
%your proof. Please follow through on this if it works, just telegraph your plans more clearly.}

%\paa{this constructive definition of monotonic properties may not pan out: feel free to revert.}
%\wrm{your definition is conservative.  first off, let's assume we can specify properties about sealed sets, so we know that coordination is monotonic.  now i want to know that adding coordination to a set makes its negation a monotonic property.  in other words, i have some \dedalus{foo} set that's async, and i want to coordinate something that depends on the negation of \dedalus{foo}.  so i change everything that defines \dedalus{foo} to some new predicate symbol \dedalus{bar}, and define \dedalus{foo} <- \dedalus{bar}, \dedalus{coordination\_foo}.  Now, we know that coordination\_foo is monotonic, and let's say \dedalus{bar} is monotonic.  When I negate \dedalus{foo}, I have the negation of two monotonic properties, but the negation of \dedalus{foo} is monotonic because the coordination is implemented properly, and only goes to true when \dedalus{foo} is complete.}

\begin{enumerate}
\item A persistent fact is a monotonic property.   
Note that persistent facts cannot be retracted once they are asserted.  They begin false
(insofar as their complement can be asserted), and if they become true remain so.

\item A fact defined with even depth of negation over a monotonic property in all derivations is also a monotonic property.  
Even negation depth ensures that the polarity of the fact in question is the
same as that the monotonic property.  In the absence of recursion, negation depth is a syntactically-checkable property.  Whenever there is recursion through negation
(as in temporally stratified \lang programs~\cite{dedalus}), the negation depth is data-dependent.

\item A fact defined via an application of a non-async rule all of whose subgoals are monotonic properties is a
monotonic property.


\end{enumerate}



This list of constructive base cases for monotonic properties is not complete:  
in Section~\ref{sec:coord} we will
axiomatize the way that certain coordination techniques can transform an otherwise 
non-monotonic predicate into a monotonic property.




\begin{lemma}
\label{lem:property}
If a monotonic property (according to the constructive definition) is true in any stable model of a program over a particular EDB, it is true in all stable models. 
\end{lemma}
\begin{proof}
\begin{comment}
Proof sketch: assume a monotonic property is true in one trace and false in another trace.  This means the monotonic property can differentiate between the two traces.  But since all async facts are persisted \jmh{when did we start requiring this?  I think you need to state it as an assumption in the lemma}, all messages eventually rendezvous.  Thus, the monotonic property must be able to observe the condition that some event has not yet occurred (but will eventually occur). \jmh{I don't know what the preceding sentence means; I don't know what it means for a property to observe a condition.}  Monotonic properties cannot observe this though, because the property will be eventually untrue (when the thing that has not yet occurred eventually occurs), thus it is not monotonic. \jmh{That only sorta made sense to me.}

\paa{or alternatively:}
\end{comment}
%We proceed by induction on derivation depth.
%The proof proceeds by structural induction on rule applications

% We will prove by structural induction that if a montononic property is true in any stable
% model of a \lang program, it is true in all stable models.  Consider a montonic property
% $p$.  
%\paa{ok, trying again to do a real proof by structural induction.
The proof proceeds by cases from the constructive definition of monotonic properties.
If $p$ is an EDB fact, it clearly exists in all stable models.
Otherwise $p$ has a derivation $d$ grounded in EDB facts.  As we showed in the
proof of Lemma~\ref{lem:guarding}, if none of the inference 
steps in $d$ is an {\em async} rule, then $p$ exists in all stable models
of the program.
and
the lemma trivially holds.  If there are {\em async} rules, their head predicates are guarded,
because otherwise those head predicates are not provably monotonic
properties (they satisfy none of the cases given above),
%\jmh{No, you only proved the inverse of that assertion: guarded$=>$monotonic.}
and hence $p$, which depends on them, is not a monotonic property.  If there 
is no negation in any inference step of $d$, then by Lemma~\ref{lem:guarding} the program
is confluent, and hence facts in stable models of the program differ only in their timestamps.
Finally, if there is negation in $d$, $p$ is defined with even syntactic depth of negation over some
monotonic property or it would not itself be.  The syntactic negation depth is fixed by the
program, and hence is the same in any stable model.  Therefore $p$ is a monotonic 
property in any stable model of the program.
%\ref{lem:guarding}
\end{proof}

%\paa{I don't really get the proof.  what does it mean for a property to observe a condition?} 

Note that facts may satisfy the semantic definition of monotonic 
properties  (Definition~\ref{def:mp})  without satisfying any of the constructive cases.  For example, a predicate may
be monotonic because it is appropriately coordinated, or because its data-dependent
negation depth is even.
Hence the lemma is defined only in the context of the constructive definition of monotonic properties.  

\begin{definition}
A \lang program is {\em monotonic} if all facts in any ultimate model are monotonic properties.  
\end{definition}

We will see that all logically monotonic programs are confluent.  However, some programs that are not logically monotonic are confluent.

\begin{theorem}
%Confluence is assured by logical monotonicity. \jmh{Can we get away with the phrase ``logical monotonicity?''  I think we may need to be more specific.}
A monotonic \lang program is confluent.
\end{theorem}
\begin{proof}
\begin{comment}
Proof sketch: If a particular ultimate model is populated by atoms that depend only on monotonic properties, then those atoms occur in any ultimate model of the program.  If all ultimate models
are populated in such a way, they are indeed all the same, unique ultimate model.  
\jmh{
\end{comment}
The proof follows directly.  By definition 13, a monotonic Dedalus program has an ultimate model consisting of only monotonic properties, and by Lemma 3 all such properties are true in all ultimate models.  Hence all ultimate models of a monotonic Dedalus program are the same.
%which are true in all traces, then there is a unique ultimate model.
\end{proof}

This discussion formalizes the CALM Conjecture mentioned above with respect to confluence, and proves it in one direction for a broad definition of monotonicity.  
We now show by example that the other direction does not hold: logical monotonicity is not necessary for confluence.

\begin{example}
A confluent Dedalus program that is not logically monotonic.

\begin{Dedalus}
//client
b(#S, I)@async :- b_edb(I), server(S);

//server
b(I)@next <- b(I), !dequeued(I);
b_lt(I, J) <- b(I), b(J), I < J;
dequeued(I)@next <- b(I), !b_lt(_, I);
mem(I) <- dequeued(I), !bt_lt(_, _);

\end{Dedalus}

%\paa{don't we just want to add a subgoal node(s) ?}
%\wrm{or, just consider the 2 counter machine...?}
\end{example}

%odd()@next <- odd(), !dequeued(_);
%odd()@next <- dequeued(), even();
%even() <- !odd();

%The ultimate model contains \dedalus{odd()} if there are an odd number of \dedalus{b\_edb(I)} %facts, and \dedalus{even()} otherwise.  
This program has a single ultimate model in which \dedalus{mem()} contains the highest
element in \dedalus{b\_edb()} according to the order \dedalus{<}.
Thus it is confluent.  However, the program is not logically monotonic because neither \dedalus{dequeued()} nor \dedalus{!dequeued()} are monotonic, so \dedalus{mem()} is not supported by only monotonic properties. Thus the program is not logically monotonic.

%\jmh{I'm kinda bummed out that we don't let UCS help us broaden monotonic properties here further.}

\subsection{Perfect Ultimate Model}
%\wrm{rework definition to make clear we aren't coordinating @nexts}
% this will have to wait -wrm
% \jmh{motivate coordination by analogy to practice}

While our definition of a monotonic property is quite permissive, there are still many cases in practice where programs will not satisfy that property.  These programs are not in themselves confluent, but they often have a single ``natural'' ultimate model that corresponds to evaluating non-monotonic properties only when they can no longer change.  Evaluation of a Datalog program with negation has the same concern: the program has multiple minimal models, so one defines a {\em perfect model} as the minimal model generated by a local synchronous evaluation procedure called {\em stratified evaluation}~\cite{ullmanbook}.
% 
% 
% Programs that are not confluent often have a single ``natural'' ultimate model that corresponds to intuition, much as Datalog programs with negation have a {\em perfect model} corresponding to the model obtained by evaluating the program in a ``natural'' order.  
Similarly, for various uses of negation, we define a {\em perfect ultimate model}, and present a rewrite technique to convert a \lang program into a confluent program that computes this model.

Consider the following example, which is analogous to Example~\ref{ex:nonconfluent2} above:

\begin{example}
\label{ex:sayers}
The diffluent ``sayers'' program.

\begin{Dedalus}
//sayer
statement(#L, S, X)@async <- statement_edb(#S, X),
                             node(L);
s_false(#L, S, X)@async <- statement_edb(#S, X),
                           false_edb(#S, X),
                           node(L);

//listener
true(X) <- statement(_, S, X), !s_false(_, S, X);
false(X) <- statement(_, S, X), s_false(_, S, X);
statement(L, S, X)@next <- statement(L, S, X);
s_false(L, S, X)@next <- s_false(L, S, X);
true(X)@next <- true(X);
\end{Dedalus}
\end{example}

Intuitively this program represents a group of nodes (the ``sayers'') making statements to all nodes (the ``listeners'').  The sayers also occasionally remark that a statement is false (but a sayer may only declare one of his own statements to be false -- not the statement of another sayer).  One may expect the contents of \dedalus{true} to contain all statements that are not \dedalus{false}.  However, this is not necessarily the case.  Recall that the un-sugared version of the third rule is:

\begin{Dedalus}
true(X,T) <- statement(X,T), !false(X,T);
\end{Dedalus}

\noindent
Thus, the contents of \dedalus{true} at time \dedalus{T} are those items in \dedalus{statement} at time \dedalus{T} that are not in \dedalus{false} at time \dedalus{T}.  So in fact, the contents of \dedalus{true} in the ultimate model consist of ``everything stated that was ever not false''.  Such counter-intuitive results are enabled because the closed-world assumption is being applied to incomplete sets.

\begin{definition}
The {\em perfect ultimate model} of a \lang program with no negative cycles in its PDG, denoted $\mathcal{P}(P, E)$, is the ultimate model induced by ensuring all asynchrony is guarded, and no rule containing a \dedalus{!} or a \dedalus{count} is satisfiable before the timestamp when the complete set of facts in the negated or aggregated predicates is sealed: a predicate \dedalus{p} is {\em sealed} at time \dedalus{t} if any fact in \dedalus{p} at time $\dedalus{s} > \dedalus{t}$ is also in \dedalus{p} at time \dedalus{t}.  
%\jmh{This doesn't seem formal enuf to me.  What does it mean. model-theoreticlly, to evaluate a rule?  What do you mean "until"?}
%In other words, one must have ``complete information'' before applying the closed-world assumption for negation.
This intuitively corresponds to the definition of a perfect model from the Datalog literature.
%\wrm{make more formal}  \paa{this is an incomplete definition, right?  we are also interested in
%programs which when temporally flattened are not syntactically stratifiable, yet have a single ultimate model 
%corresponding to their ``coordinated'' evaluation(s)}
%\wrm{Insert UCS Here}
%The perfect ultimate model of a universally constraint stratified~\cite{ross-ucs} program that is universally constraint stratified~\cite{ross-ucs} is the ultimate model induced by ensuring that for every predicate that appears negated in the program subsets are completed in the partial order associated with the stratification.
\end{definition}

%There is always a stable model representing the perfect ultimate model of a \lang program whose flattening is syntactically stratified, because there is no recursion through negation, and Lemma~\ref{cron} tells us that any choice of timestamps is permissible in this case.\jmh{huh?}

Example~\ref{ex:sayers} has no negative cycles in its PDG, thus the rule with \dedalus{!s\_false} in the body should not be evaluated until the \dedalus{s\_false} set is complete.  We can check completeness by having each sayer send a digest of \dedalus{s\_false} messages to all listeners; the listeners recompute the digest over the \dedalus{s\_false} messages they have received; when the computed digest matches the received digest, a data dependency is satisfied, which enables the rule with \dedalus{!s\_false}.

%In Example~\ref{ex:sayers}, any stable model where no \dedalus{false} message arrives after a \dedalus{statement} message with the same value results in the perfect ultimate model.
%In particular, we can modify the program to be confluent with the perfect ultimate model by ensuring that negation is not applied until the \dedalus{false} set is complete.  It turns out we can generalize this into an algorithm for all \lang programs whose temporal flattening is syntactically stratified.

%\wrm{for unstratifiable flattenings, we can introduce another notion of the ``synchronous flattening'', and fully order individual messages passing through an unstratifiable recursion through negation, and call this the perfect ultimate model...}

One possible digest is a \dedalus{count} of \dedalus{s\_false} messages\footnote{A different digest strategy that does not use \dedalus{count} has each sayer sort their messages, and send the sorted order, as well as the maximum message.}.  We add the following two rules to compute counts of false messages at each sayer, and each listener:

\begin{Dedalus}
count_false_sent(#N, S, 0) <- 
  !false_edb(#S, _), node(N);
count_false_sent(#N, S, count<X>) <- 
  false_edb(#S, X), node(N);
count_false_recv(S, count<X>) <- s_false(_, S, X);
\end{Dedalus}

Furthermore, we add a dependency on the equality of the counts into the body of the \dedalus{!s\_false} rule:

\begin{Dedalus}
true(X) <- statement(_, S, X), !s_false(_, S, X),
           count_false_recv(S, X),
           count_false_sent(_, S, X);
\end{Dedalus}

Now, independent of the assignment of timestamps, no statement from a sayer \dedalus{S} is considered to be true by any listener unless the listener has complete information about which statements are false.

\subsubsection{Coordination}
\label{sec:coord}
Given a \lang program $P$ with no negative cycles in its PDG\footnote{A similar algorithm is possible for other statically checkable stratification conditions, such as Universal Constraint Stratification~\cite{ucs}.}, we can generate a confluent program $\textsc{Coord}(P)$, such that \linebreak $\mathcal{U}(\textsc{Coord}(P), E) = \mathcal{P}(P, E)$.

\begin{algorithmic}[1]
  \Procedure{Coord}{$P$}%\Comment{}
  \ForAll{\dedalus{p} such that $\dedalus{q} \Diamondright \dedalus{p} \nrightarrow \dedalus{r}$}
  \ForAll{async rules with \dedalus{p} in the head}
  \State{change head predicate name to \dedalus{p\_local}}
  \State{remove location attribute of every atom in rule}
  \EndFor
  \State{add rules in Figure 1} \label{alg:addrules} %XXX
  \ForAll{rules with \dedalus{!p} in the body} \label{alg:lastfor}
  \State{add \dedalus{!p\_incomplete()} to body}
  \EndFor
  \EndFor%\label{euclidendwhile}
  \State \textbf{return} $P$
  \EndProcedure
\end{algorithmic}


\begin{figure}[h!]
\label{fig:coordcode}
\begin{Dedalus}
p_count_send(#Y,S,count<*>)@async <- p_local(#S,Y,\dbar{X});
p_count_send(#Y,S,0)@async <- !p_local(#S,Y,\dbar{_});
p_send(#Y,S,\dbar{X})@async <- p_local(#S,Y,\dbar{X});
p_count_recv(S,count<*>) <- p(S,\dbar{X});
p_count_send(#Y,S,C)@next <- p_count_send(#Y,S,C);
p(\dbar{X}) <- p_send(\dbar{X});
p(\dbar{X})@next <- p(\dbar{X});
p_incomplete() <- node(S), !p_count_recv(S,_);
p_incomplete() <- node(S), !p_count_send(S,_);
p_incomplete() <- p_count_recv(S, C1),
                  p_count_send(S, C2), C1 < C2;
\end{Dedalus}
\caption{Coordination code}
\end{figure}

\begin{theorem}
For any program $P$, $\mathcal{U}(\textsc{Coord}(P), E)|_{\text{Pred}(P)} = \mathcal{P}(P, E)$, where $\text{Pred}(P)$ is the set of predicates in program $P$\footnote{We consider $\mathcal{U}(\textsc{Coord}(P), E)|_{\text{Pred}(P)}$ instead of $\mathcal{U}(\textsc{Coord}(P), E)$ because the latter contains facts in the predicates \dedalus{p\_count\_send}, \dedalus{p\_count\_recv}, and \dedalus{p\_incomplete}.}.
\end{theorem}

A straightforward argument shows that if \dedalus{p\_incomplete()} is false at time \dedalus{t}, then it is false at time \dedalus{t+1}.  It is easy to see if \dedalus{p\_incomplete} is false at time \dedalus{t}, then there cannot exist a \dedalus{p} fact derived by an asynchronous rule at timestamp $\dedalus{t+1}$.  Thus, any rule with \dedalus{!p\_incomplete()} will not be satisfiable until all elements in the \dedalus{p} set are known.
%\wrm{UCS Here}

%We can similarly define a coordination protocol for more general statically checkable conditions, such as Universal Constraint Stratification~\cite{ucs}.

\subsubsection{Nondeterministic Coordination}
\label{sec:ndcoord}
As noted in Section~\ref{sec:failure}, practical distributed systems may experience failure.  A coordinated program may forever wait if a channel fails, and messages are lost.  For example, \dedalus{p\_count\_recv} and \dedalus{p\_count\_send} in Figure~\ref{fig:coordcode} may never be equal.  In these cases, the perfect model is unachievable.  In practice, exception handling techniques using timeouts are employed to write programs that handle such cases intelligently.  We define a generalization of the perfect ultimate model called a {\em consistently sealed ultimate model} that captures this notion in a way that we argue is preferable to an arbitrary ultimate model.

\begin{definition}
A {\em consistently sealed ultimate model} is an ultimate model generated by nondeterministically sealing the contents of a predicate.
\end{definition}

In Example~\ref{ex:sayers}, all consistently sealed ultimate models have the property that there is no \dedalus{statement} that is both \dedalus{true} and \dedalus{false}, however the \dedalus{true} set may be a superset of the \dedalus{true} set in the perfect ultimate model.  

Note that the perfect ultimate model is a consistently sealed ultimate model; it is the optimal such model in the sense that it commits to the largest version possible of any set.

%Consider what happens when we admit transient and permanent failures of nodes and channels to the model.  It is now the case that nodes may forever wait for a message that will never arrive.  Thus, achieving the perfect ultimate model comes at the expense of liveness.  It may be desirable to accept that the message will never arrive, and proceed with the computation.

%Example~\ref{ex:nonconfluent} performs nondeterministic sealing -- the first batch of \dedalus{p} facts are sealed in \dedalus{q}, and any further \dedalus{p} facts are ignored.  Any ingored \dedalus{p} facts correspond to ``lost'' \dedalus{p} facts, due to either transient or permanent channel or node failure.  It is easy to see that each possible ultimate model represents a possible failure scenario, and each combination of node and channel failures is represented, including the scenario with no failures~\footnote{In practical systems, we may want to model specific real-time constraints -- for example, we declare the set closed after a certain number of seconds have elapsed.  We can easily express this by adding the notion of a ``timer'' (a fact inserted into the queue at intervals measured on a wall clock) to our operational semantics.}.

For any \lang program $P$ with no negated cycles in its PDG, we can generate $\textsc{NDCoord}(P)$, the instrumentation of $P$ so it computes only consistently sealed ultimate models.  The procedure is the same as $\textsc{Coord}$, except we skip the loop on line~\ref{alg:lastfor}, and instead of line~\ref{alg:addrules}, we add the following rules, where \dedalus{boomerang\_p()} is a fresh predicate name:

\begin{Dedalus}
boomerang_p()@async;
boomerang_p()@next <- boomerang_p();
p(\dbar{X}) <- p_send(\dbar{X}), boomerang_p();
p(\dbar{X})@next <- p(\dbar{X});
p_send(\dbar{X})@next <- p_send(\dbar{X}), !boomerang_p();
\end{Dedalus}

The first rule---a local asynchronous rule\footnote{Note that this is actually a rule.  Its unsugared form is: \linebreak \dedalus{boomerang\_p(\#N) <- choice((N),(S)),time(S),node(N);}}---derives \dedalus{boomerang\_p()} at some unspecified time.  When \dedalus{boomerang\_p()} becomes true, we forever seal the \dedalus{p} set.  \dedalus{p\_send} facts accumulate until \dedalus{boomerang\_p()} is true, at which point \dedalus{p\_send} is copied to \dedalus{p}, where it is forever persisted, and deleted from \dedalus{p\_send}, atomically.  Since \dedalus{p} has no \dedalus{p\_send} facts until \dedalus{boomerang\_p()} is true, and no \dedalus{p\_send} facts are copied into \dedalus{p} after at any time after \dedalus{boomerang\_p()} first becomes true, predicate \dedalus{p} is sealed.  Thus we have:

\begin{theorem}
For any program $P$, every ultimate model of \linebreak $\textsc{NDCoord}(P)$ is a consistently sealed ultimate model.
\end{theorem}

Typically, a distributed system would implement a fixed timeout rather than waiting a nondeterministic amount of time for a \dedalus{boomerang}.  Timeouts are conceptually simple to implement; details are provided for an earlier distributed Datalog variant in~\cite{boom}.

%Applying In example~\ref{ex:sayers} above,   The instrumented version of the example would add the following rule:
%
%\begin{Dedalus}
%c_false(L, S, X)@next <- s_false(L, S, X),
%                         !c_false(_, _, _);
%\end{Dedalus}
%
%And would replace any instance of the predicate \dedalus{s\_false} in the ``listener'' rules with \dedalus{c\_false}.
