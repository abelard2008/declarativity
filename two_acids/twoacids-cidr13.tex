\documentclass{sig-alternate}

\usepackage{xspace}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{color}
\usepackage{txfonts}
\usepackage{textcomp}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{comment}
\usepackage{url}


\usepackage{listings}
\lstset{ %
basicstyle=\ttfamily\scriptsize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\ttfamily,      % the size of the fonts that are used for the line-numbers
%aboveskip=0pt,
%belowskip=0pt,
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
%numbersep=10pt,                  % how far the line-numbers are from the code
breakindent=0pt,
firstnumber=1,
%backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=leftline,
tabsize=2,      % sets default tabsize to 2 spaces
captionpos=b,       % sets the caption-position to bottom
breaklines=false,     % sets automatic line breaking
breakatwhitespace=true,    % sets if automatic breaks should only happen at whitespace
%escapeinside={\%}{)}          % if you want to add a comment within your code
columns=fixed,
basewidth=0.52em,
% are you fucking kidding me lstlistings?  who puts the line numbers outside the margin?
xleftmargin=6mm,
xrightmargin=-6mm,
numberblanklines=false,
language=Ruby,
morekeywords={declare,table,scratch,channel,interface,periodic}
}
\lstset{escapeinside={(*}{*)}}


\newcommand{\jmh}[1]{{\textcolor{red}{#1 -- jmh}}}
\newcommand{\paa}[1]{{\textcolor{blue}{[[#1 -- paa]]}}}
\newcommand{\nrc}[1]{{\textcolor{green}{[[#1 -- nrc]]}}}

\begin{document}
\conferenceinfo{CIDR}{'13 Asilomar, CA USA}

\title{{\ttlit Aqua Regia}: A Solution of Two ACIDs\titlenote{\emph{Aqua regia}
    (``royal water'') is a mixture of nitric acid and hydrochloric acid~\cite{aqua-regia}.}}
\numberofauthors{3}
\author{
\alignauthor
Joseph M.~Hellerstein, Peter~Alvaro, Neil~Conway, William~R.~Marczak\\
       \affaddr{UC Berkeley}\\
       % \affaddr{387 Soda Hall \#1776}\\
       % \affaddr{Berkeley, CA 94720-1776  USA}\\
       \email{\small \{hellerstein, palvaro, nrc, wrm\}@cs.berkeley.edu}
\alignauthor
Alan Fekete\\
       \affaddr{University of Sydney}\\
       % \affaddr{Building J12}\\
       % \affaddr{NSW 2006, Sydney, Australia}\\
       \email{\small alan.fekete@sydney.edu.au}
\alignauthor
David Maier\\
       \affaddr{Portland State University}\\
       \email{\small maier@cs.pdx.edu}
}

\maketitle
\begin{abstract}
  Distributed concurrency is supported by two scientific traditions that each
  have a vernacular centered on the acronym \textsf{ACID}.  The original use of
  the term refers, of course, to database transactions. A separate long
  tradition of reorderable convergent actions was recently dubbed
  \textsf{ACID~2.0}.  Both mnemonics are backed by formal theories:
  ACID transactions rely on the theory of serializable schedules, while the ACID
  2.0 idea recently acquired theoretical underpinnings via lattices and the CALM
  Theorem.

  We propose rethinking distributed concurrency from the application viewpoint
  and present a solution blended from both ACIDs.  We begin by combining the
  notion of conflict serializability with CALM analysis, to propose a general
  notion of \emph{Non-Monotone Serializability}.  We then look toward a
  ``system-free'' future, in which software developers write applications with
  transactional assumptions and a compiler uses program analysis to synthesize a
  solution of low-level code that (a) allows a broad range of application-level
  monotonic concurrency, and (b) produces application-specific concurrency
  control for essential non-monotonic conflicts.
\end{abstract}

\section{Introduction}
Concurrency is an evergreen problem in computer systems, which has regained a
great deal of interest in the recent era of wide-area distributed systems and
cloud computing.  There are two scientific traditions in this area that have
been dubbed with the mnemonic \textsf{ACID}, and for each there is an
accompanying theory that codifies the tradition formally and leads to useful
engineering techniques.

The original notion of ACID transactions---and its foundation in serializability theory---was defined in an era of limited physical concurrency and low-level sequential languages.  Software engineering at that time focused on carefully layered monolithic software, and compilers focused on low-level performance optimizations rather than higher-level static analysis for semantic guarantees.  In the spirit of generality, serializability captured the low-level von Neumann model of computation prevalent at the time: a single sequential program issuing a series of Reads and Writes to a mutable store.  As a result, practical work on serializability focuses on \emph{enforcing orderings of Reads and Writes} across transactions.

Work on ACID 2.0 was developed in the context of distributed systems and componentized services, and attempts to address programming at a higher level of abstraction.  Specifically,  it encourages programmers to define application-specific vocabularies of \emph{actions that tolerate reordering}---that is, operations
that produce the same final outcome regardless of the order in which they occur.  
Unfortunately, most work on ACID 2.0 assumes programmers are still using relatively low-level sequential languages and compilers that have limited ability to assess program semantics relevant to distributed systems issues.  Our recent work on the CALM theorem showed how database theory regarding monotonicity can be used to formalize ACID 2.0 maxims~\cite{Alvaro2011,Ameloot2011,Conway2012,Hellerstein2010}, and our work on Bloom showed how language design and compiler technology can use that theory to automatically inform and generate custom, application-aware systems infrastructure~\cite{Alvaro2011}.  

At this point, with the theory behind ACID 2.0 maturing and computing moving to the cloud, we believe it is time to examine theoretical and practical questions that emerge from these two ACID traditions. Can our understanding of ACID 2.0 via the CALM theorem inform the original goals of ACID transactions?  Are there lessons from ACID that can inform design and program analysis in distributed computing?  And given the radical change in computing platforms in the last half-century, is it not time to change our design philosophy from one that focuses on layered systems to one that focuses on application-aware compilation of custom code?

\subsection{Brief Update: CRDTs, CALM and Bloom}
The basics of traditional ACID transactions are well known, but ACID 2.0 is an
idea in evolution.  Hence we pause to briefly survey the field.  The ACID 2.0
acronym reminds us of the desired algebraic properties of a reorderable
vocabulary: Associativity, Commutativity, and Idempotence (the D stands for
Distributed systems).  These properties are helpful in distributed systems where
network delays and failures often lead to reordering and a need for
retry. Low-level Read and Write operations clearly do not satisfy these
properties, so work in this domain tends to suggest application-specific
vocabularies of actions where the relevant properties hold.  Formally, the ACID
2.0 properties are precisely those of the least upper bound in a \emph{join
  semilattice}, which led to recent work on Convergent Replicated Data Types
(CRDTs)~\cite{Shapiro2011a,Shapiro2011b}, an object-oriented model for defining
distributed object classes whose instances tolerate reordering and retry. CRDTs
are of limited utility, however, since an individual class instance rarely
encompasses a full-featured program.


The key property of semilattices is that they describe values that grow
\emph{monotonically} under the action of least upper bound.  The \emph{CALM
  Theorem} proved that monotonic programs have eventually consistent,
coordination-free distributed
implementations~\cite{Alvaro2011,Ameloot2011,Hellerstein2010}.  This translated some key maxims of the ACID 2.0 literature into a robust theory.  The Bloom language~\cite{Alvaro2011} embodied these ideas into a programming language and static analysis techniques. Bloom programmers write high-level programs over collections of unordered data.  By default, the order of statements and data structures in Bloom is left unspecified by the programmer, maximizing runtime latitude for parallelism and asynchrony.  A CALM-based program analysis technique examines the program statically to identify non-monotonic constructs in the program; these ``points of order'' need to be controlled via a coordination protocol to ensure deterministic behavior.  In recent work combining the idea of lattice data types with CALM, we extended Bloom to support complex programs constructed out of simple lattice instances composed 
with monotone operators; CALM analysis can then check for monotonicity and points of order in a manner that takes into account the ACID 2.0 properties of the lattices and their compositions~\cite{Conway2012}.  Since this is a new result, we give familiar examples of this idea below.  However, in order to highlight the non-monotonicity in our examples, we do not use Bloom syntax; instead, we use the formal logic Dedalus~\cite{dedalus} that is the basis of Bloom.


\section{Non-Monotone Serializability}
We begin by applying ACID 2.0 ideas to traditional ACID.  Specifically, we use the CALM theorem to enable static program analysis that can extract a range of semantically-aware serializable schedules---including schedules that would be forbidden by traditional Read/Write transaction protocols.

Recall that pure serializability has a declarative definition: it encompasses all interleavings
that produce an outcome equivalent to a serial execution of transactions.  In
practice, the more conservative notion of conflict serializability has typically
been used, which \emph{constrains the ordering of actions} across transactions:
actions conflict if they access the same data item and one of them is a
Write~\cite{Eswaran1976}.  
% Since an individual transaction is classically a
% sequential program, conflicts are only explicitly ordered between actions across
% transactions.

Viewed through the lens of ACID 2.0, we might see this more positively.  Reads are commutative, associative and idempotent, hence we can \emph{tolerate reordering of Reads}.  But what about Writes? 
As noted in the literature on stateful logic languages~\cite{dedalus,statelog}, mutable state 
involves non-monotonic logic: the persistence of a value depends on it being
NOT(revoked). Logically, a Write is an atomic pair of actions: the revocation of a previous
value (``the King is dead'') and the introduction of a new value (``long live the King!''). 
% Non-monotonic persistence logic is ``triggered'' by the revocation, so 
Write triggers revocation, which is negated in the persistence logic, hence \emph{Writes are non-monotonic}.

The CALM Theorem shows that monotonic logic is insensitive to the order of evaluation, but non-monotonic logic produces different outcomes depending on evaluation order.  Using this observation, we can generalize the notion of ``non-conflicting'' actions beyond Reads: any monotonic logic can be reordered, so transaction scheduling need only consider non-monotonic actions as conflicts.

To make this discussion more concrete, we first show how traditional Read/Write
schedules can be encoded in a stateful logic such as Dedalus~\cite{dedalus} which exposes 
the inherent non-monotonicity. We
then show how schedules containing only monotonic updates can be executed
without concurrency control and yet still guarantee serializability. Finally, we
show that schedules containing a mix of monotonic and non-monotonic updates only
require concurrency control ``around'' the non-monotonic constructs. We refer to
this as \emph{Non-Monotone Serializability}---a generalization of Conflict Serializability.

Consider the classic case of credit/debit ledgers. For the sake of exposition we begin with simple ``canned'' transactions that only issue a single credit request.  Using
traditional Read and Write operations, we can model this transaction mix using Dedalus~\cite{dedalus} as
shown in Figure~\ref{fig:balance}.


\begin{figure}[h]
\begin{scriptsize}
\lstinputlisting{code/bal.ded}
\centering
\vspace{-10pt}
\caption{Simple Read/Write credit transactions in logic.  Note the non-monotonic negation symbol in Line~\ref{line:neg}.}
\label{fig:balance}
\end{scriptsize}
\vspace{-2pt}
\end{figure}

Dedalus models discrete sequential timesteps.
The expression \texttt{p(X)@t} asserts that a fact \texttt{p(X)} is true at time
\texttt{t}; the absence of a time suffix \texttt{@t} implies the current timestep.  The time suffix \texttt{@next} denotes the timestamp that immediately follows the
current timestamp.  Line~\ref{line:neg} says that an account balance persists from one timestep (now) to the \texttt{next} unless it is revoked. The
conflict in this program is manifest in that syntax: negation
($\neg$ in line~\ref{line:neg}) is non-monotonic.  The negated logic \texttt{revoke\_bal} is derived from the \texttt{write} in Line~\ref{line:write_trans}, which is itself derived from the scheduler issuing a \texttt{sched\_W} in Line~\ref{line:write_req}.  A similar analysis can show that \texttt{read} requests do not affect the contents of \texttt{revoke\_bal}, but do depend upon the state of \texttt{revoke\_bal}.
Hence, a compiler for this language---or for a more programmer-friendly language like Bloom~\cite{Alvaro2011}---can determine that scheduler requests will need to control the order of both \texttt{read}s and \texttt{write}s with respect to \texttt{write}s, but can ignore the ordering of \texttt{read}s independent of \texttt{write}s.  That is, the compiler can statically determine the conflicts in the program from syntax.

However the writes in this program induce what we might call ``false conflicts'': in a credit-only program, balances grow monotonically, so it should in some way be OK to reorder writes.  We can avoid these false conflicts by better exposing the monotonicity of our balance updates in application logic. We
can model the account balance as an instance of an object class (a join semilattice) with a monotonic merge method ``$\sqcup$'' (least upper bound)~\cite{Conway2012}. Using this idea, we can replace the previous non-monotonic persistence rule: rather than revoking old account balances,
we say simply that the balance object grows as more credit operations are merged into it.\footnote{The choice of a merge method is pluggable, as long as it is associative, commutative, and idempotent. Note that addition is not idempotent.  To capture typical credit ledger logic, the \texttt{proposed} values can use a custom lattice data type that pairs a nonce and an integer, and a merge function that essentially removes duplicates by nonce.  Bloom supports the registration and validation of such user-defined lattice data types~\cite{Conway2012}.}  We can capture this in version of Dedalus that includes a merge method, as in Figure~\ref{fig:lattice_balance}.

\begin{figure}[h]
\begin{scriptsize}
\lstinputlisting[mathescape]{code/latticebal.ded}
\centering
\vspace{-10pt}
\caption{Credit transactions in monotonic logic.}
\label{fig:lattice_balance}
\end{scriptsize}
\vspace{-2pt}
\end{figure}

\noindent
Because this program contains only monotonic constructs, a compiler can 
certify that it is deterministic regardless of the timestamps that are
assigned to the write operations; thus, it can safely be executed without
concurrency control. 
% Said differently, we have achieved \emph{compiler-cert conflict-free}.

We now consider how to support non-monotonicity: debit operations that include an overdraft test. 
Intuitively, from the CALM perspective debits would seem to introduce conflicts because they are 
non-monotonic relative to credits: monotone operations can only cause the
\texttt{bal} lattice to increase. However, in traditional ledgers, debits are recorded separately from credits as a monotonic list.  It is the balance calculations---and specifically, the tests for overdrafts---where non-monotonicity becomes apparent.  In the interest of space, we present pseudocode:
\begin{quote}
	\begin{scriptsize}
 	\textsf{debit account \textbf{if} (old\_balance + SUM(credits) - SUM(debits) $>$ 0)}
	\end{scriptsize}
\end{quote}
Basic rules of arithmetic---easily registered with a compiler~\cite{Conway2012}---tell us that as credits and debits grow monotonically, the left-hand-side of this inequality goes up {\em and down}, and hence the truth value of the predicate can oscillate non-monotonically as well.  Much like our discussion of Figure~\ref{fig:balance}, the logic for the overdraft test contains a non-monotonic clause (arithmetic subtraction), which is transitively dependent on a monotonic operation (accumulation of debits).  Hence scheduler requests will need to control the order of both credits and debits with respect to debits, but can ignore the ordering of credits independent of debits.\footnote{Note that we coupled debit with an overdraft check.  A common design pattern decouples overdraft checks---e.g. it applies them only once per day.  This introduces the potential to allow free reordering of debits and credits during a single day without affecting the non-monotonic logic.  We return to this point in the next section.}

Summing up, we make two key points.  First, the CALM theorem allows us to explain and generalize the notion of conflict via non-monotonicity---and thus ACID 2.0 helps us generalize ACID.  Second, and more suggestively, compilers for new high-level languages like Bloom can pinpoint semantic conflicts in application logic and synthesize application-specific serializability enforcement code that protects only those conflicts.

\begin{comment}
 What follows is an example of a mutable key-value store ``king'',
keyed by country.  We express this using the logic notation of
Dedalus~\cite{dedalus}, which exposes the inherent non-monotonicity:
\begin{quote}
	\emph{// Define read: each item of the set king: }\\
	read(C,K) :- king(C,K)\\
	\\
	\emph{// Define mutable persistence in logic:}\\
	\emph{// The King lives at the next timestep if he is NOT(dead)}\\
	king(C,K)@next :- king(C,K), $\neg$dead\_king(C,K)\\
	\\
	\emph{// Write: visible at subsequent timestep}\\
	\emph{// The King is Dead, Long Live The King}\\
	dead\_king(C,K0)@next :- king(C,K0), write(C,K1)\\
	king(C,K1)@next :- write(C,K1)\\
	\\
	write(France, Charles6)@1380.\\
	write(France, Charles7)@1422.
\end{quote}
The problem (or ``conflict'') with the Write-able key/value pairs king(C, X) is
clear from the syntax: ``NOT'' ($\neg$) is the basic non-monotonic operator in
logic.  Operationally, a Read of the current state of king must wait for the
contents of dead\_king to be completely accounted for.\nrc{Assuming you want to
  read the ``final''/confluent value of king, which of course is not the usual
  assumption in ACID transactions.}

By the CALM theorem, the ``coordination'' required to seal the contents of a set
(e.g., dead\_king) is not necessary for purely monotonic logic.  Said
differently, \emph{monotonic logic is conflict-free}.  This suggests a broader
class of schedules that we can guarantee to be ``conflict''-serializable: those
in which the non-monotonic conflicts are ordered the same as some serial
schedule.  We refer to this as \emph{Non-Monotone Serializability}.

As an example, consider the classic case of debit/credit ledgers.  We begin with
the simple case in which all actions are credits. We assume that an account is
liquid if its balance is above zero. In Bloom's lattice notation~\cite{Conway2012},
this looks like so:
\begin{quote}
  \jmh{`ledger' is an lmap with key `acctId' and value an lset of [xid, amount]
    pairs.  `balance' is a derived lmap with key `acctId' and an lmax `value'.
    `liquid' is an lmap from `acctId' to an lbool `status'.  The monotone
    function sum maps from [xid,amount] to `value', and the monotone function gt
    maps from `value' to `status'. Need to give syntax and explanation, perhaps
    in a figure if its too big a gulp inline.  I fudged: ledger is not a
    sequence, it's a set.  That's OK for the credit-only scenario, but kinda
    bogus.}
\end{quote}
In a credit-only scenario, the ledger grows monotonically as a set of credits,
the account balances grow monotonically as integers, and account liquidity grows
monotonically as a boolean (from false to true).  However, consider introducing
debits that succeed only if the account would remain liquid: ``conflicts'' can
arise between the liquidity checks of debits and credits, so we must establish
ordering constraints for each debit with respect to preceding credits.  However,
the order among credits remains irrelevant---the only order that matters relates
to the non-monotone operator `debit'.  In Bloom, this non-monotonicity is again
clear from the syntax and checkable by a parser:
\begin{quote}
	\jmh{debit logic including threshold test goes here}
\end{quote}
Note that we can be finer-grained in our assessment of conflict: there are no
conflicts across acctIds, which can be determined via analyzing the program
logic: the grouping of balance by acctID \emph{partitions} the ledger.  This
partitioning is akin to the partitioning of Reads and Writes, which conflict
only if they are on the same object.
\end{comment}

\section{ACID ACID 2.0}
In the previous section, we saw how the ACID 2.0 idea of
\emph{tolerating disorder} at application level informs
serializable transaction processing. The idea is not surprising.  With only ACID 2.0 actions, (1) all serial schedules have the same outcome,  (2) all transaction schedules have the same outcome, and hence (3) all schedules are serializable.  Indeed, in this scenario, transaction IDs is irrelevant---the set of actions alone defines the outcome of the schedule.  As a result, fully ACID 2.0 systems admit \emph{streaming} implementations:  any order of operations produces the same (desired) result, so operations can be applied on demand.

But what can traditional ACID tell us about ACID 2.0?  How can Serializability inform CALM?  
The answer must come from consideration of non-monotonicity---cases where ACID 2.0 design do not apply, or what the papers on CALM call \emph{points of order}~\cite{Alvaro2011}.

Non-monotonic logic induces a ``point of order'' because the order of execution affects their logical outcomes.  While predicates in non-monotonic logic are growing, our conclusions about them can change.  In order to evaluate a non-monotonic statement (e.g., ``set $S$ does not contain $x$''), we must \emph{seal} the contents of its inputs---exclude the possibility of future changes.  Sealing a predicate marks a point in time: ``before'' (mutable but unknown) and ``after'' (immutable and known).  Once a non-monotonic predicate is sealed, we can proceed with otherwise-monotonic reasoning, confident in the knowledge that even our non-monotonic predicates will not shrink.  

Said differently, non-monotonic logic should be defined by an \emph{Atomic} input set, whose contents are \emph{Durable} throughout the evaluation of subsequent application logic!  We can see the connection to transactions more readily when we face the realities of a live service supporting non-monotonic logic.  In a typical live service, some of the ``sets'' have infinite external origins: they capture the stream of requests from users.  If these sets participate transitively in non-monotonic logic, how or when do we ``seal'' them?  What are the atomic boundaries?  Consider a concrete example from the previous section: in a debit/credit ledger, when should we check account overdrafts?  If we check them on every debit, we get a very strictly-ordered system with little more concurrency than a naive Read/Write analysis.  On the other hand, if we follow the standard accounting practice of checking once daily, we get a system with significant latitude for reordering. 

Outside the rosy halo of pure ACID 2.0 and streaming execution, the partitioning of the input stream is a critical design decision of a live service. 
It is standard practice to partition a service's input into bounded subsequences: \emph{sessions}, \emph{epochs}, and the like. Logically, this involves little more than adding a field to each data item that captures the identifier value for partitioning (session ID, epoch number, etc.).  
% The definition of ``sealed'' or ``atomic'' batches of input---the mapping of actions to transaction IDs---is a critical aspect of the design of a distributed service.  
It is of course also possible to further partition on other keys in the application data: account IDs, geographic locations, etc.  But the partitioning of unbounded inputs is required for liveness.

Notice that in this worldview, the mapping of actions to transactions is a \emph{proactive} aspect of system design.  In traditional literature on ACID, we assume that transaction boundaries are given by ``application logic'', and the transaction system reacts by enforcing ordering constraints.  In ACID 2.0, we are explicitly interested in designing the application logic.  A key part of that design process---even if we eschew serializability---is to design the transaction boundaries to achieve the right balance of application semantics (e.g. how much time can elapse with an overdrawn account) and system efficiency (how often we want to invoke coordination logic to compute balances.)  CALM analysis can aid in this task by exposing points of order precisely, and enabling the application designer to think about rewriting programs to change partitioning in a way that allows points of order to be crossed less often (to improve throughput by reducing coordination), more often (to improve latency at the expense of more frequent coordination) and/or on smaller partitions of the data (to limit the parties who may have inputs to coordinate).

This discussion did not yet mention Consistency or Isolation.  The ``C'' in ACID is a common source of confusion in discussions of transactions and distributed systems, and we will ignore it here\footnote{Consistency was apparently added to ACID simply to make it a more entertaining acronym.  The most concrete definition from the database literature is ``enforcement of database integrity constraints'', which has some bearing on our discussion here of partitioning keys, but will have to wait for a longer paper.  The distributed systems community tends to use the term Consistency differently, and with an equally frustrating disregard for formalism, but without even the justification of forming an entertaining acronym.}.  With regard to Isolation, the previous section on serializability and CALM comes into play directly.  Once transaction boundaries are assigned to a program, CALM analysis can expose the non-monotonic conflicts in the program, and their transitive data dependencies.  The programmer can then choose how to resolve the observable non-monotonicity.  Serializability provides a certain Isolation guarantee, which the programmer could choose to enforce across their non-monotonic boundaries.  But the literature offers many others, and CALM-style dataflow analysis of a logic program can help determine the transitive effects of weaker models: by chasing logical dependencies (data derivations across tables and keys within tables) it can help the programmer understand and control how non-monotonic non-determinism can ``flow'' through a program.  This discussion is admittedly brief, but we note that (a) the various definitions of isolation should be formally translatable into the framework of non-monotonic conflicts and data dependencies as in the previous section, and (b) there is a rich tradition of formal literature on chasing program dependencies that can aid in the compiler analysis~\cite{alicebook}.

In short, we have argued here that (1) live distributed systems that are not pure ACID 2.0 \emph{must} consider the ``A \& D'' of traditional ACID, (2) the proactive design of ``transaction'' or ``session'' boundaries is a key aspect of distributed systems that goes beyond ACID 2.0, and (3) compiler techniques based on the CALM theorem can help programmers reason about and control this design with respect to traditional ``A \& D'' as well as ``I''.

\section{The Vision}



\jmh{Original notes:}  
Application-specific ACID.  Understand ``true'' conflicts.  Highlight the need
for ``slicing'' into ``batches''.  Guide the application toward an efficient,
useful choice of batches and barriers.

Example: slicing into totally independent batches: easy concurrency control
across transactions (but possibly need coordination intra-transaction)

Example: dependent batches -- this is really just serializability?  No -- also
intra-transaction parallelism.

\paa{the below is something to shoot holes in}

Our end goal is an adaptable, application-specific embodiment of the best of both ACID traditions in a practical programming language. This language will require program analysis tools to recognize ``true'' conflicts---operations that produce different end states when reordered---and resolve them in a manner appropriate to the application.

We observed a similarity between transactions in databases and sessions in distributed systems.  Both represent names for a logical grouping of actions, and mechanisms that control the visibility of these groupings can be used to ensure atomicity (these actions occur together) and isolation (I need not reason about interleavings with other actions).  When these grouping correspond to finite, continguous ranges of real time, they can be used to constructively define (potentially infinite) collections in which partitions grow, become ``sealed,'' and are henceforth immutable.  As we recall from stratified logic programming, nonmonotonic operations against complete inputs produce a single model; similarly, distributed programs that constrain their nonmonotonic deductions to sealed partitions are insensitive to the order in which the contents of those partitions materialized.
 
Similarities aside, the shopping cart sessions described in Alvaro et al fundamentally differ from general database transactions in their mutual independence: no two users update the same cart, and in this simple example, no two carts conflict in any of their operations (e.g., by changing inventory).  Hence trivial coordination---in the form of manifest shipping---was sufficient to ensure that replicas agreed, and no concurrency control was necessary.  A transactional store cannot make such an independence guarantee over transactions, which may access and mutate shared state.   It is precisely this dependency on nonmonotonically updated shared state that makes serializability (a weaker correctness criterion than confluence) the ``right'' correctness criterion for transactional systems---when different orders produce different results, we require a protocol to choose which order ``wins.'' 

If we write programs in the right language, detecting nonmonotonicity is easy, as is extending such a language with support for programmer-guided ``sealing'' of input partitions.  Given a particular sealing attribute---for example, ``session id''---a straightforward dependency analysis can tell us whether individual seals are dependent by answering (conservatively) whether the consequences of processing one partition can affect outputs associated with another.  This whole program analysis will tell us that because we cannot prove independence of seals (transactions) a generic transactional store requires additional arbitration in the form of an ordering \emph{across} seals.  By contrast, we can prove independence of seals (sessions) for the shopping cart application---even when the cart is implemented atop a transactional kvs.  

Thus we may choose the appropriate protocols ``just in time'' based upon a given composition; a transactional store exposes the abstraction of transactions, but uses an order-enforcing mechanism like locks to implement them only when the access patterns implied by the application make it necessary. The programmer is forced to provide some extra information---\emph{what} partitions are we sealing; \emph{when} do we seal them?---and in return the compiler synthesizes custom coordination to exclude bad outcomes.  

%ACKNOWLEDGMENTS are optional
% \section{Acknowledgments}

\bibliographystyle{abbrv}
\bibliography{cidr}
\end{document}
