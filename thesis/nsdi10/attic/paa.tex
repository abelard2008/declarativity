\subsection{some points about databases}
\begin{itemize}

\item they tend to pipeline operations (only materializing for blocking operators usually needed for orders (sort) and memory-intensive operations merge/hash joins, and for bushy tree parallelism (because synchronizing subtrees is hard))

\item they use both demand- and data-driven control flows.  demand-driven are easier to reason about; data driven deal better when operators proceed at different speeds, but require dealing with queues, etc.  it's good to be able to do both.
\item they handle failure at the granularity of a transaction (coarsely).  so they punt on retry, but since a transaction may write to the database at any time (not just at the end), they have elaborate undo.
\end{itemize}

\subsection{some points about mapreduce}
\begin{itemize}
\item it materializes all stages: both between M-R and between stages of MR-M.  no pipelining.

\item it uses a demand-driven dataflow during the shuffle stage: reducers request data from the filesystems of mappers.

\item it handles failure at the granularity of a task.  maps represent some constant fraction of the input, and reduces represent some (hopefully close) fraction of the output.  so transactions can make progress through many fine-grained failures.  since the MR pair is (in principle) purely functional, we only see writes back to the store (DFS) after reduces, and atomic rename makes the reduces idempotent.  hence, a failure does not cause abort, and if we want to abort, it's a simple matter of stopping the job.
\end{itemize}

\subsection{anecdotal problems with mapreduce}
\begin{itemize}

\item{TCP incast collapse}  

the many-1 nature of communication between a reduces and ALL mappers,
each of which contains a slice of the reducer's partition, makes
incast issues likely in itself.  MR's communication pattern, in which
all reducers begin requesting data from all mappers at roughly the
same time (the shuffle phase), aggravates this issue by clustering the
many-1 communication pattern in time. \nrc{Description here is
  slightly wrong: reducers only request data from at most \emph{n}
  mappers at a time.}

\item{I/O seek contention}  

the MR pattern of having all reducers connect to a single mapper to read from its local disk causes a lot of seek behavior on the partitioned, sorted, spilled map output.  again, the general communication is part of the architecture, but better control of the timing could convert many of these seeks to scans.

\end{itemize}

\subsection{a solution?}

pipelining with data-driven control -- that is to say, sending data 'all along' during the map computation, rather than all at the end.  this could mean:
without changing the communication pattern, we address incast by spreading the communication out in time.
by pushing data to the sinks as it becomes available, we give control
over the disk reads to the source.  this will result in better opportunities to schedule sequential scans.

\subsection{a problem?}

eagerly sending map'd data to the reducer means favoring sending over storage and local computation.  in the limit, we could pipeline a tuple at a time, which would mean forfeiting the ability to perform any local computation (aggregation of keys and valuelists, calling the combiner function) and poor network utilization.  so in general there is a tradeoff between buffering and combing locally and eagerly pipelining, and specifically, any pipelining approach will push more aggregation work (merging, sorting, hashing, combining) to the reducer.

early in our experiments, we implemented mapper pipelining by sending map'd data directly to the reducer in the same thread that executed the map function, and observed a considerable decrease in performance. (how many reducers were we using?)  we attribute this to two factors.  First, pipelining (as noted above) pushes a considerable amount more of the distributed aggregation to the reducers.  in vanilla hadoop, each mapper will essentially perform the sorts of the partial valuelist for each key, leaving only the (possibly multi-step) merge to the reducer, followed by application of the reduce function to the valuelist of each key.  in our naive pipelining, we are either pushing ALL sort work to the reducer (in the limit case), or are sending much smaller sorted runs. in many cases, this caused a dramatic slowdown of the reducer, and our simple pipelining caused the mappers to be bound by the speed of the reducers.

\subsection{observations}

pipelining isn't always ideal.  intuitively, pipelining will win for
short jobs, jobs with selective or expensive map functions, generally
jobs whose output is much smaller than their input.  when the map
output is large, or the reduce function expensive, it doesn't make
sense to have the mappers block on writes when there's useful work
they could be doing locally (like running the map function on input,
and spilling to disk if necessary). \nrc{Slightly wrong, in that any
  sensible implementation wouldn't need to block on sending map output
  to the reduce: there's no reason we can't overlap running the map
  function with sending data to the reducer. The tradeoff here is
  between map and reduce compute cycles and network bandwidth; the
  problem is \emph{not} that we can't do ``useful work'' while doing
  pipelined network I/O.}

we'd like a system that can do better when pipelining wins, and not do any worse when it doesn't.

\subsection{a better solution}

first, we need to decouple the computation of the map function from the pipelining of map output to the reducer.  we do this by implementing the pipelining client as a separate thread that 'follows' the mapper by reading its spill files.  If the reducers run at mapper speed, so does this thread.  if the reducers are slow, then the follower will read more spill files each time it alternates between reading and pipelining, and will have more opportunities to do precomputation (sorting, merging) before sending to the reducer.  this amount to an adaptive solution to the problems of when to materialize or pipeline...

\subsection{does this still solve the incast problem?}

we solve the incast problem by de-synchronizing the communication between mappers and reducers.  if the reducers are very slow or do not contact us quickly enough, we'll end up (mostly) imitating vanilla hadoop, doing a potentially multi-pass sort on the mapper output, and sending it to the reducer.  if we assume that often, most map task take the same amount of time to complete, in this extreme case we'd expect the same incast issues.

\subsection{does this still solve the disk seek problem?}

arguably, yes.  the mappers are in control of their seeks and scans, and the reducers remain in control of theirs.

\subsection{the lesson?}

one studently thing to say would be this:

when looking at system design, thinking about system refactoring, etc, for distributed dataflow systems, it is helpful to think along the (somewhat) orthogonal lines of pipelining vs. materialization along dataflow edges, demand-driven vs. data-driven control flow, and the various possible granularities of recovery, communication and control (which need not be the same, as they are in MR/hadoop). because of the opaque nature of map and reduce functions, it is a clear mistake to choose from among these options statically.  etc.
