\section{Background}
\label{sec:background}
In this section, we review the MapReduce programming model and describe the
salient features of Hadoop, a popular open-source implementation of MapReduce.

\subsection{Programming Model}
\label{sec:progmodel}
To use MapReduce, the programmer expresses their desired computation as a series
of \emph{jobs}. The input to a job is an input specification that will yield
key-value pairs. Each job consists of two stages: first, a user-defined
\emph{map} function is applied to each input record to produce a list of
intermediate key-value pairs. Second, a user-defined \emph{reduce} function is
called once for each distinct key in the map output and passed the list of
intermediate values associated with that key.  The MapReduce framework
automatically parallelizes the execution of these functions and ensures fault
tolerance.

Optionally, the user can supply a \emph{combiner}
function~\cite{mapreduce-osdi}. Combiners are similar to reduce functions,
except that they are not passed \emph{all} the values for a given key: instead,
a combiner emits an output value that summarizes the input values it was
passed. Combiners are typically used to perform map-side ``pre-aggregation,''
which reduces the amount of network traffic required between the map and reduce
steps.

\subsection{Hadoop Architecture}
Hadoop is composed of \emph{Hadoop MapReduce}, an implementation of
MapReduce designed for large clusters, and the \emph{Hadoop
  Distributed File System} (HDFS), a file system optimized for
batch-oriented workloads such as MapReduce. In most Hadoop jobs, HDFS
is used to store both the input to the map step and the output of the
reduce step. Note that HDFS is \emph{not} used to store intermediate
results (e.g., the output of the map step): these are kept on each
node's local file system.

A Hadoop installation consists of a single master node and many worker
nodes. The master, called the \emph{JobTracker}, is responsible for
accepting jobs from clients, dividing those jobs into \emph{tasks},
and assigning those tasks to be executed by worker nodes. Each worker
runs a \emph{TaskTracker} process that manages the execution of the
tasks currently assigned to that node. Each {\TT} has a fixed number
of slots for executing tasks (two maps and two reduces by
default).%  A heartbeat protocol between each {\TT} and the {\JT} is used
% to update the {\JT}'s bookkeeping of the state of running tasks, and
% drive the scheduling of new tasks: if the \JT identifies free {\TT}
% slots, it will schedule further tasks on the {\TT}.

\subsection{Map Task Execution}
\label{sec:maptask}
\begin{figure}[t]
\begin{minipage}{\linewidth}
\begin{small}
\begin{verbatim}
public interface Mapper<K1, V1, K2, V2> {
  void map(K1 key, V1 value,
           OutputCollector<K2, V2> output);

  void close();
}
\end{verbatim}
\end{small}
\end{minipage}
\vspace{-4pt}
\caption{Map function interface.}
\label{fig:mapfunction}
\end{figure}

Each map task is assigned a portion of the input file called a \emph{split}. By
default, a split contains a single HDFS block (64MB by default), so the total
number of file blocks determines the number of map tasks.

The execution of a map task is divided into two phases.
\begin{enumerate}
\item
  The \emph{map} phase reads the task's split from HDFS, parses it into
  records (key/value pairs), and applies the map function to each
  record.
\item
  After the map function has been applied to each input record, the
  \emph{commit} phase registers the final output with the {\TT}, which
  then informs the {\JT} that the task has finished executing.
\end{enumerate}

Figure~\ref{fig:mapfunction} contains the interface that must be
implemented by user-defined map functions. After the \emph{map}
function has been applied to each record in the split, the
\emph{close} method is invoked.

The third argument to the \emph{map} method specifies an \emph{OutputCollector}
instance, which accumulates the output records produced by the map function. The
output of the map step is consumed by the reduce step, so the OutputCollector
stores map output in a format that is easy for reduce tasks to
consume. Intermediate keys are assigned to reducers by applying a partitioning
function, so the OutputCollector applies that function to each key produced by
the map function, and stores each record and partition number in an in-memory
buffer. The OutputCollector spills this buffer to disk when it reaches capacity.

A spill of the in-memory buffer involves first sorting the records in
the buffer by partition number and then by key. The buffer content is
written to the local file system as an index file and a data file
(Figure~\ref{fig:mapoutput}). The index file points to the offset of
each partition in the data file. The data file contains only the
records, which are sorted by the key within each partition segment.

During the \emph{commit} phase, the final output of the map task is generated by
merging all the spill files produced by this task into a single pair of data and
index files. These files are registered with the {\TT} before the task
completes. The \TT will read these files when servicing requests from reduce
tasks.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.90\linewidth]{figures/spill_file.pdf}
  \vspace{-8pt}
  \caption{Map task index and data file format (2 partition/reduce case).}
  \label{fig:mapoutput}
\end{figure}

\subsection{Reduce Task Execution}
\label{sec:reducetask}
The execution of a reduce task is divided into three phases.
\begin{enumerate}
\item The \emph{shuffle} phase fetches the reduce task's input
  data. Each reduce task is assigned a partition of the key range
  produced by the map step, so the reduce task must fetch the content
  of this partition from every map task's output.
\item The \emph{sort} phase groups records with the same key together.
\item The \emph{reduce} phase applies the user-defined reduce function
  to each key and corresponding list of values.
\end{enumerate}

In the \emph{shuffle} phase, a reduce task fetches data from each map task by
issuing HTTP requests to a configurable number of {\TT}s at once (5 by
default). The {\JT} relays the location of every {\TT} that hosts map output to
every {\TT} that is executing a reduce task. Note that a reduce task cannot
fetch the output of a map task until the map has finished executing and
committed its final output to disk.

\begin{figure}[t]
\begin{minipage}{\linewidth}
\begin{small}
\begin{verbatim}
public interface Reducer<K2, V2, K3, V3> {
  void reduce(K2 key, Iterator<V2> values,
              OutputCollector<K3, V3> output);

  void close();
}
\end{verbatim}
\end{small}
\end{minipage}
\vspace{-4pt}
\caption{Reduce function interface.}
\label{fig:reducefunction}
\end{figure}

After receiving its partition from all map outputs, the reduce task enters the
\emph{sort} phase. The map output for each partition is already sorted by the
reduce key. The reduce task merges these runs together to produce a single run
that is sorted by key. The task then enters the \emph{reduce} phase, in which it
invokes the user-defined reduce function for each distinct key in sorted order,
passing it the associated list of values. The output of the reduce function is
written to a temporary location on HDFS\@. After the reduce function has been
applied to each key in the reduce task's partition, the task's HDFS output file
is atomically renamed from its temporary location to its final location.

In this design, the output of both map and reduce tasks is written to disk
before it can be consumed. This is particularly expensive for reduce tasks,
because their output is written to HDFS\@. Output materialization simplifies
fault tolerance, because it reduces the amount of state that must be restored to
consistency after a node failure. If any task (either map or reduce) fails, the
{\JT} simply schedules a new task to perform the same work as the failed
task. Since a task never exports any data other than its final answer, no
further recovery steps are needed.

 % Mention fault-tolerance/atomic
% rename here?

% \subsection{Dataflow in Hadoop}
% Hadoop \emph{materializes} the output of the map and reduce functions
% in two places:

% \begin{itemize}
% \item
%   Between the map and reduce steps, the output of each map task is
%   written to disk before it can be read by any reduce tasks.
% \item
%   The output of the reduce step is written to HDFS at the end of each
%   job. As we observed in Section~\ref{sec:progmodel}, many practical
%   computations are written as a chain of MapReduce computations
%   (e.g.\ PageRank). In that setting, writing the output of each job to
%   HDFS and then reading it back again is inefficient.
% \end{itemize}
