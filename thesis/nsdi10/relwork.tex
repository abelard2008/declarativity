\section{Related Work}
\label{sec:relwork}
The work in this paper relates to literature on parallel dataflow frameworks, online aggregation, and continuous query processing.

\subsection{Parallel Dataflow}
Dean and Ghemawat's paper on Google's MapReduce~\cite{mapreduce-osdi}
has become a standard reference, and forms the basis of the
open-source Hadoop implementation.  As noted in
Section~\ref{sec:intro}, the Google MapReduce design targets very
large clusters where the probability of worker failure or slowdown is
high.  This led to their elegant checkpoint/restart approach to fault
tolerance, and their lack of pipelining.  Our work
extends the Google design to accommodate pipelining without significant
modification to their core programming model or fault tolerance
mechanisms.

\emph{Dryad}~\cite{dryad07} is a data-parallel programming model and runtime
that is often compared to MapReduce, supporting a more general model of acyclic
dataflow graphs.  Like MapReduce, Dryad puts disk materialization steps between
dataflow stages by default, breaking pipelines.  The Dryad paper describes
support for optionally ``encapsulating'' multiple asynchronous stages into a
single process so they can pipeline, but this requires a more complicated
programming interface.
%The paper does not describe how fault tolerance works in this setting, but presumably when these encapsulated stages fail they restart in their entirety.  
The Dryad paper explicitly mentions that the system is targeted at batch processing, and not at scenarios like continuous queries.  
%By contrast, our work preserves the MapReduce checkpoint/restart fault-tolerance behavior between stages, while also providing pipelining features to support continuous queries, online aggregation, and related interactive functionalities.

It has been noted that parallel database systems have long provided partitioned
dataflow frameworks~\cite{pavlo09}, and recent commercial databases have begun
to offer MapReduce programming models on top of those
frameworks~\cite{aster,greenplum}.  Most parallel database systems can provide
pipelined execution akin to our work here, but they use a more tightly coupled
iterator and \emph{Exchange} model that keeps producers and consumers
rate-matched via queues, spreading the work of each dataflow stage across all
nodes in the cluster~\cite{exchange}.  This provides less scheduling flexibility
than MapReduce and typically offers no tolerance to mid-query worker
faults. Yang et al.\ recently proposed a scheme to add support for mid-query
fault tolerance to traditional parallel databases, using a middleware-based
approach that shares some similarities with MapReduce~\cite{osprey-icde}.

Logothetis and Yocum describe a MapReduce interface over a continuous query
system called \emph{Mortar} that is similar in some ways to our
work~\cite{logoyocum08}.  Like HOP, their mappers push data to reducers in a
pipelined fashion.  They focus on specific issues in efficient stream query
processing, including minimization of work for aggregates in overlapping windows
via special reducer APIs.  They are not built on Hadoop, and explicitly sidestep
issues in fault tolerance.

\emph{Hadoop Streaming} is part of the Hadoop distribution, and allows map and
reduce functions to be expressed as UNIX shell command lines.  It does not
stream data through map and reduce phases in a pipelined fashion.

\subsection{Online Aggregation}
Online aggregation was originally proposed in the context of simple single-table
SQL queries involving ``Group By'' aggregations, a workload quite similar to
MapReduce~\cite{onlineagg}.  The focus of the initial work was on providing not
only ``early returns'' to these SQL queries, but also statistically robust
estimators and confidence interval metrics for the final result based on random
sampling.  These statistical matters do not generalize to arbitrary MapReduce
jobs, though our framework can support those that have been developed.
Subsequently, online aggregation was extended to handle join queries (via the
\emph{Ripple Join} method), and the \emph{CONTROL} project generalized the idea
of online query processing to provide interactivity for data cleaning, data
mining, and data visualization tasks~\cite{ieeecontrol}.  That work was targeted
at single-processor systems.  Luo et al.\ developed a partitioned-parallel
variant of Ripple Join, without statistical guarantees on approximate
answers~\cite{luo-ripple}.

In recent years, this topic has seen renewed interest, starting with Jermaine et
al.'s work on the \emph{DBO} system~\cite{dbo}.  That effort includes more
disk-conscious online join algorithms, as well as techniques for maintaining
randomly-shuffled files to remove any potential for statistical bias in
scans~\cite{jermaine-shuffle}.  Wu et al.\ describe a system for peer-to-peer
online aggregation in a distributed hash table context~\cite{wu-vldb09}.  The
open programmability and fault-tolerance of MapReduce are not addressed
significantly in prior work on online aggregation.

An alternative to online aggregation combines precomputation with sampling,
storing fixed samples and summaries to provide small storage footprints and
interactive performance~\cite{gibbons98new}. An advantage of these techniques is
that they are compatible with both pipelining and blocking models of
MapReduce. The downside of these techniques is that they do not allow users to
choose the query stopping points or time/accuracy trade-offs
dynamically~\cite{ieeecontrol}.

%Unlike much of the work on online aggregation, we do not focus here on statistical guarantees because of the flexibility of the MapReduce programming model.  These guarantees are crafted for specific SQL aggregates like SUMs, COUNTs, and AVERAGEs, and modified to account for processing techniques like the join algorithms used.  The focus of our work here is architectural: to provide ``early returns'' interactions within the powerful scalability and fault tolerance facilities of MapReduce frameworks.  The statistical guarantees from the literature only apply to SQL-style reduce functions; statistical guarantees for other online reducers would need to be developed in a case-by-case basis.  We expect that in many cases users will settle for simply observing changes in the output of a job over time, and make their own decisions about whether early returns are sufficient.

\subsection{Continuous Queries}
In the last decade there was a great deal of work in the database research
community on the topic of continuous queries over data streams, including
systems such as Borealis~\cite{borealis}, STREAM~\cite{stream}, and
Telegraph~\cite{tcq-cidr}.  Of these, Borealis and Telegraph~\cite{flux-ft}
studied fault tolerance and load balancing across machines.  In the Borealis
context this was done for pipelined dataflows, but without partitioned
parallelism: each stage (``operator'') of the pipeline runs serially on a
different machine in the wide area, and fault tolerance deals with failures of
entire operators~\cite{borealisFT}.  SBON~\cite{sbon} is an overlay network that
can be integrated with Borealis, which handles ``operator placement''
optimizations for these wide-area pipelined dataflows.

Telegraph's \emph{FLuX} operator~\cite{flux-ft,flux-lb} is the only work to our
knowledge that addresses mid-stream fault-tolerance for dataflows that are both
pipelined and partitioned in the style of HOP\@. FLuX (``Fault-tolerant,
Load-balanced eXchange'') is a dataflow operator that encapsulates the shuffling
done between stages such as map and reduce.  It provides load-balancing
interfaces that can migrate operator state (e.g., reducer state) between nodes,
while handling scheduling policy and changes to data-routing
policies~\cite{flux-lb}.  For fault tolerance, FLuX develops a solution based on
process pairs~\cite{flux-ft}, which work redundantly to ensure that operator
state is always being maintained live on multiple nodes.  This removes any
burden on the continuous query programmer of the sort we describe in
Section~\ref{sec:continuous}.  On the other hand, the FLuX protocol is far more
complex and resource-intensive than our pipelined adaptation of Google's
checkpoint/restart tolerance model.
