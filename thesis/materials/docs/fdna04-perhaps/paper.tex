\documentclass[twocolumn,10pt]{article}
\usepackage{color}
\usepackage{enumerate}
\usepackage{graphicx}
%\usepackage{parskip}
\usepackage{times}
\usepackage{url}
\usepackage{xspace}
\usepackage{fancyhdr}

%\newcommand{\note}[1]{[\textcolor{red}{\textit{#1}}]}
\newcommand{\note}[1]{}
\renewcommand{\ttdefault}{cmtt}

\setlength{\voffset}{0in}
\setlength{\hoffset}{-0.1in}
\setlength{\headheight}{0pt}
\setlength{\topmargin}{-0.2in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9.2in}
\setlength{\textwidth}{6.7in}

\setlength{\headsep}{0in}

\def\proheader{Draft for review purposes only.  Not for distribution
   or attribution.}
\def\Sys{Network Oracle\xspace}

\pagestyle{fancy}
\chead{}
%\chead{\proheader}
\rhead{}
\lhead{}
%\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\begin{document}
\relax
\title{The Network Oracle}
\author{Joseph M. Hellerstein \and Vern Paxson \and Larry Peterson
  \and Timothy Roscoe \and Scott Shenker \and David Wetherall}
\date{\today}
\maketitle

\begin{abstract}
This paper sets out a high-level research agenda aimed at
building a collaborative, global end-system monitoring and information
infrastructure for the Internet's core state.  We argue that such a
system is beneficial, feasible and timely.  We start by hypothesizing
the benefits of a ``\Sys'' that could answer real-time
questions about the global state of the Internet.  We then argue that
it is actually possible to provide a useful approximation of 
such an oracle today, gathering information from a large number of end
hosts and delivering useful views to each end system.  We further
argue that this can and should be done in a decentralized 
fashion.  We provide an outline of a plan to build such a system: it
employs sensing agents, along with a distributed query and dissemination
engine, and possible attractive end-user applications.  A key point of
our discussion is the timeliness and importance of a 
grassroots agenda for Internet monitoring and state-sharing.  While
significant social and economic barriers to deploying a
centralized, public Internet monitoring infrastructure exist, there are
corresponding social and economic incentives for a collaborative
approach.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Vision}
\label{sec:vision}

In this paper, we set out a broad agenda for the research community to
collaborate in building a global end-system
monitoring and information infrastructure for the
Internet's core state. 

Our discussion begins with a thought experiment.  Setting aside
concerns about social and technical barriers, suppose there existed a
\Sys: a queryable object that any end-system could use to immediately
receive information about global network state, from the recent past
to real-time updates.  This state could include complete
network maps (including addressing realms and NAT gateways), link
loading, point-to-point latency and bandwidth measurements, event
detections (e.g., from firewalls), naming (DNS, ASes, etc.),
end-system software configuration information, even router
configurations and routing tables.

This is considerably more information than is available to end-systems
today.  The existence of the \Sys\ would allow end-systems to make
more sophisticated decisions about every aspect of their interaction
with the network: the parties they communicate with, the routes and
resources they use, and the qualities of the various actors in the
communication chain.  In Section~\ref{sec:forwhom} we give concrete
examples of how end-systems and end-users could benefit from this
information.

%% Contrast with what's done today

How far away from this vision are we today?  Network monitoring is not
a new activity.  Many parties collect significant information in
today's Internet, including carriers and large IT departments.
However, the 
information collected is by no means comprehensive; it is chosen with
relatively narrow goals in mind, usually with a focus on backbone
traffic engineering and academic networking research.  Also, since the
data is typically collected ``in the middle'' of the network, it only
captures packets as they traverse those links; it misses significant
information about the properties and traffic in small Intranets, in
switched subnets of large Intranets,
in WiFi communities, and in similar rich and evolving ``microclimates'' at the
edges of today's Internet.  
% \note{A network administrator for a large corporation tells a story
%   about how they have increasingly deep hierarchies of routers and switches,
%   and can't afford to monitor traffic across the lower-level
%   switches. Hence even in well-provisioned Intranets a lot of the
%   traffic is not monitored.  If we want to add this war story, it
%   needs some polish. -- JMH }  

Furthermore, current network monitoring systems
focus on data collection, but ignore public-access query or
dissemination facilities.  Information gathered by today's network
monitors is generally available neither to end-users nor their
protocols or applications.  This places inherent limits on
innovation.  Making this information widely available in
near-real-time can significantly change the protocol and distributed
system design landscape, in a way that offline centralized analysis
cannot.  Today's Internet was designed under the assumption that it is
not feasible to gather and disseminate such information at scale, and
researchers and developers of end-user network applications constrain
their design space accordingly.  We argue below that this
assumption no longer holds. Eliminating these constraints can open up
new opportunities for significant innovation in network functionality
and robustness.

Recently, a proposal for a ``knowledge plane''~\cite{Clark2003} laid
out a broad vision for an artificial intelligence-based approach to
self-managing networks.  Our model of a \Sys is a more concrete
proposal, with a more specific agenda.  Moreover, it is more
short-term in scope: we argue that it is feasible today. 

The rest of this paper is organized as follows.  In the next section
we describe a number of benefits of the \Sys that help to motivate
the idea.  Section~\ref{sec:whynow} argues that approaching the vision
of a \Sys is now both feasible and important.  In
Section~\ref{sec:whyus} we further argue that it is the research
community, rather than commercial organizations, who are best
positioned to make the vision a reality.  Section~\ref{sec:how}
outlines how this can be achieved, and sets out the principal research
challenges involved.  We conclude in Section~\ref{sec:conclusion}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{What For?}
\label{sec:forwhom}

We conjecture that a \Sys would have multiple benefits for various
parties at differing timescales.  These include (a)
social benefits such as raised awareness of security risks and the
importance of end-system maintenance, (b) medium-term network
engineering benefits including localization of performance problems
and identification of malevolent actors, and (c) long-term design
opportunities including the development of end-to-end network
protocols and distributed applications that leverage the global information.
In this section we present several examples.
%   We will argue this vision presents considerable opportunities 
% for moving networking technology forward from its current, somewhat
% ossified state. 

\vspace{1em}\noindent
{\bf Performance Fault Diagnosis.}  
\note{Wetherall} There are
very few options for diagnosing performance faults within the
Internet today. When a site loses connectivity, it is possible to
use tools such as traceroute to determine whether the problem is
likely local or remote. But when the Internet performs poorly from a
given site it is difficult to determine whether the problem lies
with that site or elsewhere, and whether there is in fact a problem
to be corrected rather than a temporary overload.  The key
difficulty is that there is often no baseline that the site can use
to guage what level of performance it should obtain.

The \Sys we envision addresses this difficulty directly by
allowing a site to compare and contrast its performance with that of
other sites, both in the past and at the given moment in time. For
instance, a site might note suspect destinations for which it obtains
persistently low performance, and query the oracle for these suspect
destinations. If these destinations subscribe to the oracle, then
current and historical information will be available to assess the
overall level of performance of the destination and whether it matches
past performance.  Recent falloffs suggest a problem with the site;
consistent performance suggests a problem elsewhere, and if all other
sites reporting information about the suspect destination are
receiving equally poor performance, then the problem likely lies with
the site itself which is simply overloaded. This same test can be
applied to regions of the network between the site and
destinations. The effect is to narrow the region of performance
faults, facilitating correction, as well as to identify alternative
paths that offer improved performance. It is the sharing of
performance data across sites facilitated by the oracle that makes
this possible.

\vspace{1em}\noindent{\bf Tracking attacks.}
\note{Paxson} Hosts on the Internet experience incessant
attack~\cite{pang_imc_2004}. This engenders an edginess in Internet
users, much of which boils down to questions like: 
	\emph{(i)} Is this remote host attempting to contact me a bad guy?
	\emph{(ii)} Am I being targeted, or just enduring what everyone else endures?
	\emph{(iii)} Is there a new type of attack going on?
	\emph{(iv)} Is something happening on a global Internet scale?

By providing insight into activity experienced by one's peers and
Internet sites in general, the \Sys can help answer questions both
about types of activity, and which hosts have been seen doing what.
This kind of shared information is not only interesting, but
potentially actionable.  As a simple example, studies have
demonstrated that very few source IPs are responsible for generating a
significant fraction of port scans~\cite{yegneswaran-sigmetrics03}.  A
real-time distributed query could fairly easily compute the global
``worst offenders'' list, and allow firewalls at the endpoints to
adaptively change their packet filters to track changes in the list.
Placing the \Sys at the core of a global security agenda raises major
research issues concerning attacks on and subversion of the
infrastructure and its data, and in general ensuring that the
information has an ``actionable'' degree of fidelity.  We return to
this point in Section~\ref{sec:how}. 

\vspace{1em}\noindent{\bf Network Routing Protocols.}
\note{Shenker}
The current routing infrastructure computes routing tables that, to a
first approximation, are applied to all flows headed to the same
destination.  However, it is clear that no uniformly applied routing
protocol, no matter how well designed, can accommodate every flow's
policy and/or performance requirements.  Starting with source routing,
and continuing with more recent designs such as NIRA~\cite{nira} and
TRIAD~\cite{gritterarchitecture}, there is a 
large literature about mechanisms that would allow flows choose their
own route.  Research has typically focused on mechanisms for
expressing and implementing the desired route, but much less attention
has been paid to how flows (or the hosts acting on their behalf) could
determine which route would best serve their needs.  If only a very
small fraction of flows were making such individualized route choices,
then fairly primitive, and bandwidth-expensive, route exploration
mechanisms could be used.  But if source-specified routing became
commonplace, then a more scalable approach would be needed.  In
particular, one would want the information used to decide routes to be shared,
rather then individually discovered.  The \Sys approach, in which
general classes of information are gathered and made available, could
provide a virtual repository for the relevant information.  Initial work (e.g.,
\cite{karthik-routeservice,boon-recursion-tr}) suggests
that such a repository, along with a general querying facility, could
be used for such route computation.


\vspace{1em}\noindent{\bf Adaptive Applications.} 
\note{Peterson} A natural extension of having collected a wealth
of information about the health of the network is for applications to
adapt; to react to this information by selecting alternative protocols,
alternative routes, or even alternative sources for the content they
are trying to access. While adaptation might happen purely at the end
system (e.g., selecting a variant of TCP most suitable for the current
end-to-end path), it is easy to imagine the emergence of
way-stations that help end systems avoid network trouble spots and
rapidly recover from failure~\cite{i3:sigcomm02,ron:sosp01}, as well as more
globally coordinated network services that are able to distribute
network load over a wider swath of the Internet~\cite{codeen,coral}.

\vspace{1em}\noindent{\bf An Internet Screensaver.} 
\note{Hellerstein} Distributed
computation projects like SETI@Home~\cite{anderson_cacm_2002} have
demonstrated that individuals will contribute private computing
resources to the common good
 -- particularly if rewarded with the right combination of
entertainment (e.g., interesting screensavers) and community-building
tools (e.g., 
the SETI ``leader board'' listing the top contributors).  
Given mounting
press and public concern about Internet viruses and worms, the time
seems ripe to build an Internet Screensaver -- a peer-to-peer
application of end-hosts monitoring the network for security events
and performance anomalies.  Such an application could have multiple
benefits.  First, it could serve as an attractive, sizable testbed
for a prototype \Sys, measuring the network from the richness of a
variety of ``last-mile'' endpoints.  Second, if properly designed it
could engender a unique culture of enlightened vigilance, with
client machines swapping notes on anomalous traffic for a variety of
purposes.  For example, end-users could set up social networks for
``community watch'', actively probing each other's machines for
vulnerabilities.  They could swap notes on passively-monitored
undesirable traffic (worms, port-scans, spam), to help configure
firewall rules.  They could compare performance across ISPs.  While
they may not provide the most accurate measurements or the most effective
security measures, these techniques would give Internet 
users insight into their own experience and
incentive to control it more carefully.  
% This kind of
% community education could have noticeable benefits in the network
% security realm -- well-known viruses and worms continue to consume
% significant Internet bandwidth because users do not practice ``safe
% computing''.

\vspace{1em}\noindent{\bf Serendipity.}
\note{Shenker}
While the previous scenarios described practical uses for the
\Sys, its relevance to networking research should not be neglected.
The network measurement literature is huge, and continues to grow at
an astounding rate, so the field is hardly lacking for interesting
questions to ask and relevant data to answer them.  However, much of
the research deals with data that was gathered with the specific
question in mind, or at least in a specific context with limited
scope; for example, a routing study might collect BGP routing tables
but would be unlikely to also simultaneously collect data from
firewalls or application logs at points nearby the relevant routers.
Thus, current measurement studies have a naturally limited ken which
may prevent certain questions from being answered, or even being
asked.  We hope and anticipate that, should it be built, a
general-purpose \Sys could open up surprising new connections -- both
for research and for application.

% \note{Shenker, channelled by Mothy}
% Finally, we note that the vision laid out for the \Sys is very
% general.  While a special-purpose system could conceivably be built
% for many of the scenarios we have described, a general approach makes
% the incremental work needed for any specific application of the oracle
% much lower.  
% 
% More importantly, a general approach aids serendipity in both
% measurements and applications.  For example, researchers will 
% look for unexpected correlations in data (traffic spikes linked
% with intrusion detection warnings, BGP routing table changes linked to
% unusual traffic patterns, etc.).  We hope and anticipate that, should
% it be built, applications for the \Sys will emerge
% unforeseen by the community engaged in building it.  




%% \note{Shenker} one thing I'd like to mention (as a theme, or a
%%   scenario) is "searching for gold".  That is, researchers looking for
%%   unexpected correlations in data (traffic spikes linked with
%%   intrusion detection warnings, BGP routing table changes linked to
%%   unusual traffic patterns, etc.).  For many scenarios, one can
%%   imagine a special purpose system that could work, and our argument
%%   there is that we are building a general platform that makes the
%%   incremental work needed for any specific scenario lower.  However,
%%   one thing that a special purpose system won't help you with is
%%   serendipity.

%%%%%
%%%%% Scenarios:
%%%%% \begin{itemize}
%%%%% \item \note{An intranet measurement example.}  Within a large enterprise network, monitoring and managing the
%%%%%   use of internal BW is a well-acknowledged problem.   Being able to
%%%%%   fuse data from network monitoring and configuration information,
%%%%%   combined with modeling techniques ranging from Network
%%%%%   Calculus~\cite{boudec_netcalc} to Predicate
%%%%%   Routing~\cite{roscoe_hotnets_2002}, can lead to much more effective
%%%%%   capacity planning and fault diagnosis in such networks. 
%%%%% 
%%%%% \item \note {An internet measurement example.}  Internet epidemiology.  Network
%%%%%   telescopes~\cite{caida_telescopes} provide good 
%%%%%   evidence that monitoring the network at multiple addresses helps to
%%%%%   understand the behavior of Internet worms and viruses.
%%%%%   While ``honeyfarms'' extend the acuity of Internet telescopes by
%%%%%   responding to random inbound probes that sniff for live endpoints,
%%%%%   the holy grail for network security monitoring is extensive coverage
%%%%%   of end-hosts.  Pragmatically, a combination of telescopes,
%%%%%   honeyfarms, and end-system monitoring is the right way to go, and
%%%%%   may in fact yield more than any one alone -- even in an ideal world.
%%%%% 
%%%%% \item \note{A protocol development example.}  Overlay networks that try to provide a differentiated form of
%%%%%   service on the network, such path resilience and low
%%%%%   latency~\cite{ron:sosp01} or controllable tradeoffs between loss and
%%%%%   delay~\cite{laksmi_overqos} rely on extensive measurement of network
%%%%%   state to deliver their service (often based on application-level
%%%%%   source routing).   The duplication of such measurement traffic is 
%%%%%   a significant source of network load on deployment platforms such as
%%%%%   PlanetLab~\cite{planetlab:hotnets}. 
%%%%% 
%%%%% \item \note{Something that uses global state for security purposes at
%%%%%   endpoints.  A simple example is auto-generated blacklists, a la
%%%%%   DOMINO.  But perhaps something more sophisticated and time-bound
%%%%%   like a decision to deploy doomsday measures of various sorts based
%%%%%   on patterns of reports and silence from the outside?  This requires
%%%%%   detecting doomsday and reacting before it hits you locally.  The theory
%%%%%   here is that some network clouds will have temporary barriers --
%%%%%   e.g. firewalls -- that don't get jumped for a while even though
%%%%%   sensory data is coming through.  Note that they're highly likely to
%%%%%   get jumped eventually (mobile laptops, trojan horse executions, etc.) so
%%%%%   the risk of doomsday is still critical.}
%%%%% \item \note{End-user education and empowerment.  I.e. the public health
%%%%%   pitch that many people's net problems today are real but
%%%%%   preventable, and increased involvement of individual end-users could
%%%%%   help both the individuals as well as the collective.  Firewall logs
%%%%%   is one example.  Clock jitter + Code Red is another.  Etc.}
%%%%% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Why Now?}
\label{sec:whynow}

The vision of making detailed, relevant information about global
network state available at all endpoints was not realistic several
years ago.  We argue that the vision is now realistic, and is becoming
easier.  

\subsubsection*{The Social Case}
Our first point is social, not technical.  A widespread monitoring
infrastructure will only be achieved (and sustained) if the sensors
are widely desired and deployed.  We believe that the time is ripe for
encouraging end-users to install software to help improve the
Internet.

We are at a point in Internet history where the need
for protective action has hit the common consciousness.  
Spam, viruses and worms have led even
unsophisticated users to take non-trivial technical steps to try and
ameliorate these problems.  Simultaneously, news stories about
identity theft, Internet credit card fraud and the U.S. Patriot Act have
raised popular sensitivity to the importance of the Internet in their
lives.  We believe that many users are not only willing and ready, but
thirsty to install
applications that can improve their trust in the Internet, and better
inform them of risks.

Working against this, of course, is concern with individual privacy --
often argued to be in tension with safety.  A recent 
article~\cite{berinato_futuresec} suggests that the strategic need to
secure the Internet, combined with the ease of surveillance as a tool
for doing so, will lead to an Internet that is a ``broadly surveilled
police state''.  Is it possible for the community to be vigilant
without compromising their privacy?

This can be construed as a technical or social challenge; ultimately
it is both.  The technical agenda in privacy-preserving
information sharing is just beginning, and it may be years before
cryptographic guarantees can be made about the privacy of information
in a \Sys.  We conjecture that the deployment of an early-stage \Sys
need not wait for such technology to mature.  Many users may be
willing to opt into a decentralized \Sys, even if they would not do so
with a centralized system offered by a large organization (commercial
or non-profit), based on the user's presumption that no single
malicious individual would see much of their information.  This
soft attitude toward privacy is surprisingly widespread --
witness the number of people who identify their IP address to unknown
peers in order to acquire copyrighted material illegally, or the often
vigorous participation in 
pseudo-anonymous forums 
such as newsgroups and chat rooms.  The social issue of privacy is
less contentious when large interests (companies, governments,
standards bodies) are removed from the debate.  We explore the
implications of the requirement for decentralization below. 

\subsubsection*{Technology Trends}
Our second point concerns the technical feasibility of disseminating
adequate information about the state of the Internet.  We believe that
technology trends are working in our favor. 

We conjecture that {\em the ``metadata'' of the Internet's behavior is
  shrinking relative to data being shipped across the Internet.}  The
bandwidth required to ship useful measurement data
around the network to end users, as a fraction of the total bandwidth
available to end users, is decreasing.  This makes it more attractive
to start making measurement data available to all end
systems.

We note that while both bandwidth and flow size are increasing,
the data required to describe a flow is not.  Available network
bandwidth is increasing, both in residential (DSL, Cable Modems) and
business settings (100--1000~Mbps Ethernet), a trend  even more
pronounced in technologically advanced countries like South Korea and
Japan.  At the same time, the size of data objects transferred through
these larger pipes is also increasing 
(video streams, large downloads, VoIP sessions, even web-page objects are
becoming larger), while the amount of data required to describe
this activity is staying relatively constant -- it scales with
the number of end systems, rather than the available link capacity.
While there are counter-examples (such as the proliferation of
instant-message traffic), in general the principle
holds: the amount of data one needs to know at endpoints to make
informed decisions is growing more slowly than the access bandwidth at
these end systems.

Moreover, processing power in end systems (indeed, systems anywhere in
the network) is outstripping the increase in available network
bandwidth.  We are a long way from the days when 80\% of the CPU
cycles of an Alto were devoted to running a 3Mb/s Ethernet interface.
A modern IA32 server can comfortably handle a 1Gb/s Ethernet interface
at line rate with most of its CPU cycles to spare.  The conclusion is
that systems can afford to give much more consideration to traffic (in
particular, its temporal characteristics) than is typically assumed in
protocols and implementations.  A corollary is that users with more
cycles to throw at the problem may be capable of extracting more value
(per b/s) from their network link.

\subsubsection*{Technology Innovations}

Our final point is that we are seeing the appearance of
technologies that can take this opportunity and make it useful.  In
recent years the building blocks for delivering a modest but
non-trivial approximation of the \Sys are falling into place, with
contributions from a number of research communities: for example,
content-addressable networks, distributed query processing,
statistical data reduction techniques, and statistical machine
learning techniques.  We return to these in Section~\ref{sec:how}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Why Us?}
\label{sec:whyus}
The research community is uniquely
positioned to realize an initial approximation to the \Sys.  Other
parties have no incentive to pursue this agenda: while
the long-term benefits are significant, neither network providers nor
equipment vendors gain obvious advantage from investing in such an
endeavor at this stage. By contrast, this is an opportunity for
researchers: while the \Sys vision represents a shift in emphasis
for some networking research groups, substantial new research agendas
and synergies exist in this direction. 

Historically, ISPs have been resistant to
the sharing of measurement information, except as marketing and sales
aids.  Where they have instrumented their networks, it has been with
the internal goal of traffic engineering.    Carriers have little
interest (for sound commercial reasons) in disseminating end-to-end
performance or security measurements.   Consequently, equipment
vendors have little interest in the problem; indeed, a shared, global
management infrastructure may be a disruptive technology that
threatens their market position. 

In short, we are in a situation where commercial
benefits of change are indirect but communally valuable, however the
commercial threats are direct.  The \Sys is not going to happen
commercially at first. 
However, the situation is very different for the networking research
community.  In particular, the fields of network measurement and
security have much to gain from realizing a \Sys. 

Internet measurement in academia has been heavily restricted by the forms of
measurement to which it has access, with rising security and privacy
concerns making this increasingly difficult rather than the
situation easing over time.  This often leads to 
%insular
work that is implicitly
driven and shaped by (shrinking) measurement opportunities. 
Internet security research, on the other hand, has struggled
with the rise of rapid, automated attack technology, 
the loss of a defensible ``perimeter'' with mobile Internet
devices, and an inability to adequately track and share
information about miscreants.  Both communities stand to gain
by realizing a \Sys.

The \Sys would act as a rallying point where
unnecessarily divergent research thrusts 
can be brought back together: network measurement,
security, distributed systems, distributed databases, and statistical
methods.  Measurement
researchers would gain access to much larger, shared datasets
than provided by the limited opportunities available to them today.
Similarly, security researchers would gain the ability to directly
tackle problems at global scale, and with global resources. 

Research in turn performs a bootstrapping function.
The \Sys puts monitoring, diagnosis, and measurement
functionality directly into end systems.  If it can demonstrate
value to end users in this way, it provides a path by which many
measurement, monitoring, and diagnostic techniques can achieve a
critical deployment mass without first requiring productization.
Organizations with strategic interests in the deployment of such
techniques can thus derive immense benefit from ``plugging into
the information substrate.'' Products follow deployment, rather
than the other way around.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{How?}
\label{sec:how}
The \Sys will not appear overnight.  Here we highlight some
of the open questions in bringing this vision to light.
%   We first discuss possible architectures and adoption paths for
% the infrastructure.  We then overview a number of technical
% challenges, and evidence that initial solutions to these challenges
% are within reach.

\vspace{1em}\noindent{\bf Adoption and uptake.}
The \Sys is characteristically a shared infrastructure.  
For reasons we have already outlined, we expect, at best, limited data
from ISPs to start with.  Consequently, the \Sys will rely at first on
end-systems for both data sources and applications (consumers of
data).  

Projects like NETI@home~\cite{Simpson+:pam2004} and
DIMES~\cite{dimes:website} are exploring large-scale network
measurement from end hosts, and the DShield
project~\cite{dshield:website} warehouses firewall data sent in from
many sources.  Bundling a sensor with a global query visualization
like an Internet Screensaver seems like a good incentive here, with a
quid-pro-quo opt-in model: for the features you publish, you can see
distributed results involving such features. 

Moreover, another rich source of endpoints is ``dark'' IP address
space, as monitored by Network Telescopes.  While the traffic at these
addresses is idiosyncratic, it is of interest to many parties, and
could serve as ``seed'' data to populate the screensavers of early
adopters.

The \Sys must also be able to locate and interface with large
curated databases of information as well as distributed real-time
sources.  This includes slowly-changing network data (e.g., WHOIS) and
archival data warehouses of traffic information (e.g. RouteViews).

Finally, we note that the definition of end systems expands 
to include large distributed services like P2P networks and CDNs. 
Public-minded instances of these services can share interesting
traffic data with the \Sys, along the lines of
PlanetSeer~\cite{planetseer}. 

\vspace{1em}\noindent{\bf Architecture and deployment.}
We envision a healthy diversity of popular sensors targeted at
different network features, sharing an integrated query processing
backplane.  The function of this backplane is the processing
(filtering, summarization, correlation) of data from many sources, and
the delivery of relevant results to interested end systems.  The
amount of source data involved means that computation needs to occur
``in the network'', along the network path from the data source to the
consumer.

The deployment of this computational infrastructure could evolve
in a number of ways.  As we have discussed, we do not forsee a
centrally administered solution succeeding.
One option is to have a consortium manage a
well-provisioned infrastructure, conceivably federated in nature like
the Internet today.  The recent success of PlanetLab is
encouraging in this regard, but it remains unclear whether a
consortium can maintain a production service like the \Sys without
sustainable, measurable benefit to the institutions hosting the
machines.

An alternative is an organic p2p deployment, with the query
processor being bundled with the sensors.  This is easier to deploy
than the ``distributed platform'' 
approach, and is in an important sense more self-sustaining: the
system remains up as long as sensors are deployed.  It has a populist
flavor that may allay some concerns about privacy and control.
% ; many
% end-users distrust centralized administrative bodies -- even (or
% especially!) non-profit consortia of intellectual instutions.
Of course, robustness in the p2p approach raises many technical
challenges of scale, management, and resistance to attack or
manipulation. 

\vspace{1em}\noindent{\bf Technical approach.}
Challenging as the \Sys may seem from a technical standpoint, we think
most of pieces of the puzzle have fallen into place in recent years,
and a focused research effort could bring together a usable 
system in the spirit of a \Sys in short order.  We present a brief
selection here. 

First, distributed query processing and content-addressable networks
(DHTs) are getting much better at providing the right information to
the right place at the right cost.  P2p systems like
PIER~\cite{huebsch_vldb03} push computation into the
network to reduce the data shipped during query
answering.  A key tenet is the {\em data independence} that
underpins relational databases: the physical
organization (e.g., network location) of data should be separate from
the logical data model and query interface.  Queries can 
be posed on data regardless of its location, and data can be
reorganized without requiring changes in queries or 
applications that embed them.

DHTs are the first technology that provide this kind of
data independence at Internet scale.  PIER's
``flat'' DHT infrastructure is potentially a better fit for the \Sys
than hierarchically organized systems; it does not depend on a small
number of ``roots'' for the information and processing as DNS does, nor
does it restrict the system to queries that traverse the hierarchy (as
does, e.g., Astrolabe~\cite{astrolabe}).

Second, the practical application of statistical methods in systems
has been maturing over the last decade, in particular in computing
approximate answers to queries (e.g.~\cite{newjersey}). %,minos-tutorial
These techniques compute over small statistical summaries (samples,
histograms, wavelets, random projections, etc.) rather than the full
dataset.  Early exploration of these techniques in distributed
settings are promising~(e.g., \cite{gibbons04}).
Also, distributed implementations of techniques like graphical
models are emerging in the machine learning community, largely in the
sensor network space~(e.g., \cite{paskinguestrin}). 
These approaches model statistical
correlations in data, and use the models to predict data values and
quantify uncertainty; this is useful
both for predicting missing data, and for ``cleansing'' noisy acquired data.

Thirdly, the security and trustworthiness of the \Sys implies several
key challenges: validating the fidelity of data and computations,
managing resource consumption at the end-hosts, providing
accountability of misbehaving components, ensuring that the system
itself is not used maliciously as an attack platform, and a viable and
enforceable privacy framework.    Security in p2p
systems has been a topic of interest in recent years, with progress
being made on topics including self-certifying data, secure routing,
and fair sharing of work (e.g.~\cite{wallach02}).

Finally, the research community now has the resources to do non-trivial
test deployments of global-scale systems both in controlled,
repeatable ``laboratory'' settings~\cite{White+:osdi02,
  Vahdat+:osdi02} and more permanently in the real
Internet~\cite{planetlab:hotnets}.

% 
% \vspace{12pt}
% 
% In this paper, we propose to realize the \Sys using a Peer-to-Peer
% approach, and connecting network data sources (including end-systems)
% and clients (applications, overlays, and interested users) using a
% distributed query processor.  We now describe why we believe this to
% be the only feasible deployment path. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{How: Distributed Sensing and Querying}


%% \subsection{Toward New Design Principles}

%% \note{This section is a place to put in some common sense design
%%   principles for designers who would use the system.  The best
%%   criticism of this work is that it would be unwise to depend upon
%%   new, more complex infrastructure -- if it's big and complicated, it
%%   will lower the robustness of the network to failure or attack.  So
%%   some design principles are in order.  For example, \emph{design assuming
%%   that the oracle may not be right, or not be available}. So one design
%%   principle should be ``First, do no harm'': be sure that in the
%%   absence of the oracle, things work as well as they do today.  Another
%%   might be ``Trust but verify'': complement oracle measurements
%%   with local measurements to detect inconsistencies and malicious
%%   misinformation.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Related Work}
% 
% \note{Much of this is already threaded through the paper, or is being
%   done by possible coauthors of the paper. }
% \note{I think this can be skipped, it's adequately sprinkled through
% the paper already. -- VP}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}


In speculating about a \Sys, our goal is not to map out an
unattainable research ideal.  On the contrary, a useful approximation
of the vision is broadly desirable, and eminently feasible.  

A research agenda for the community in this direction must encompass a
range of areas, including overlay routing, query processing and
database management, network measurement techniques, distributed
intrusion detection, distributed statistical methods, and a broad set
of issues in security and privacy.  To achieve impact, this research must be
informed and complemented by a thoughtful strategy for gaining and
sustaining uptake in the system infrastructure. Such an effort would
benefit significantly from the research community rallying around the
problem, seeding the space with interesting data streams and useful
compute resources, and working together toward common techniques and
protocols.

\bibliographystyle{abbrv} 
\footnotesize
\bibliography{paper,rfcs}

\end{document}


%%%% Comments
\section{Comments}
\note{
Main points seem to be:
\begin{itemize}
\item Address Knowledge Plane and related work up front.
\item What do you do with the info?  (Hopefully the new scenarios help
  with this?)
\item Are big distributed systems in the middle (a la Codeen) a viable
  alternative to p2p?
\item Benefits of various time-scales.
\item Call out the research challenges in building it.
\item Set up the Oracle->DQP->P2P argument more strongly.  DQP is
  about flexibility in devising queries.  Can you build a
  general-purpose Oracle instead of many special cases?
\item Tech Push is just DHTs.  Needs more.  Data reduction and
  approximate queries.   Distributed inference.  {\bf Added stuff there.}
\item The role of locality? 
\item Privacy shouldn't rest so strongly on p2p.  Aggregates and the
  like should help.
\item Drop ``Why us''?  Did Vern's rewrite help?
\end{itemize}
}
\onecolumn
\begin{verbatim}
	From: 	  llp@CS.Princeton.EDU
	Subject: 	Re: reconnecting on PHI
	Date: 	June 29, 2004 7:43:55 AM PDT
	To: 	  jmh@cs.berkeley.edu
	Cc: 	  hari@lcs.mit.edu, stuart@nevisnetworks.com, vern@icir.org, dga@lcs.mit.edu, shenker@icsi.berkeley.edu, provos@citi.umich.edu, savage@cs.ucsd.edu, kaashoek@csail.mit.edu, tom@cs.washington.edu, troscoe@intel-research.net

Very nice. I like the Network Oracle take on the problem. I do have some
random comments that I'll just throw out...

1) It may be intentional, but I would argue that you're missing a
   selling point (application) for the oracle: adaptive applications.
   As soon as you provide a tool that helps me isolate problems, I'll
   build a tool that is smart enough to work around (route around) them.
   You may consider this an aspect of "long-term design opportunities",
   but I would call out an example like this in section 3.

   In general, the current paper doesn't say enough about the "react"
   phase of the observe-analyze-react loop.

2) There is a pretty clear distinction made between end systems (which have
   a lot of value yet to harness) and "in the middle" systems (which you
   characterize as the focus of past monitoring efforts). While the paper
   could say more about tapping into existing mid-network sources (I do
   recall a couple sentences to this effect), if you believe there will be
   an increasing number of planetary-scale services deployed, these systems
   will become a new player in the inside/edge scene. My example here is
   CDNs. As Vivek's PlanetSeer demonstrates, CDN nodes are in a unique
   position of being widely distributed (like end nodes) but carrying on
   enough connections (more like an interior node) to witness and be able
   to diagnose far more anomalies than ever before. Well, maybe if you
   could get people to install "NetworkOracle@Home" you'd get this benefit,
   but planetary-scale services could be one of the trends that makes now
   a good time.

3) It might also be useful to talk about time scales. Low-impact (passive)
   monitoring can run continuously, with high-impact/invasive monitoring
   triggered by suspicious events. One of the research challenges is to
   minimize the overhead. To tie this to my first point, there are realtime
   uses for the data being collected. This could use additional discussion.

   In general, the paper could do a better job of calling out the research
   challenges.

4) Selling P2P as the knife that cuts the Gordian knot isn't convincing.
   I think there needs to be a more rigorous and clear progression from
   oracle to distributed query processing to P2P. The first link is treated
   pretty well in 2.2, but the DQP take-away probably isn't as strong as
   it needs to be. The DQP->P2P link needs to be more than the last paragraph
   of the social case section. The Tech Push section is ok, but I'm left
   wondering why, of all the tech pushes you could have talked about, DHT
   is the one you elected to focus on.

   This is the kind of comment I hate to get, and I apologize in advance.
   I'm not sure I can offer any advise on how to make this point stronger,
   but I hope I'll be able to recognize the right case when I see it.

   Now that I think about it more... this was one of the hardest points for
   us to make with Sophia. There are some big-honk'en centralized monitoring
   tools out there (e.g., AOL). We tried to argue that (1) the amount of
   monitoring data available for consumption was orders of magnitude greater
   (it's not just counters, but could even include packet traces), and (2) the
   data wasn't needed at just one NOC, but everywhere (since we were arguing
   that every service deployed on PlanetLab (the Internet) would want to tap
   into this data). This means analysis had to be pushed out to the edges, near
   the sensors, and that additional aggregation needed to happen at many points
   in the middle of the network, not just one NOC.

5) You're going to need to answer the "isn't this just the Knowledge Plane"
   objection at some point. See the Sophia paper for one possible answer:
   KP-AI = InfoPlane.

Larry


	From: 	  tom@cs.washington.edu
	Subject: 	RE: reconnecting on PHI
	Date: 	July 2, 2004 6:32:53 PM PDT
	To: 	  llp@CS.Princeton.EDU
	Cc: 	  jmh@cs.berkeley.edu, troscoe@intel-research.net, djw@cs.washington.edu

I think the oracle - dqp - p2p connection is the crux of the matter.

Starting bottom up, I'd suggest drilling down on several of
the scenarios, to see what would really be needed to solve them.

Specifically, if I think about say, protocols that take advantage
of global knowledge of network state, the vast majority of useful
information is going to be local -- what are my neighbors experiencing?
in certain failure cases, perhaps this will be combined with some data 
about other points of view
to diagnose problems that are in common with my neighbors.

the specific structure of the information needed in this case suggests
a stronger solution to the privacy/publicity dilemma -- I only need to
know aggregate data of performance to regions of the network, not
arbitrary access to flow level data.  

in this case at least, locality cuts the gordian knot, not dht's.
I completely did not buy the argument that a p2p network would
solve my privacy worries -- that somehow I'd rather trust nameless
hordes downloading information about my surfing patterns than
a centralized database?  But I might be willing to trust others
enough to show them generic info about network state that I've collected,
that's abstracted enough to make it impossible to know where I'm surfing
(e.g., seattle -> california via sprint is slow today, vs. seattle -> michael moore
server at the san jose data center is slow today).

The dqp argument is that you need flexibility in devising/updating queries,
vs. just constructing a special purpose data collection/dissemination engine
for a single application.  I would agree with this, but I think that the argument
turns on the speicifics of the variety of applications you envision for this,
and whether you can feasibly build a general purpose, flexible system that
can approach the performance and reliability of a system that takes advantage
of application-specific knowledge (such as that most of the data I need is
local and can be abstracted in the following way).

ps. drop the whole "why us" section.  the first rule of starting a research area:
don't insult your friends, no matter how much it seems like they deserve it.


	From: 	  shenker@icsi.berkeley.edu
	Subject: 	Re: reconnecting on PHI
	Date: 	July 8, 2004 5:28:34 PM PDT
	To: 	  jmh@cs.berkeley.edu, troscoe@intel-research.net

Joe and Mothy,

Thanks for putting this very good draft together and giving us all a much-needed kick-in-the-butt.  I talked with Joe a bit about some of my comments, but I wanted to reiterate three of them here.

(1)  To echo Larry's comment, the draft has to address (right up
front) the Knowledge Plane.  KP may be vaporware, but it has
significant mindshare; the paper will get booted immediately if KP
isn't discussed up front. 

(2)  I loved the words in the draft, but when I got to the examples I
felt they didn't live up to the grand vision that came before.  We
either need much better examples, or we need to tone down the
rhetoric. 

(3)  Given the dual difficulties of editing a document with more
authors than pages, and more vision than specifics, we might do better
to focus first on the example applications that would best inform our
vision (and best communicate that vision to the reader).  This would
both help hone our focus (for instance, Tom mentioned the role of
locality in queries as being important, and that's going to come up
through an examination of examples) and also is something we can
easily discuss in email (as opposed to writing pages of visionary
text, which doesn't make for constructive email exchanges). 

--Scott

	From: 	  llp@CS.Princeton.EDU
	Subject: 	Re: HotNets "Network Oracle" paper.  Action Req'd.
	Date: 	July 23, 2004 7:33:50 AM PDT
	To: 	  jmh@cs.berkeley.edu
	Cc: 	  vern@icir.org, troscoe@intel-research.net, shenker@icsi.berkeley.edu, djw@cs.washington.edu

I prefer "Internet Public Health" (or the permutation that gives you
PHI) for the title, but I like "network oracle" as a rhetorical tool,
at least early in the body of the paper. However, I'm not sure Network
Oracle is the right name for the artifact that the paper argues the
community should collectively build. I'd call it the "Information
Plane" if left to my druthers :-), but am ok with PHI.

Which gets us to related work... This will be (at least) the third
position paper calling for something like this. If we were talking
about a system we'd built, we could list the differences and most
reviewers will be satisfied. I think we need to confront (and even
embrace) the related position papers in the introduction. This
paper has the potential to paint a much more comprehensive and
compelling picture of what we want and how we should go about
getting it. The Knowledge Plane paper was pretty abstract (and had
the AI component that everyone complains about) and the Sophia paper
focused in pretty quickly on a particular approach, without really
exploring much of the space. This paper has much more of a "first principles" feel, and takes you all the way to a plausible roadmap
for a tangible system. I also like it "grass roots" (SETI@home-like)
slant.

Larry

\end{verbatim}


%% ABANDONED TEXT
%% Main issues in getting from here to there
\subsection{The Road Ahead}
\note{The following two paragraphs came from the old ``Vision''
  section at the beginning.}
In order to approach the functionality of a \Sys, two main hurdles
need to be cleared.  First, information must be extracted from enough
elements of the network.  This is particularly challenging because the
vast majority of network nodes are endpoints, which are the hardest
network elements to reach and to control.  In an ideal world, each
component would be equipped with built-in (software or hardware) {\em
  sensors}, which would provide the appropriate state in a standard
format.  Achieving significant deployment of such sensors at
endpoints is an interesting challenge in its own right, and should be
a creative focus for any effort in this direction.  Even significant
success in that regard will not result in universal sensing. In the
absence of specific sensors, there must be an ability to predict or
deduce what would have been sensed in that location. This challenge
can be tackled both via statistical methods based on correlation
models, and via the injection of additional traffic to derive specific
measurements.

Second, a mechanism must be developed to harness all of these data
sources, and compute and deliver information to masses of network
nodes, on demand.  The information being delivered could be based on
global properties, as well as on information from arbitrary collections
of the nodes in the network: subnets, geographic regions, clusters of
similar endpoints (e.g. those running the same OS patchset), etc.  The
simplest solution here would be to disseminate all state to all nodes,
but this is fundamentally infeasible.  Instead, this information must
be computed via a rich integration of distributed query processing,
statistical summarization, and information dissemination techniques.  

\note{The next two paras are from the old ``social argument'' section}

\note{Bundling the sensor with a global query visualization seems like
  an obvious tack here, with a quid-pro-quo opt-in model: for the
  features you publish, you can see the queries involving those features.}

The time is ripe for technologies that can cut this Gordian knot.  A
simple first step in this agenda is to willingly share information
piecewise, rather than to centralize it all in one location.  The
argument here is that -- early on, at least -- many users will be
willing to opt into a P2P-based \Sys system, even if they would not do so
with a centralized system offered by a large organization (commercial
or non-profit).  This is based on people's belief that no one
malicious individual would see much of their own information.
\note{Maybe could add here something like ``witness the often vigorous
participation in pseudo-anonymous forums such as newsgroups and chat
rooms. -- VP}
Over
time, this feeling could hopefully be enhanced by technical solutions
that, for example, could provide strong cryptographic assurances about
the entropy of the signal sent from one peer to another.  In short,
privacy is to a large extent a social issue, and currently one that is
less contentious when large interests (companies, governments,
standards bodies) are removed from the debate.

\note{Something here about ``centralized'' -> ``federated'' -> P2P?}


