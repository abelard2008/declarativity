\documentclass[10pt,twocolumn]{MyTightStyle}
%\topmargin -0.4in
%\textwidth 6.5in

%\textheight 9.0in
%\def\tenrm{\fontsize{10}{12}\normalfont\rmfamily\selectfont}
%\def\BibTeX{{\rmfamily B\kern-.05em{\scshape i\kern-.025em b}\kern-.08em \TeX}}

\usepackage{alltt}
\usepackage{graphicx}
\usepackage{times}
\usepackage{url}
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{authblk}
\usepackage{color}
\usepackage{listings}

\newcommand{\cell}[2]{\parbox[c]{#1\textwidth}{ \smallskip #2 \smallskip}}
\newcommand{\ncell}[1]{\cell{0.2}{#1}}
\newcommand{\wcell}[1]{\cell{0.36}{#1}}
\newenvironment{overlog}{\begin{alltt}\footnotesize}{\end{alltt}}
\newcommand{\ol}[1]{{\tt\footnotesize#1}}


\begin{document}

\title{Auto-Parallelization for Declarative Network Monitoring}

\author[1]{Robert Soul\'{e}}
\author[1]{Robert Grimm}
\author[2]{Petros Maniatis}
\affil[1]{New York University}
\affil[2]{Intel Research Berkeley}


\date{} 
\maketitle 

\begin{abstract}

As distributed computing environments become progressively more complex and dynamic, network monitoring has become increasingly challenging. A typical network requires several monitoring applications performing non-trivial computations with different time and space requirements. The complexity of these applications demands consideration of how to execute efficiently over available resources. Declarative languages offer an attractive alternative to traditional implementations, and open the possibility for static program analysis to exploit opportunities for concurrency. We explore a language-based approach for achieving auto-parallelism through the use of static analysis, and sketch an implementation strategy in the context of the P2 declarative networking system.


\end{abstract}


\section{The Pain of Network Monitoring} \label{pain}


As distributed computing environments grow more complicated and
dynamic, network monitoring is big business, whose complexity
can be broken down along at least three axes.  On the first axis,
network monitoring must handle different tasks, often in the same
enterprise setting. For example, a typical enterprise network will
require a signature-based intrusion detection system such as
Snort~\cite{attig05} or Bro~\cite{paxson99} looking for known exploit
patterns in passing traffic; an anomaly detector such as that described
by Huang et al.~\cite{huang07}, identifying statistical trends in network
metrics (e.g., number of connections, number of end-points) via
techniques such as Principal Component Analysis (PCA), which it uses to find
anomalous deviations and outliers; or even forensic monitoring tasks
geared towards troubleshooting performance bugs~\cite{ArpaciDusseau03} or
intrusions~\cite{King2005}.

Regardless of the intended task, along a second axis
the monitoring complexity is due to the different required operational
patterns. Whereas some tasks are local (e.g., learning a model about a
single computer), many tend to be distributed, such as collaborative
signature detection in Autograph~\cite{kim04} or alarm corroboration via
gossip in DDI~\cite{dash06}. Some tasks can be long-running and
infrequently changing, for instance traffic matrix computation for
routing adaptation, but others are interactive and single-shot, such as
complex on-line forensic analysis of a particular bug or
intrusion~\cite{Tucek2006}. Though in the past, such tasks were performed by
backhauling logs into one place and running local data analysis queries,
increasingly they are performed in-place, by querying individual
firewalls' buffers and capture devices' collectors~\cite{barletros07}.

Finally, to top off the smorgasbord of complexity, one must consider the
third axis of available monitoring infrastructures: distributed PCA
collects and filters data at low-end end-hosts in the enterprise but
runs the expensive computations in a heavy-duty, multi-processing
compute server; Como~\cite{barletros07} runs multiple queries on the same
set of collection/monitoring boxes within an enterprise; Google's
MapReduce~\cite{dean04} and Microsoft's Dryad~\cite{isard07} can be used
to monitor and compute complex statistics over an enterprise by using
large clusters of dual-core blade servers across distributed
datacenters; and hundreds of applications on PlanetLab learn about the
network conditions on which they operate by using CoMon~\cite{park06} and
Oasis~\cite{freedman06} atop loosely coupled virtual machines hosted
across the world.


What is a network monitoring expert to do in this 3D (at least)
complexity design space? Consider the trend extraction example: one must know
all about statistics and the PCA technique, how to adapt the
technique to a distributed environment of end-hosts and a central
compute server, and how to take advantage of the multiprocessing
capabilities of the compute server. Similarly, a network
administrator who wants to do just-in-time forensics with a
computationally expensive, complex data analysis query must either have handy a
particular implementation that does the exact same thing and takes
advantage of available parallelism and 
distribution in her setting, or resort to an inefficient, one-size-fits all
implementation that might cost precious seconds before a countermeasure
is installed.  What is worse, often the task, pattern, and infrastructure
problem must be solved for multiple tasks, multiple patterns, and
multiple infrastructures at the same time.  Although a designer, administrator, or researcher may know
exactly \emph{what} network monitoring analysis he wishes to do, often he has to waste
effort, time, and brain cells on \emph{how} to do it efficiently.




\section{The Promise of Declarative Programming} \label{rescue}

More and more, researchers from several communities, including databases
and programming languages, have suggested that declarative languages can
ease the complexities associated with networked
monitoring~\cite{hellerstein07,wawrzoniak03,roscoe02,loo05,loo06}.
Declarative languages are an attractive option because they operate at a
higher level of abstraction than imperative languages, allowing
programmers to concentrate on the {\em what} and leave the {\em how} to
automated tools or those who care to deal with it.

In a declarative programming framework, the network administrator or
monitoring expert expresses the task at hand in a high-level language;
the system figures out how to adapt that task to a particular
implementation environment. For example, in the distributed PCA
case, what the programmer has to express is the statistical computation (PCA),
the inputs (a number of distributed streams) and the destination of the
result (a human or an anomaly detection agent): whether the computation
is performed on a uniprocessor, a multiprocessor, a cluster, or PlanetLab;
whether those streams are collected at the server and consumed there or
on replicated servers or in-place at their collection points or over an
aggregation topology; whether bits are transmitted via a reliable
messaging tool such as Tibco's and WebSphere MQ's tools or through a TCP
transport mechanism, (and so on), it is all immaterial to 
performing PCA on a bunch of input streams.

In this paper, we argue that the combination of network monitoring and
declarative networking is particularly fortuitous in conjunction with
efficiently using available computational resources. Historically, logic
based languages have been an attractive target for exploiting concurrent
execution~\cite{gupta01}, in multiprocessors or clusters. Because
declarative programs are
closer to pseudocode than to the implementation idioms of any particular
infrastructure type, control and decisions about parallelism
are much more accessible to the compiler. Just as declarative
programming offers the promise of performing static checks for
correctness properties~\cite{feamster05} in networking, it also offers
the opportunity to perform static checks for safe concurrency
and auto-parallelization.

In the remainder, we illustrate how complex network monitoring tasks expressed in one
such  declarative networking environment, the P2 system~\cite{loo05},
can be automatically adapted to available parallel and clustered
computational facilities with the help of static program analysis. Though
we do not claim that the solution presented is readily deployable
tomorrow, we do argue that our techniques are practical, implementable,
and applicable to most network monitoring tasks we have encountered,
allowing a single implementation-unspecific descrption of such a task
to run efficiently on vastly different computational facilities.





\section{P2 and Overlog Overview}


The P2 system~\cite{loo05} combines ideas from the PIER~\cite{huebsch03}
peer-to-peer query engine, and the Click~\cite{kohler00} modular
router. The P2 framework consists of a high-level declarative language,
OverLog, a compiler that parses, statically checks, and translates
OverLog to a dataflow graph (akin to Click dataflow), and a runtime that
executes the dataflow graph on incoming data.

Figure \ref{example} shows a simple set of  monitoring tasks  written in
OverLog. We will use this example to describe the language, and its
semantics, before diving into automatic parallelization in the next
section.

An OverLog program operates on \emph{tuples} of a fixed number and type
of values. A tuple may represent stored state (e.g., an alarm that was
recently issued by some detector, \ol{alarm(...)}) or transient information flowing
between system components (e.g., a packet just received from the
network, \ol{pktIn(...)}). Names starting with \ol{f\_} are
functions (e.g., \ol{f\_now()} gets the current time and \ol{f\_tcpSyn(D)}
checks if D is a packet containing a TCP protocol frame with the SYN
flag set). Words starting with an upper-case letter are variables and
``\ol{\_}'' means ``don't care.''

A program consists of a number of commands (e.g., the
\ol{materialize} statements in the beginning of the example) and rules
(the remaining lines). The \ol{materialize} commands here state that
information about tuples with names \ol{synIn}, \ol{ackIn},
etc., are stored; all other
tuples that appear in the program are transient tuples. \ol{materialize}
also determines the maximum number of tuples to be stored in a table,
the number of seconds before a tuple should be evicted, and the tuple
fields to be used for indexing into storage.

Rules in OverLog are reminiscent of logical implication a la Prolog and
of production rules a la Datalog. The first rule in the example has
identifier \ol{r1}, then the \emph{rule head}  \ol{synIn(@IP, Port, SrcIP, SrcPort,
f\_now())} on the left of the \ol{:-} symbol, and a \emph{rule body} on
the right consisting of a set of conjuncts. The particular rule could be
read  ``when you see a tuple
\ol{pktIn} with 5 fields, the last of which we shall call \ol{D}, and if
you apply function \ol{f\_tcpSyn} on that \ol{D} and it returns true,
then you should generate a tuple called \ol{synIn} with the same first 4
fields as \ol{pktIn} and a last field that is the result of running the
function \ol{f\_now()}.'' In the particular application, this means that
when a packet arrives from some source \ol{SrcIP}:\ol{SrcPort} to a node
at address and port \ol{IP}:\ol{Port} and with payload \ol{D}, if \ol{D}
is a TCP frame with the SYN flag set then a new tuple should be stored
in the table \ol{synIn} with the packet fields and the time of
interception, collecting all incoming SYN packets.

OverLog contains the notion of a \emph{location specifier}, denoted with
the \ol{@} symbol. This determines the node where a particular tuple
should appear. OverLog has no explicit way to express ``send a tuple
somewhere''; instead, if a newly generated tuple has a location
specifier different from the tuples used to generate it, that new
tuple should ``land'' at the appropriate network location.  For example,
rule \ol{r8} says that when a packet arrives at node \ol{IP}, and its
payload matches the SLAMMER signature (the \ol{f\_matchSlammer}
function), and node \ol{IP} has in a local table called
\ol{globalDetector} a tuple with some \ol{G}, then the
\ol{alarm} tuple described in the head should ``land'' at the node whose
address is \ol{G}.

\begin{figure} 
\begin{overlog}

materialize(synIn, infinity, 172800, keys(2,3)).
materialize(ackIn, infinity, 172800, keys(2,3)).
materialize(synackOut, infinity, 172800,
   keys(2,3)).
materialize(globalDetector, infinity, 10, keys(2)).
materialize(alarm, infinity, 172800, keys()).

r1 synIn(@IP, Port, SrcIP, SrcPort, f_now()) :-
   pktIn(@IP, Port, SrcIP, SrcPort, D),
   f_tcpSyn(D).

r2 rstIn(@IP, Port, SrcIP, SrcPort, f_now()) :-
   pktIn(@IP, Port, SrcIP, SrcPort, D),
   f_tcpRst(D).

r3 synackIn(@IP, Port, SrcIP, SrcPort, f_now())
   :- pktIn(@IP, Port, SrcIP, SrcPort, D),
   f_tcpSynAck(D).

r4 ackIn(@IP, Port, SrcIP, SrcPort, f_now())
   :- pktIn(@IP, Port, SrcIP, SrcPort, D),
   f_tcpSynAckAck(D).

r5 synackOut(@IP, Port, DstIP, DstPort, f_now())
   :- pktOut(@IP, Port, DstIP, DstPort, D),
   f_tcpSynAck(D).

r6 alarm(@G, IP, Port, SrcIP, SrcPort, "CONNRST",
   f_now()) :-
   rstIn(@IP, SrcIP, SrcPort, TRST),
   synIn(@IP, Port, SrcIP, SrcPort, TSYN),
   synackOut(@IP, Port, SrcIP, SrcPort, TSYNACK),
   not ackIn(@IP, Port, SrcIP, SrcPort, TACK),
   TSYNACK > TSYN, TACK > TSYNACK,
   TRST > TSYNACK, globalDetector(@IP, G).

r7 alarm(G, @IP, Port, SrcIP, SrcPort, "UNSOLICITED",
   f_now()) :-
   synackIn(@IP, Port, SrcIP, SrcPort, TSA),
   not synOut(@IP, Port, SrcIP, SrcPort, TS),
   TS < TSA, globalDetector(@IP, G).

r8 alarm(@G, IP, Port, SrcIP, SrcPort,
   "SLAMMER", f_now()) :-
   pktIn(@IP, Port, SrcIP, SrcPort, D),
   f_matchSlammer(D),
   globalDetector(@IP, G).

r9 alarm(@G, IP, Port, SrcIP, SrcPort,
   "SSH", f_now()) :-
   pktIn(@IP, Port, SrcIP, SrcPort, D),
   f_peekSSH(D),
   f_matchSSHExploit(D),
   globalDetector(@IP, G).

r10 windowCnt(@G, T, a_COUNT<*>) :-
   periodic(@G, E, 86400),
   alarm(@G, IP, Port, _, _, A, T),
   alarm(@G, _, _, _, _, A', T'),
   T' in (T - twindow, T),
   T in (f_now() - 86400, f_now()).

r11 gAlarm(@G', G, T, Count) :-
   windowCnt(@G, T, Count),
   Count > theshold,
   globalDetector(@G, G'), G != G'.
\end{overlog}
\caption{A sample monitoring application in Overlog.} 
\label{example} 
\end{figure} 


Going back to the example, rules \ol{r1} through \ol{r5} perform simple
signature detection on incoming packets, looking for TCP SYN, TCP RST,
TCP SYNACK, TCP ACK (as response to a 3-way TCP handshake). Rules
\ol{r3} and \ol{r5} both look for SYN ACK, the former in incoming
packets while the latter in outgoing packets. Such signature detections
are used subsequently to identify more complex patterns.


Rule \ol{r6}, is triggered whenever a TCP RST is received for an
origin-destination pair \ol{SrcIP}:\ol{SrcPort}-\ol{IP}:\ol{Port}; when
this happens shortly after an incomplete TCP handshake (i.e. a reception
of a SYN, followed by the transmission of a SYN/ACK, followed by a RST
without having received in  between an ACK to conclude the
handshake) an alarm is triggered of type ``CONNRST.'' This is similar to
the pattern identified by Lee et al.~\cite{lee98}, indicating a port
scan. Note that the alarm is sent to a global detector as discussed earlier.
 Rule \ol{r7} monitors for a similar suspicious TCP pattern, unsolicited
 SYN/ACKs. Rule \ol{r9} checks for two signatures at the same time: the
 presence of an SSH payload within the packet and, at the same time a
 particular SSH exploit signature in that payload; checking for the
 exploit signature in isolation might conceivably trigger even for
 non-SSH packets, causing a false positive.

Rule \ol{r10} looks at stored collected alarms from individual local
detectors; it counts alarms detected anywhere during any sliding window
of duration \ol{twindow} (a constant), over alarms collected during the
prior day.  The \ol{periodic} tuple in the beginning of the rule is
special. It denotes that this rule is to be triggered once every day
(86400 seconds).   The resulting tuple, \ol{windowCnt} then triggers a
thresholding rule, rule \ol{r11}, which checks if the count found for a
given window (defined by its starting time) exceeds the constant
\ol{threshold}.  If it does, the rule disseminates this ``global'' alarm
to all other global detectors. This mechanism is a simplification of the
corroboration mechanisms in DDI~\cite{dash06}.




\subsection{Operational Semantics}

Now that the example is fully explained, we briefly discuss the
operational semantics of P2: how this specification should be executed
according to the definition of the language.

At every node running an OverLog program there is a persistently stored
database (all stored tables) and an event queue (received but
undelivered messages, periodic timers, and state change callbacks).
The basic execution building block is the \emph{single-event atomic
  fixpoint}.

At each step, an event is picked from the event queue (the
\emph{external} queue), and a
fixpoint computation begins for that event, in multiple iterations. At each iteration,
all rules that trigger given the current event are executed, and their
actions (rule heads), if any were generated, collected. Among those,
non-stored tuples with the local location specifier are put in a local
fixpoint queue (the \emph{internal} queue); those with a remote location specifier are buffered
until the end of the fixpoint; those for local stored tables
are buffered until the end of the fixpoint. As long as the internal
queue still contains tuples, the same fixpoint proceeds with further
iterations. When the internal queue is depleted, all buffered 
remote tuples are pushed to the outgoing network queue to be delivered,
and all buffered tuples for stored tables are atomically placed into the
local state. This concludes a single fixpoint. Another event is pulled
from the external queue and a new fixpoint computation begins.

A single-event atomic fixpoint has the following properties. First, the
triggering external event and all rules within the same fixpoint 
see the same system state; no stored table changes during a single
fixpoint's iiterations. Second, all actions deduced are applied atomically at the 
end of the iteration, as if they ocurred on the same time instant.

For example, when a \ol{pktIn} event is handled by a node in our
example, in the same fixpoint first rules \ol{r1}, \ol{r2}, \ol{r3},
\ol{r4}, \ol{r8} and \ol{r9} will be triggered in the first
iteration. If rules \ol{r1}, \ol{r4} result in new tuples, they will be
buffered because they correspond to materialized tables. If rules
\ol{r8} and \ol{r9} result in new tuples, they will be buffered because
they have a different location specifier. The remaining rules generate
local, internal tuples, leading in further iterations for the same
fixpoint. If rule \ol{r2} generates an \ol{rstIn}, it will trigger rule
\ol{r6} and if that produces an \ol{alarm}, it will be buffered until
the end of the fixpoint. Similarly for rule \ol{r3} leading to
\ol{r7}. When no more internal tuples remain unhandled, any buffered
\ol{alarm} tuples will be sent to the network, and any buffered
\ol{synIn} and \ol{ackIn} tuples will be atomically stored in the local tables.

The current implementation of the P2 scheduler runs one fixpoint, to
completion, at a time.


\section{Extracting Parallelism}


We turn now to the basic challenge we seek to address: identifying and
exploiting inherent parallelism in declaratively specified network
monitoring applications.

Logic languages (or logic-like languages including Datalog and OverLog)
are {\em single assignment}, which relieves the burden of tracking
certain types of flow dependencies. In purely logical languages, rules
may be executed in any order without altering the semantics of a
program. In OverLog, with its stored state and production rule-like
semantics, the challenge is to find opportunities for concurrent
execution that do not violate fixpoint atomicity and higher-level consistency.


We use the sample monitoring code in Figure~\ref{example} to elucidate
the kind of static program analysis that can pinpoint parallelization
opportunities\footnote{When discussing logic based languages, there is a
distinction to be made between \emph{forward chaining} and
\emph{backward chaining}. Forward chaining starts with a set of facts,
and keeps applying production rules to generate more facts; OverLog and
Datalog are forward-chaining languages.  Backward chaining, on the other
hand, starts with a goal (a fact to be proven), and looks backwards
through the rules that might produce that fact, leading to subgoals that
should be true for that to happen, until it reaches known facts.  Prolog
is a backward-chaining language.  Though we focus on forward-chaining
parallelization opportunities, where appropriate, we point out where a
backward-chaining opportunity might aise as well.}. We define three
types or parallelism, which we briefly define here, and discuss in
detail in the sections below.

\begin{itemize}
\item {\em inter-fixpoint parallelism} identifies opportunities 
for multiple fixpoint computations to proceed at the same time
\item {\em intra-fixpoint parallelism} identifies computations 
that can proceed in parallel within a single fixpoint
\item {\em data parallelism} occurs when there is a single 
instruction stream operating on multiple data sets, such as 
applying the same operation on every item in a list
\end{itemize}

We implement this static
analysis using the {\em xtc}~\cite{grimm06,hirzel07} toolkit.


\subsection{Inter-fixpoint Parallelism}

Inter-fixpoint parallelism identifies opportunities for multiple
fixpoint computations to proceed at the same time. In the sample
application in figure~\ref{example}, a fixpoint computation begins when
a \ol{pktIn} event is removed from the external event queue. This leads to a
series of deductions, resulting in the possible execution of rules \ol{r1}, 
\ol{r2}, \ol{r3}, and \ol{r4}. If rules \ol{r1}, \ol{r3}, or
\ol{r4} are executed, then the resulting event is stored in a
materialized table. If rule \ol{r2} is executed, then the rule \ol{r6}
may also be executed during this fixpoint computation, resulting in a
store in a remote materialized \ol{alarm} table. 

Recall from the operational semantics of P2 that every fixpoint
computation will result in potentially both tuples queued for
transmission, or a bunch of state changes in local tables (in general,
those may be insertions, deletions, or updates of existing tuples). All such actions are buffered until the end of the fixpoint, and then committed. 

In a systematic way that mirrors how we described the example fixpoint,
the static analyzer computes the transitive closure of all rules
potentially involved in the computation of a single fixpoint, by
following events from rule body to rule head recursively, until either a
stored tuple or a remote tuple are encountered, terminating the
recursion. All tuples encountered during such a traversal can be
partitioned into three sets: unstored tuples are inconsequential and
ignored, stored tuples encountered in rule bodies are
collected in a {\em read set} for this fixpoint, and stored tuples in
rule heads are collected in a {\em write set} of the fixpoint. In the
example, the read set for the fixpoint of \ol{pktIn} consists of the
tuples \ol{synackOut}, and \ol{ackIn}. The write set consists of the
tuples \ol{synIn}, and \ol{ackIn}.

In other words, the write set consists of all written stored tuples, and the
write set consists of all read stored tuples from local state. Two fixpoints are completely
independent if they do not overlap in their read and write
sets. Independent fixpoints can
therefore be safely parallelized. 




\subsection{Intra-fixpoint Parallelism}

Intra-fixpoint parallelism identifies computations that can proceed in parallel within a single fixpoint. In P2, this means either of two cases. First, it could be executing multiple rules triggered by the same event at the same time. Second, it could mean executing multiple computations on the same rule body or multiple ways to derive matches.

The former case is fairly straightforward. In the P2 implementation, a fixpoint's internal event queue will have multiple events pending execution at any given point in time. Since state does not change in the read-only environment of a fixpoint computation, all events can proceed in parallel provided there are available worker threads.

The latter case is more challenging. Consider rule \ol{r9}, which is
scanning data packets for SSH exploitation attacks. The function \ol{f\_peekSSH()} is used to determine if the data is an SSH frame in the
TCP connection. The function \ol{f\_matchSSHExploit()} checks to see if
the payload matches the known signature of the exploit. We only want to
raise the global \ol{alarm} if both functions return true. These
functions may be run in parallel, providing intra-fixpoint
parallelism. This can be significant when such functions may be
expensive, e.g., digest computation functions, cryptographic functions,
or statistical computations.

Again, we can modify our Overlog interpreter to check for these conditions using static analysis. We examine the tuples or expressions involved in the execution of a rule, and check to see if they write to any common variables.

In logic programming, intra-fixpoint parallelism might also mean either of two cases. First, it could mean pursuing multiple rules with the same antecedent (rule head) at the same time. Alternatively, it could mean pursuing satisfactions of multiple terms in the same rule body.

Parallel execution of logic programs on a single machine has been the focus of much previous research from the language research communities. The two types of parallelism that are most commonly explored are {\em Or-parallelism}~\cite{gupta01} and {\em And-parallelism}~\cite{gupta01}. Or-parallelism is achieved when multiple branches of the same search tree are explored in parallel, in order to satisfy a query. And-parallelism is the concurrent execution of goals in a rule body. A special case of And-parallelism occurs when subgoals do not share any variable bindings. This is known as Independent And-parallelism, and is the type of intra-fixpoint parallelism which is most similar to the P2-style of executing multiple computations on the same rule body.




\subsection{Data Parallelism}

Up until this point, we have discussed two types of {\em control} parallelism, identifying which computations can proceed in parallel on a local machine. However, there is another type of parallelism that is often required for network monitoring. Data parallelism occurs when there is a single instruction stream
operating on multiple data sets, such as applying the same operation on every item in a list. 

This is the type of parallelism exploited by MapReduce~\cite{dean04},
and it is simple for programmers to explicitly encode this kind of
parallelism into logic programs. However, while one can clearly write an
application as a {\em map-reduce} instance, the more difficult question
of how to identify data parallelism in non-map-reduce applications
remains. For example, consider rule \ol{r11} in the sample code.  Rule
\ol{r11} executes daily, and checks to see if the number of alarms in a
sliding window of time exceeds a threshold value. We can clearly
partition the table \ol{alarm} into row groups and run rule \ol{r10}
\ol{windowCnt} on individual rows in parallel. This is much like loop
unrolling in an optimizing compiler, except there is no semantics-implied ordering.

The solution to identifying implicit data parallelism requires more than
static analysis. Besides tracking the size of the datasets operated on,
the system requires an estimate of the latency incurred by transferring
pending state and inputs to cooperating processes in the local case, or
cooperating nodes in the distributed case, to determine if the
parallelism will be beneficial.


\subsection{OverLog Scheduler}

Having identified how static analysis can identify opportunities for parallelization, we now discuss how to expose that information to the P2 runtime environment.

Recall that the current implementation of the P2 scheduler removes a
single event from the external event queue, and runs one fixpoint, to
completion, at a time. To support inter-fixpoint parallelism, we must
modify the scheduler to support multiple internal event queues at the
same time, and remember at any point in time which events' fixpoints it
is executing. When a processing engine becomes available, the next event
is removed from the external event queue and looked up in the table
built by the static analyzer to identify its read and write sets. Those
are compared to the unioned read and write sets of all currently
executing events. If no conflict exists, the event is assigned to a
processing engine and starts executing. If a conflict exists, the
available engines remain idle until a running fixpoint concludes, at
which point the read- and write-set independence is reexamined.

When completed, fixpoints are committed in the order in which their
events were pulled from the external event queue, to ensure typical
serialization guarantees.

Intra-fixpoint computation can be supported by the addition of a pool of
worker threads for on-going fixpoint computations. We need to modify the
P2 runtime by adding a structure that pushes worker threads to fixpoints
that need work done. Data parallelism can be similarly supported with a
pool of worker threads, and a cost function.

Note that we take a very conservative approach to
serializability. Optimistic approaches that fill up all processing
engines but may subsequently abort some speculative computations are
also possible, though we omit the details here.


\section{Related Work}

Exploiting concurrency for logic programs has been the focus of much research. A survey of the techniques for the Parallel Execution of Prolog programs can be found in~\cite{gupta01}. Notable systems that exploit And-Parallelism include \&-Prolog~\cite{hermenegildo90} and \&-Ace~\cite{gupta97}. Although this paper focuses on static analysis, it is worth mentioning that there is an opportunity for achieving finer grained parallelism through the use of run time checks and modifications to the language. Examples of this include 
the \&-Prolog~\cite{hermenegildo90} system which introduces the parallel conjunction operator \& that is used instead of the comma operator when explicit parallelism may be executed. 
Our prototype implementation aims to provide parallelization to the OverLog language, however our approach is not platform specific. Sophia~\cite{wawrzoniak03}, and InfoSpect~\cite{roscoe02}, which have both demonstrated the applicability of logic based languages to network monitoring, could benefit from static analysis for auto-parallelization.

Safe At Any Speed~\cite{jannotti06} uses static analysis to ensure that any parallelism is concurrency-safe by default. Our approach differs in that we apply the static analysis to a higher level declarative language, while they apply it to lower level atomic building blocks, called handles. Compile time analysis is used to determine which global variables are reachable by a handle. If two handles can access the same global variable, then they are said to be in conflict, and may not be run at the same time. 


\section{Conclusions and Outlook}

We have identified several important limitations to our approach. First, given the number of existing monitoring applications, it would be admittedly difficult to migrate them all to a declarative networking framework. On the other hand, we feel that our approach of declarative networking and static analysis significantly lowers the cost of entry for new monitoring applications.

Currently, we have only studied static analysis for applications written entirely in OverLog. An interesting challenge will be to see if we can provide similar safety guarantees for the concurrent execution of programs that have been written in languages like C or C++, which have been imported into the P2 framework. Perhaps a combination of exporting annotations, static analysis, and runtime support can extend the scope of applications we support.

We plan to implement an OverLog version distributed intrusion detection of Huang~\cite{huang07}, which will provide a litmus test for our approach.


{\footnotesize
\bibliographystyle{abbrv}
\bibliography{main}
}

\end{document} 

