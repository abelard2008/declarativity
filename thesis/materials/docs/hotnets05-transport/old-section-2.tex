



% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Stuff removed from previous 2, some will find its way back}

\paragraph{Routing alternatives:} when sending a message through an
overlay network, the system typically has a number of valid
alternative next-hop choices.  This is in contrast to client-server
applications, where the client is generally restricted to talking to a
single server. 

Overlay networks present a number of different routing metrics that go
well beyond simple point to point communication. The GIA system is an
overlay network based on the Gnutella protocol that designates high
capacity nodes as super peers. A super peer is a node that holds a local
index of search terms used to answer lookup queries for data maintained
by the overlay. Route selection for search terms is done using a random 
walk, biased toward the high capacity neighboring nodes, where
they are most likely answered. The protocol is designed to make the Gnutella
protocol more scalable, since traditionally search queries are flooded to 
all neighboring nodes.

DHT overlay networks also present some alternatives in the selection of nodes
(hops) that a message traverses to the ultimate destination. The
primary metric for next hop route selection is maximize the key space distance
without overshooting the destination. Although such a metric minimizes the 
number of hops taken from source to destination, it does not mean the message
will traverse the path with the lowest latency. The reason being that node placement
in the overlay network is often done in a purely random fashion, e.g., taking the
SHA-1 of the node IP address. Therefore, the ideal next hop (one that maximizes
the distance in key space) could point to a node half way around the globe, where a
next hop slightly before the ideal in the key space could be within the same LAN.

Route selection may also depend on message requirements demanded by
the application. Many secure routing algorithms have the requirement
that a message traverse a diverse set of paths on its way to the
destination. The goal being to ensure that at least one replica of the
message make it to the destination with high probability. That is, by
maximizing the diversity in path selection we minimize the chances
that all replicas are intercepted by intervening malicious nodes. To
perform redundant routing in an overlay network, a node will often
replicate the message to some number of neighboring nodes. However,
this does not mean that the messages traverse diverse paths in the
underlining topology (e.g., the IP topology) or that the messages
won't converge through a single overlay node prior to the ultimate
destination.


\paragraph{Per-hop computation:} overlays tend to perform contribution
on messages on each hop, ranging from relatively simple statistics
gathering or network measurement to complex calculations such as
in-network data aggregation.  For this and other reasons, many such
applications perform per-hop acknowledgements rather than, or in
addition to, an end-to-end acknowledgement.  Indeed, the whole notion
of ``end-to-end'' becomes harder to pin down in such networks, since
it is often ambiguous where the end-points for communication actually
are. 

\paragraph{Lots of buffering:} being software processes, nodes in an
overlay network have large buffer resources available.  Furthermore,
since each hop in a multi-hop path might involve non-trivial data
transformations that migh result in compression or expansion of data
volumes, buffering might be semantically imposed by the application;
examples could be in-network aggregation, network coding, signed streams
with delayed-disclosure keys~\cite{Perrig2000}, etc.

A number of tradeoffs come into play when determining buffer placement
in a system. For instance, having buffers close to the inputs of the system
means that less work is wasted when dropping messages on buffer overflows,
or simply under heavy loaded conditions. However, this results in other problems 
such as head of line blocking scenarios or, even worse, receiver livelock. Buffer placement on the outputs can lead to easier code at the expense of wasted work on
message drops and increased latency through the system 
(examples?? Click or SEDA maybe).


\paragraph{Message-oriented:} communication in overlay applications is
typically message-based, and messages often show relatively little
locality in terms of the paths they take through the system. This is
in contrast to flow-oriented communication in web, email, and
centralized IM applications.  P2P communication in this sense is much
more ``connectionless'' than typical TCP-based systems.

\note{Perhaps discuss an example in detail here: such as Bamboo.}

\note{Monolithic TCP not the right answer}In an overlay, communication
between a source and a destination proceeds hop-by-hop over successive
application-layer connections.  A host typically has several
alternatives for its next application-layer hop (e.g., different
candidate nodes for a given Chord finger entry).  If the first choice
becomes undesirable --- perhaps because a transmitted packet is stuck in
a sending queue too long or is dropped on its first try due to
congestion --- typical TCP behavior would back off and retransmit the
packet until the next-hop overlay node eventually gets it.  However, in
an overlay the ultimate goal is that the destination get the packet, not
every node along a particular path to that destination.  Using standard
TCP for application-layer routing in such an environment is the wrong
thing to do.  For example, at the first sign of trouble with its
original next-hop choice, a sender may wish to try a different next-hop
node, instead of retrying with its original choice until success.
Although the TCP components are all important here (congestion control,
retransmissions, in-order delivery), this example suggests that a
different way of wiring these components together might be desirable
sometimes.

\note{Differences of opinion} Congestion control is recommended and
often required for communication between two endpoints performing 
many message exchanges. The traffic between two endpoints of an overlay 
network vary in amount and duration, and therefore would benefit from different
CC algorithms and organizations. For instance, a node that is departing
the network may need to offload its data to a neighbor, resulting in a large 
transfer in a short amount of time. A receiver based congestion control algorithm
would work well here since we'd like to quickly ramp up to the capacity of the 
network in a TCP friendly fashion. Another example is a node that simply refers 
to some other node in its neighbor table. Such a connection would likely be 
long in duration that may periodically send heart beat or lookup messages. 
It is unlikely that such traffic would be overwhelming to the receiver and therefore
would be best served by a sender (e.g., window) based CC algorithm. Yet 
another example occurs when a node wants to perform iterative routing, as done 
in Chord. In this case the source node does not know which node it will be
communicating with beyond the first hop. Communication that is short in both
amount and duration communication does not match well to any particular CC 
algorithm. The reason being that all CC algorithms require priming, which is
done through some kind of network probing. However, such probing is futile if
only to send a few messages. 

The priming of a CC component can be solved through the placement of CC 
in the dataflow. Placement of the CC layer before multiplexing ensures that communication to each overlay neighbor is TCP friendly. The median session 
time for a neighbor has been measured on the order of minutes, more than 
enough time to prime any CC element. The interesting case occurs when we
place the CC element after multiplexing. The Chord folks do this because....
Using the dataflow abstraction we're able to guide messages through a
CC algorithm and placement based on message context.

\note{Application semantics determines positioning} Buffering before
marshaling.  Causes the queue to be pushed as early as possible before
transmission.

\note{Tweakability} Knowing what queries/workload I'm currently running
can affect how I set my retransmission timers.


\paragraph{Multiparty communication:} \note{JMH added, speculative.}
Traditional transport focuses on point-to-point communication,
particularly session-oriented pt-2-pt.  Many overlay applications --
and indeed the underlying overlay network maintenance protocols --
communicate in patterns more akin to flooding, multicast and incast
than point-to-point.  Involves 1:n, n:1 and m:n communication
patterns, with membership in the communication being based on
transient or logical properties rather than static
addressing. Examples include query dissemination and response in p2p
apps, but also overlay maintenance -- e.g. connectivity gossiping,
\note{add more.}  \note{Be sure to thread this in later --
  e.g. w.r.t. CC and Mux?}

\note{Tyson added, follow on from Joe}
CC can be either sender or receiver based. Both methods work well for point-to-point
communication but one may be preferred over the other when dealing with 1:n, n:1, 
or m:n communication. Traditional TCP-like CC is sender based and usually
implemented with some kind of windowing protocol. A sender based CC element
will react to any congestion detected at a receiver, which makes it inappropriate 
for 1:n or m:n communication. The reason being that a sender based CC element 
provides no isolation between multiple receivers. The sender based CC element will
react to any one receiver that signals congestion, reducing its overall sending rate
regardless of the destination.

The solution used by Bamboo is to allocate a sender based CC element per receiver.
Are there any problems with that? Perhaps, the overhead involved with having a
CC element per receiver.

A receiver based CC element is more appropriate for 1:n communication patterns.
Receiver based CC, like TFRC, are traditionally based on
rates. The receiver provides feedback to the sender, which enables the sender to 
compute a proper sending rate to the receiver. The ability to store a rate per
receiver provides isolation, in that a reduction in any one receiver rate does 
not affect the rates of other receivers. On the other hand, a receiver based CC 
element would not be appropriate for n:1 or m:n communication since the 
rate computed by the receiver represents an aggregate. It is unclear how the
receiver should divvy up this aggregate rate to the possible senders since the
receiver may not know all the senders or their communication requirements, nor does
a sender know the other senders. 

The m:n communication pattern presents some new issues since neither sender or
receiver based protocols handle it well. One solution would be to use one sender
based CC element per destination, as done in Bamboo. However, this brings up
the issue of whether this is too much overhead. An ideal solution would group
receivers that lie behind the same bottleneck link and allocate a sender
based CC element per group. One way to do this is through dynamic allocation and
coalescing of CC elements. Assume we start with one sender based CC element.
If congestion is detected by a particular receiver, instead of reducing the window
of the single CC element, we dynamically allocate a new CC element, drop its window,
and assign it to that receiver. Now given a set of CC elements, if we notice 
that certain CC elements exhibit similar congestion patterns, we can coalesce them
into a single CC element. Allocating new CC elements on congestion gives us 
receiver isolation, while coalescing CC elements reduces overhead. 

\paragraph{Dataflow graph}
Traditional transport layer architectures are implemented as a stack
based dataflow.  
Each message traverses a single path through a series of layered protocol 
modules before being placed on the wire. Buffers are often placed at the inputs
and outputs of each layer for message hand off. The stack based dataflow 
organization has a number of drawbacks. The main drawback being that a
message must traverse all layers prior to being sent on the wire or received
by the application. If buffers are placed at each layer then a message
that does  
not require some protocol support (e.g., retry) will be delayed at
that layer's I/O  
buffers for no good reason. Another drawback to strict layering is in
code complexity.  
In our previous example, the 
fact that certain packets traverse modules that do not require the
service performed 
by the module, means that packets must be tailored and code be written
to special case  
these situations. A better approach would organize the dataflow around
the  traffic that traverses it. Overlay networks have a number of
different types of 
messages that would benefit from having dataflow graphs tuned to
specific message 
types.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Requirements}
\label{sec:requirements}

In this section, we try to look at what kinds of properties a
transport protocol for P2P overlays might actually need.  Most of these
properties were illustrated implicitly in the examples of the previous
section.

\paragraph{Componentized:} All examples illustrate that having the
transport functionality broken up into bits allows a separation of
concerns that can be invaluable, as per the following. 


\paragraph{Recombinable:} The retransmission example
illustrates that it is important to be able to change the policy with
which retransmission makes use of the bulk-transfer components of the
transport (e.g., to accommodate multiple routing choices).  The
multiplexing example illustrates that different co-existing applications
may well require different ways of performing this wiring.

\paragraph{Separable:} The queueing example illustrates that
judicious violation of layering isolation can be beneficial.

\paragraph{Deeply inspectable:} Good to know queue
lengths or when something has been dropped.



\subsection{Traffic Classification}

The problem of handling certain traffic based on the type of data it carries has been
studied extensively in the QoS literature. A variety of traffic types traverse overlay
networks, with each having particular priorities and service requirements. For instance, maintenance traffic is crucial in environments that exhibit high churn among participant
nodes, but unimportant in small relatively static environments (e.g., LANs). Search
queries can be replicated along multiple paths for security or to enable consistency
measurements on the results returned. Security policies may deem
particular messages be routed iteratively vs. recursively to ensure traffic is not routed 
via some domain. For these and many other examples, overlay networks require a 
transport layer that is flexible in both features and configurability. We believe that the 
dataflow abstraction is the right way to provide this flexibility and meet the requirements 
of today's network applications.



\subsection{Substitution of elements}
\label{sec:substitution}
The previous section described changes to the transport layer behavior through
the modification of the dataflow graph. The main benefit is that individual elements
need not be rewritten but rather reorganized, such that the element semantics 
produce alternative goals in the new arrangement. Replacement of individual
elements for new, and possibly improved, versions is not prohibitive. On the contrary, 
the very nature of elements as being small micro-kernels, enhances the overall development cycle. As long as the individual element semantics remain the same, 
new elements can simply replace old ones in the new dataflow graph, without further
code modification. 

Entire families of elements can be developed, whereby an element family defines
a particular set of semantics on its members. One example of an element family 
is in the congestion control algorithms, where we find both sender and receiver 
based implementations. Since the semantics of congestion control remain the 
same (send a packet at a TCP-friendly rate), it does not matter to the remaining 
elements which of these versions are used. 
This is not to say that performance may vary depending on which congestion control
element is used. In fact we have noticed that a receiver based congestion control
performs much better than a sender based in a lossy environment. This has been
observed by others and has led to the preference of receiver based congestion
control in wireless network environments. Sender based congestion control is more
common in wired environments due to the low loss rates and added security in 
sender based versions. 

Overlay networks add yet another environment to the list. One that can exhibit a
variety of loss rates and latency measurements. Depending on the type of environment 
the overlay network lives in (e.g., LAN or WAN) a particular congestion control algorithm 
may be preferred. In P2, we are able to take in to account application and network
subtleties, and substitution the appropriate transport layer dataflow graph.
 
