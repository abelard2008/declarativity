\documentclass[twocolumn,10pt]{article}
\usepackage{times,url}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage[medium,compact]{titlesec}

\usepackage{alltt}
\usepackage{url}
% Type1 fonts please!
\usepackage[T1]{fontenc}
%\usepackage{times,courier,mathptmx}
\usepackage{times}
\usepackage{textcomp}

\usepackage[tight]{subfigure}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{epsfig}
\usepackage{cite}
\usepackage{color}
\usepackage{xspace}

\begin{document}


\section{Hierarchical BFT with assisted agreement}

Basic idea is to partition the replicas into smaller groups to reduce
the overhead. When the smaller replica groups fail, switch to the whole group
PBFT to perform the operation by  getting help from the other group: this is why it is called assisted agreement. This falls into 
our safe hints architecture since as long as the smaller group does not loose liveness, performance
improves.

Naming convention: G-PBFT is group based PBFT. N-PBFT is the original PBFT where every replica is part
of the same PBFT protocol.

\paragraph{Assumptions and variables} 

\begin{enumerate}

\item{} application state composed of objects 

\item{} operations on different objects can progress in parallel

\item{} N is the total number of replicas

\item{} F is the total number of simultaneous faults

\item{} Q is the size of quorum in N-PBFT. Q = 2F+1.

\item{} $|$G$|$ is the size of group. Q$_{G}$ is the size of quorum in a group G.

\end{enumerate}
% S is the size of response

\paragraph{Intuition} PBFT can be parameterized for different levels of 
fault tolerance. For example, PBFT can provide safety and liveness 
as long as $F \le N/3$. Also, PBFT can be configured to provide
safety for $F \le 2N/3$ but looses liveness as soon as $N/6$ replicas
become faulty. This parameterisation determines the quorum size selected during
protocol execution.


\paragraph{Group size} We form 2 groups, each of size N/2. We configure
each group to provide safety as long as upto 2/3-rd of the group is faulty. 
This means that even if all the faults F appear in one of the group, we 
still have the safety but liveness may be lost for that group. In fact, depending on the fault
distribution, it is possible that both groups loose the liveness though still being safe.
Q$_{G}$ $>$ 5/6.$|$G$|$. %, S = (F+1).

\paragraph{Group size: II} We form 3 groups, each of size (F+1). Q$_{G}$ = $|$G$|$ = (F+1). We do not 
go into this setting for the first cut. 

\subsection{Liveness recovery requirement} All committed requests in any
group are visible to at least Q replicas. This is because our liveness recovery protocol can make progress
only if each committed request is present in those many replicas.


\subsection{Request Handling} Each group starts the processing of the request
as per PBFT. We only modify the ``execute'' phase. 
During execute phase, each group needs to do additional processing before
reflecting the request into the application state. Each
replica in a group needs to propagate information about the requests being
committed in their group to members outside of the group. It then waits to receive
an ACK from at least $k$ nodes outside of its own group. Value of $k$ is chosen
such that total number of nodes that know about the requests committed are of size
(2F+1). A replica outside the group sends an ACK only after receiving matching
commit requests from a quorum Q$_{G}$ from the other group.

\subsection{Liveness failure detection} Similar to the condition for view change.
Each non-faulty replica receives every request and 
starts a timer. If the request is not executed by the group responsible for the
object before the timer expires, liveness is probably lost in that group. A 
non-faulty replica initiates ``liveness recovery phase'' for the faulting group by
sending such a message to the whole group.


\subsection{Liveness Recovery Mechanism} Similar to view change protocol. 
As soon as one of the group fails, we switch to the whole group PBFT for all
groups. Each non-faulty replica, upon detecting that a given group
has lost its liveness, broadcasts a message SwitchGroup(G1, G2, P, Q', C, i). 
Here, G1 represents the failed group's identifier, G2 the identifier of the group
to fall back, P the set of requests that committed or prepared in G1, Q' the set of
requests that pre-prepared in G1 and C the latest checkpoint in G1 and i represents
the identifier of the replica.

Upon receiving a quorum of 2F+1 such SwitchGroup messages, the primary of the
big group initiates a liveness recovery. Members of the failed group, upon receiving
such quorum of SwitchGroup messages leave the group, i.e., do not handle any further
requests in their group. All replicas participate in the liveness recovery phase.

\paragraph{Primary}
Primary of the bigger group has more responsibility, similarly to the role of new
primary in the traditional view change protocol. First, primary identifies the latest
stable checkpoint (C) for the failed group, by observing the SwitchGroup message
from 2F+1 replicas. Then, it identifies all requests that committed after the latest
checkpoint by observing the set P present in the 2F+1 replicas. Finally, it identifies
all pre-prepared requests in the failed group after the last stable checkpoint by
looking at the Q' set. Finally, for all sequence numbers, primary runs the N-PBFT protocol.

\paragraph{Invariants} For safety, we need to ensure that following two invariants hold:
\begin{enumerate}
\item{} No two requests get the same sequence number in any group, across view changes.
\item{} A request, once committed in a group at a sequence number i, remains committed at the 
same sequence number across group and view changes.
\end{enumerate}


\paragraph{Informal correctness argument} Invariant 1 holds from the traditional view change
protocol of PBFT protocol. Invariant 2 also holds due to the fact that all requests that 
committed in a group are visible to at least a quorum of 2F+1 replicas, irrespective of the
group. At least one non-faulty replica's SwitchGroup message will contain such a request and
a non-faulty primary will re-execute the request at the same sequence number. 


%\paragraph{Alternate strategy} A better strategy might be to only switch to the whole group PBFT for
%the failed group and keep using the smaller groups working as it is. We are 
%focussing only on the simple protocol right now.

\subsection{Cost of group change protocol}
Similar to view change protocol.

\subsection{Benefit: Cost reduction}
Number of messages sent in first 3 phases of PBFT = G + G.G + G.G, execute
phase takes G.(N-G).2 messages since group members need to send the commit
messages to outside group members and get an ACK. So, overall, we have
G + 2G.G + 2.N.G - 2.G.G = G + 2.N.G. If G=N/2, we send N/2 + N.N messages.
With N-PBFT, we send N + N.N + N.N = N + 2N.N. So, we get a factor of 2 
benefit. 

\section{Fault distribution}

\subsection{Fault free}
Achieve a factor of 2 reduction in overhead.

\subsection{Uniform faults}
%If overall we have F faults in the system, then both groups may have upto F/2 faults 
%each. Original PBFT is both live and safe. However, we have 2 cases to consider depending
%on whether primary is one of the faulty node.
Assume for now that primary is not the faulty replica.
Let f$_{l}$ represents the number of faults in the small group that once happen, sacrifice
the liveness in the small group. Let f represent the actual number of faults in the whole system.
Note that f$_{l}$ $\le$ G/6 $\le$ N/12.

\begin{enumerate}
\item{\textbf{0 $\le$ f $<$ 2.f$_{l}$}} In this configuration, G-PBFT in both small groups is alive and safe.
So, G-PBFT provides factor of 2 reduction in number of messages.

\item{\textbf{2.f$_{l}$ $\le$ f $\le$ F}} In this setting, G-PBFT in both groups will loose liveness while N-PBFT would not.
So, under these many faults, N-PBFT is preferable.
\end{enumerate}

\subsection{Non-uniform}
Suppose one group experiences f$_{1}$ faults while other experiences f$_{2}$ faults and f$_{1}$+f$_{2}$
$\le$ F. 

\begin{enumerate}

\item{\textbf{0 $\le$ f $<$ f$_{l}$}} In this configuration, both G-PBFT are safe as well as live. Hence, we achieve
factor of 2 reduction in this setting.

\item{\textbf{f$_{l}$ $\le$ f $<$ 2.f$_{l}$}} In this configuration, at most one G-PBFT group looses liveness.

\item{\textbf{Else}} If all faults are located in one group, then that group would loose liveness. However, both G-PBFT may loose liveness, even though depending on precise fault distribution.

\end{enumerate}

\section{Remaining questions}

Questions:

1. When to switch back to G-PBFT after liveness recovery? Switching from G-PBFT to N-PBFT is simple, i.e. when liveness
is lost in the smaller group. However, it is not clear when should we switch back to G-PBFT. If we switch immediately after "liveness recovery phase" and there are faulty replicas in the smaller group, we might have to recover liveness again and this might be costly compared to using the N-PBFT for processing the future requests for the smaller group. However, if there are no faulty replicas in the smaller group, reverting back to the smaller group is better. 


2. How does partitioning of state works? What about transactions?



\end{document}
