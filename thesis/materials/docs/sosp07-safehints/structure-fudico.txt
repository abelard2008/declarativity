
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Structure of Fudico-III talk: Take I  %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Motivation:

	(a) Byzantine fault tolerance: lot of attention recently, rightly so
	
	(b) Improving performance is the primary goal of the recent research
	    - Exploit optimism and/or make clients do part of the work, usually
	      a new protocol is introduced
	    - However, performance dependent on many practical issues: workload, timer settings,
	      faults at networking layer, faulty clients, faulty replicas etc
	    
	 (c) Our goal is to "not" introduce a new protocol from scratch, but to
	    - Apply well known "optimization" principles to the existing BFT protocols
	      at the same time not loosing the attractive safety properties
	    - Identify a methodology or framework to introduce newer or existing optimizations
	       --- Something what "Opimizer" does in System R automatically (is this true?)


Non-goals

	(a) These are not lower level optimizations, e.g., C code optimizations
	(b) Worst-case performance of the protocols remains same or becomes worse due to 
	    some amount of extra work needed to handle wrong optimizations.
	(c) Theoretical bounds still apply, as is true for any existing protocols.

Optimization Techniques from Literature

	(a) Optimize for the common case: compilers inline functions frequently visited
	(b) Optimistic speculation: prefetching in OS, branch prediction in hardware pipelining
	(c) Relax specification: consistency in distributed FS (CODA, Bayou)
	(d) Avoid redundant work: caching everywhere

Optimization 1: Optimistic speculation

	(a) Hope that no-one is faulty
	(b) Allow a replica to serialize request stream going to a Quorum protocol
		- Quorum protocol especially vulnerable to faulty clients/non-ideal workloads
	(c) Speculation is on the fact that serializing replica is "non-faulty"
		- Speculation could be incorrect: in that case, switch to different one
		- Safety is preserved since speculator is similar to a faulty client (do not need extra 
			mechanisms)
		- If correct, protocol performance is close to the best-case performance, independent of 
		  contention in workload, faulty clients


Optimization 1: Optimize for the common case

	(a) Fault-free is the "common" case for BFT protocols
	(b) Agreement protocols: PBFT [OSDI'99]
		- Multiple phases, need to generate MAC for a quorum of replicas (2N/3+1)
		- Crypto ops increase with (8/3b)*N (with batching of size b)
	(c) Optimization: Partition the work among replicas (Divide and Conquer)
		- Small groups perform "most" of the work
		- Crypto operations increases with (4/3b)*N 
		- However liveness may be lost under failures 
			-- need a safe way to recover liveness (similar to view change)
			-- Need for a separate mechanism to do that
	
Opimization 3: Relax specification/guarantees

	(a) Well known to distributed systems community
	(b) We relax consistency gurantees but keep inconsistency error bounded (TACT)
	(c) Relaxing consistency is harder to adopt: different semantics 
		- Still useful for numerical applications
		- Examples: Spam prevention via quotas [NSDI'06], Resource Sharing using SHARP [SOSP'03]q
	(d) High level idea: replicas respond unilaterally until local error is violated
		- Once reached the bound, synchronize using RSM
		- However, additional checks necessary to gaurantee safety of bounded error
		- Need for a separate mechanism to do that

Optimization 4: Avoid redundant work

	(a) Caching is based on this principle (web caching, disk block caching, etc.)
	(b) Avoid crypto operations where not strictly needed
		- When primary proposes an ordering in Agreement protocols, it does need to authenticate
		  itself
		- Safety provided by the protocol (No need for separate mechanism)
		- When speculator serializes requests in our first optimization, it does not need to
		  authenticate itself

Ongoing work: is there a least-common-denominator (LCD)among these optimizations that can be factored out?
Can we isolate them in such a way that we can call our techniques as a framework to introduce safe
optimizations in BFT domain?

LCD so far:
	(a) Need to ensure safety is not violated as well as a safe way to recover from
	    faulty optimizations. (Thats where our safety validator comes)
	(b) A component to suggest what to do in each optimization (optimizer?)
	(c) Base BFT protocol may/will change.



Summary: "Safe Optimizations for BFT Protocols" seems like a better term. 

Note: (1) Protocol Switcher appears like a meta optimization where we pick a given optimization based
on the cost-benefit analysis. 		
(2) Our hope/claim to keep base BFT protocol unchanged seems to not fit this model.


Newer ideas:
(1) Can we remove MAC from the fast path and use some cheaper mechanism?
(2) Introduce MAD in the picture: how are BFT relevent to these systems?
(3) We know that PBFT can be configured to tolerate higher faults and still have safety
	- BFT protocols are criticized for having a fixed bound on faults
	- Once that bound is breached, at best can get fork* consistency when upto 2/3rd are faulty
	
