\documentclass[twocolumn,10pt]{article}
\usepackage{color}
\usepackage{enumerate}
\usepackage{graphicx}
%\usepackage{parskip}
\usepackage{times}
\usepackage{url}
\usepackage{xspace}
\usepackage{fancyhdr}

%\newcommand{\note}[1]{[\textcolor{red}{\textit{#1}}]}
\newcommand{\note}[1]{}
\renewcommand{\ttdefault}{cmtt}

\setlength{\voffset}{0in}
\setlength{\hoffset}{0in}
\setlength{\headheight}{0pt}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9in}
\setlength{\textwidth}{6.5in}

\setlength{\headsep}{0in}

\def\proheader{Intel Confidential. Not for distribution or attribution.}
\def\Sys{Network Oracle\xspace}
\def\Lrp{Phi\xspace}

\pagestyle{fancy}
\chead{}
%\chead{\proheader}
\rhead{}
\lhead{}
%\cfoot{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\begin{document}
\relax
\title{\Lrp LRP White Paper}
\author{Timothy Roscoe and Joseph M. Hellerstein\thanks{Parts of this
    paper are modified from a 
    vision paper entitled ``The Network Oracle'' by Joseph M. 
    Hellerstein, Vern Paxson, Larry Peterson, Timothy Roscoe, Scott
    Shenker, and David Wetherall~\cite{hellerstein_oracle}. This
    paper will be available shortly as an Intel Research
    technical report.}} 
\date{\today}
\maketitle

\begin{abstract}
This Whitepaper provides an introduction to the background and goals
of the \Lrp project.  We articulate a vision of networking where much
more information about internal network state is delivered to end
systems than today, and provide several examples of what this might
enable.  We then discuss the role of the networking and security
communities in bringing this vision about, and the critical part
played by \Lrp in this process. 

We describe the background of the \Lrp project, including recent
developments in distributed systems and databases that have made the
idea plausible.  This is followed by the technical approach we plan to
take, and discussion of the principle research challenges: multiquery
optimization, data fidelity, and algorithmic taxonomy.  We finish by
addressing some of the non-technical aspects of the project: which
related projects we expect to acquire data from and collaborate with,
and how we see \Lrp evolving after the termination of the project. 

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Vision}
\label{sec:vision}

Our discussion begins with a thought experiment.  Setting aside
concerns about social and technical barriers, suppose there existed a
``\Sys'': a queryable object that any end-system could use to immediately
receive information about global network state, from the recent past
to real-time updates.  This state could include complete
network maps (including addressing realms and NAT gateways), link
loading, point-to-point latency and bandwidth measurements, event
detections (e.g., from firewalls), naming (DNS, ASes, etc.),
end-system software configuration information, even router
configurations and routing tables.

This is considerably more information than is available to end-systems
today.  The existence of the \Sys\ would allow end-systems to make
more sophisticated decisions about every aspect of their interaction
with the network: the parties they communicate with, the routes and
resources they use, and the qualities of the various actors in the
communication chain.  In Section~\ref{sec:forwhom} we give concrete
examples of how end-systems and end-users could benefit from this
information.

%% Contrast with what's done today

How far away from this vision are we today?  Network monitoring is not
a new activity.  Many parties collect significant information in
today's Internet, including carriers and large IT departments.
However, the 
information collected is by no means comprehensive; it is chosen with
relatively narrow goals in mind, usually with a focus on backbone
traffic engineering and academic networking research.  Also, since the
data is typically collected ``in the middle'' of the network, it only
captures packets as they traverse those links; it misses significant
information about the properties and traffic in small Intranets, in
switched subnets of large Intranets,
in WiFi communities, and in similar rich and evolving ``micro-climates'' at the
edges of today's Internet.  

Furthermore, current network monitoring systems
focus on data collection, but ignore public-access query or
dissemination facilities.  Information gathered by today's network
monitors is generally available neither to end-users nor their
protocols or applications.  This places inherent limits on
innovation.  Making this information widely available in
near-real-time can significantly change the protocol and distributed
system design landscape, in a way that offline centralized analysis
cannot.  Today's Internet was designed under the assumption that it is
not feasible to gather and disseminate such information at scale, and
researchers and developers of end-user network applications constrain
their design space accordingly.  We argue below that this
assumption no longer holds. Eliminating these constraints can open up
new opportunities for significant innovation in network functionality
and robustness.

\textbf{The role of Intel Research, and the \Lrp project in
particular}, is key in bringing this about, and facilitating the
wider research community to collaborate in building a global
end-system monitoring and information infrastructure for the
Internet's core state.  Specifically, there is currently no
technical solution to the problem of allowing millions of end systems
to efficiently issue complex queries across millions of sources of
network data in near-real-time. 

The \Lrp project will address this by:
\begin{itemize}
\item Defining a set of \textit{protocols} as the basis for a
planetary-scale dataflow engine suitable for federating many
heterogeneous sources of network data
\item Building a software artefact implementing those protocols, and
\item Deploying an initial instance of this dataflow engine on the
  Internet, federating real-time data sets provided by our
  collaborators in the network measurement and security communities
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation}
\label{sec:forwhom}

We conjecture that a \Sys enabled by the \Lrp project would have
multiple benefits for various parties at differing timescales.  These
include (a) social benefits such as raised awareness of security risks
and the importance of end-system maintenance, (b) medium-term network
engineering benefits including localization of performance problems
and identification of malevolent actors, and (c) long-term design
opportunities including the development of end-to-end network
protocols and distributed applications that leverage the global information.
In this section we present several examples.
We will argue this vision presents considerable opportunities 
for moving networking technology forward from its current, somewhat
ossified state. 

\vspace{1em}\noindent
{\bf Performance Fault Diagnosis.}  
\note{Wetherall} There are
very few options for diagnosing performance faults within the
Internet today. When a site loses connectivity, it is possible to
use tools such as traceroute to determine whether the problem is
likely local or remote. But when the Internet performs poorly from a
given site it is difficult to determine whether the problem lies
with that site or elsewhere, and whether there is in fact a problem
to be corrected rather than a temporary overload.  The key
difficulty is that there is often no baseline that the site can use
to guage what level of performance it should obtain.

The \Sys we envision addresses this difficulty directly by
allowing a site to compare and contrast its performance with that of
other sites, both in the past and at the given moment in time. For
instance, a site might note suspect destinations for which it obtains
persistently low performance, and query the oracle for these suspect
destinations. If these destinations subscribe to the oracle, then
current and historical information will be available to assess the
overall level of performance of the destination and whether it matches
past performance.  Recent falloffs suggest a problem with the site;
consistent performance suggests a problem elsewhere, and if all other
sites reporting information about the suspect destination are
receiving equally poor performance, then the problem likely lies with
the site itself which is simply overloaded. This same test can be
applied to regions of the network between the site and
destinations. The effect is to narrow the region of performance
faults, facilitating correction, as well as to identify alternative
paths that offer improved performance. It is the sharing of
performance data across sites facilitated by the oracle that makes
this possible.

\vspace{1em}\noindent{\bf Tracking attacks.}
\note{Paxson} Hosts on the Internet experience incessant
attack~\cite{pang_imc_2004}. This engenders an edginess in Internet
users, much of which boils down to questions like: 
	\emph{(i)} Is this remote host attempting to contact me a bad guy?
	\emph{(ii)} Am I being targeted, or just enduring what everyone else endures?
	\emph{(iii)} Is there a new type of attack going on?
	\emph{(iv)} Is something happening on a global Internet scale?

By providing insight into activity experienced by one's peers and
Internet sites in general, the \Sys can help answer questions both
about types of activity, and which hosts have been seen doing what.
This kind of shared information is not only interesting, but
potentially actionable.  As a simple example, studies have
demonstrated that very few source IP addresses are responsible for generating a
significant fraction of port scans~\cite{yegneswaran-sigmetrics03}.  A
real-time distributed query could fairly easily compute the global
``worst offenders'' list, and allow firewalls at the endpoints to
adaptively change their packet filters to track changes in the list.

\vspace{1em}\noindent{\bf Network Routing Protocols.}
\note{Shenker}
The current routing infrastructure computes routing tables that, to a
first approximation, are applied to all flows headed to the same
destination.  However, it is clear that no uniformly applied routing
protocol, no matter how well designed, can accommodate every flow's
policy and/or performance requirements.  Starting with source routing,
and continuing with more recent designs such as NIRA~\cite{nira} and
TRIAD~\cite{gritterarchitecture}, there is a 
large literature about mechanisms that would allow flows choose their
own route.  Research has typically focused on mechanisms for
expressing and implementing the desired route, but much less attention
has been paid to how flows (or the hosts acting on their behalf) could
determine which route would best serve their needs.  If only a very
small fraction of flows were making such individualized route choices,
then fairly primitive, and bandwidth-expensive, route exploration
mechanisms could be used.  But if source-specified routing became
commonplace, then a more scalable approach would be needed.  In
particular, one would want the information used to decide routes to be shared,
rather then individually discovered.  The \Sys approach, in which
general classes of information are gathered and made available, could
provide a virtual repository for the relevant information.  Initial work (e.g.,
\cite{karthik-routeservice,boon-recursion-tr}) suggests
that such a repository, along with a general querying facility, could
be used for such route computation.

\vspace{1em}\noindent{\bf Adaptive Applications.} 
\note{Peterson} A natural extension of having collected a wealth
of information about the health of the network is for applications to
adapt; to react to this information by selecting alternative protocols,
alternative routes, or even alternative sources for the content they
are trying to access. While adaptation might happen purely at the end
system (e.g., selecting a variant of TCP most suitable for the current
end-to-end path), it is easy to imagine the emergence of
way-stations that help end systems avoid network trouble spots and
rapidly recover from failure~\cite{i3:sigcomm02,ron:sosp01}, as well as more
globally coordinated network services that are able to distribute
network load over a wider swath of the Internet~\cite{codeen,coral}.

\vspace{1em}\noindent{\bf An Internet Screensaver.} 
\note{Hellerstein} Distributed
computation projects like SETI@Home~\cite{anderson_cacm_2002} have
demonstrated that individuals will contribute private computing
resources to the common good
 -- particularly if rewarded with the right combination of
entertainment (e.g., interesting screensavers) and community-building
tools (e.g., 
the SETI ``leader board'' listing the top contributors).  
Given mounting
press and public concern about Internet viruses and worms, the time
seems ripe to build an Internet Screensaver -- a peer-to-peer
application of end-hosts monitoring the network for security events
and performance anomalies.  Such an application could have multiple
benefits.  First, it could serve as an attractive, sizable testbed
for a prototype \Sys, measuring the network from the richness of a
variety of ``last-mile'' endpoints.  Second, if properly designed it
could engender a unique culture of enlightened vigilance, with
client machines swapping notes on anomalous traffic for a variety of
purposes.  For example, end-users could set up social networks for
``community watch'', actively probing each other's machines for
vulnerabilities.  They could swap notes on passively-monitored
undesirable traffic (worms, port-scans, spam), to help configure
firewall rules.  They could compare performance across ISPs.  While
they may not provide the most accurate measurements or the most effective
security measures, these techniques would give Internet 
users insight into their own experience and
incentive to control it more carefully.  
% This kind of
% community education could have noticeable benefits in the network
% security realm -- well-known viruses and worms continue to consume
% significant Internet bandwidth because users do not practice ``safe
% computing''.

\vspace{1em}\noindent{\bf Serendipity.}
\note{Shenker}
While the previous scenarios described practical uses for the
\Sys, its relevance to networking research should not be neglected.
The network measurement literature is huge, and continues to grow at
an astounding rate, so the field is hardly lacking for interesting
questions to ask and relevant data to answer them.  However, much of
the research deals with data that was gathered with the specific
question in mind, or at least in a specific context with limited
scope; for example, a routing study might collect BGP routing tables
but would be unlikely to also simultaneously collect data from
firewalls or application logs at points nearby the relevant routers.
Thus, current measurement studies have a naturally limited ken which
may prevent certain questions from being answered, or even being
asked.  We hope and anticipate that, should it be built, a
general-purpose \Sys could open up surprising new connections -- both
for research and for application.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Why Us?}
\label{sec:whyus}

\note{Retool this section so as to articulate why it is researchers
  who will build the \Sys, and it is \Lrp that supplies the missing
  part of the puzzle.}

Why \Lrp?  This question can be answered in two parts.  Firstly, the
networking research community is uniquely positioned to realize an initial
approximation to the \Sys.  Secondly, the \Lrp team is particularly
suited to catalyze this effort through the \Lrp project, by developing
the missing required piece of technology.  

Other parties than the research community have no incentive to pursue
the \Sys agenda: while  
the long-term benefits are significant, neither network providers nor
equipment vendors gain obvious advantage from investing in such an
endeavor at this stage. By contrast, this is an opportunity for
researchers: while the \Sys vision represents a shift in emphasis
for some networking research groups, substantial new research agendas
and synergies exist in this direction. 

Historically, ISPs have been resistant to
the sharing of measurement information, except as marketing and sales
aids.  Where they have instrumented their networks, it has been with
the internal goal of traffic engineering.    Carriers have little
interest (for sound commercial reasons) in disseminating end-to-end
performance or security measurements.   Consequently, equipment
vendors have little interest in the problem; indeed, a shared, global
measurement and diagnostic infrastructure is a disruptive technology that
threatens their market position. 

In short, we are in a situation where commercial
benefits of change are indirect but communally valuable, however the
commercial threats are direct.  The \Sys is not going to happen
commercially at first. 
However, the situation is very different for the networking research
community.  In particular, the fields of network measurement and
security have much to gain from realizing a \Sys. 

Internet measurement in academia has been heavily restricted by the forms of
measurement to which it has access, with rising security and privacy
concerns making this increasingly difficult rather than the
situation easing over time.  This often leads to 
%insular
work that is implicitly
driven and shaped by (shrinking) measurement opportunities. 
Internet security research, on the other hand, has struggled
with the rise of rapid, automated attack technology, 
the loss of a defensible ``perimeter'' with mobile Internet
devices, and an inability to adequately track and share
information about miscreants.  Both communities stand to gain
by realizing a \Sys.

We aim to create a rallying point where
unnecessarily divergent research thrusts 
can be brought back together: network measurement,
security, distributed systems, distributed databases, and statistical
methods.  Measurement
researchers would gain access to much larger, shared datasets
than provided by the limited opportunities available to them today.
Similarly, security researchers would gain the ability to directly
tackle problems at global scale, and with global resources. 

Research in turn performs a bootstrapping function.
The \Sys puts monitoring, diagnosis, and measurement
functionality directly into end systems.  If it can demonstrate
value to end users in this way, it provides a path by which many
measurement, monitoring, and diagnostic techniques can achieve a
critical deployment mass without first requiring productization.
Organizations with strategic interests in the deployment of such
techniques can thus derive immense benefit from ``plugging into
the information substrate.'' Products follow deployment, rather
than the other way around.

The \Lrp project is the mostly likely catalyst for the vision to come
about.  Building an approximation to the \Sys involves
interdisciplinary research expertise in distributed data management,
networking, security, and distributed systems.  We know of no other
research institutions (other than our collaborators) working on a
similar project.  Furthermore,
the \Lrp team can build on and learn from extensive in-house
experience with PlanetLab~\cite{planetlab:hotnets}, peer-to-peer
systems such as 
Bamboo~\cite{rhea_usenix_2004}, distributed query processing using
PIER~\cite{huebsch_vldb03}, data reduction techniques such as
principal component analysis applied to network measurement
data, etc.  We discuss the additional opportunities
provided by close collaboration with other Intel Research projects
such as CoMo below. 

Finally, realizing the concept and building an information plane
for the Internet involves a significant degree of external
collaboration and hegemonizing around the vision.  This is exactly the
kind of activity that Intel Research LRPs are good at. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Technical goals and background}

The \Lrp project will create a distributed dataflow engine which will
act as an ``information plane''~\cite{worlds04_infoplanes} for fusing
data from a large number of sources on the network (end-system and
in-network monitoring and monitoring, centralized network databases,
etc.), processing it in the 
network, and delivering requested results efficiently to a large
number of Internet end-points.   This can also be viewed as a
continuous query processor for the Internet.  \Lrp's framework is
intended to incorporate a variety of data reduction algorithms,
provided they are amenable to a distributed implementation. 

This dataflow functionality is currently the missing part of the
technology required to realize the \Sys.  It is not an aim of
\Lrp to perform measurement per se, instead we anticipate distributing the
results of other measurement efforts (including Intel projects such as
CoMo).  Likewise, it is not an aim of the \Lrp to develop
new distributed data analysis and reduction algorithms, rather we will
seek to understand the general space of such algorithms from the
perspective of implementing the dataflow engine. 

Challenging as the \Sys vision may seem from a technical standpoint, we think
most of pieces of the puzzle have fallen into place in recent years,
and a focused research effort could bring together a usable and useful 
system in short order.  We present a brief selection here. 

First, distributed query processing and content-addressable networks
(DHTs) are getting much better at providing the right information to
the right place at the right cost.  P2p systems like
PIER~\cite{huebsch_vldb03} push computation into the
network to reduce the data shipped during query
answering.  A key tenet is the {\em data independence} that
underpins relational databases: the physical
organization (e.g., network location) of data should be separate from
the logical data model and query interface.  Queries can 
be posed on data regardless of its location, and data can be
reorganized without requiring changes in queries or 
applications that embed them.

DHTs are the first technology that provide this kind of
data independence at Internet scale.  PIER's
``flat'' DHT infrastructure is potentially a better fit for the \Sys
than hierarchically organized systems; it does not depend on a small
number of ``roots'' for the information and processing as DNS does, nor
does it restrict the system to queries that traverse the hierarchy (as
does, e.g., Astrolabe~\cite{astrolabe}).

Second, the practical application of statistical methods in systems
has been maturing over the last decade, in particular in computing
approximate answers to queries (e.g.~\cite{newjersey}). %,minos-tutorial
These techniques compute over small statistical summaries (samples,
histograms, wavelets, random projections, etc.) rather than the full
dataset.  Early exploration of these techniques in distributed
settings are promising~(e.g., \cite{gibbons04}).
Also, distributed implementations of techniques like graphical
models are emerging in the machine learning community, largely in the
sensor network space~(e.g., \cite{paskinguestrin}). 
These approaches model statistical
correlations in data, and use the models to predict data values and
quantify uncertainty; this is useful
both for predicting missing data, and for ``cleansing'' noisy acquired data.

Thirdly, the security and trustworthiness of \Lrp implies several
key challenges: validating the fidelity of data and computations,
managing resource consumption at the end-hosts, providing
accountability of misbehaving components, ensuring that the system
itself is not used maliciously as an attack platform, and a viable and
enforceable privacy framework.    Security in p2p
systems has been a topic of interest in recent years, with progress
being made on topics including self-certifying data, secure routing,
and fair sharing of work (e.g.~\cite{wallach02}).

Finally, the research community now has the resources to do non-trivial
test deployments of global-scale systems both in controlled,
repeatable ``laboratory'' settings~\cite{White+:osdi02,
  Vahdat+:osdi02} and more permanently in the real
Internet~\cite{planetlab:hotnets}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Technical Approach}

Our starting point for \Lrp is the PIER relational query processor,
and we refer readers to~\cite{huebsch_vldb03} for more details.
Viewed from a networking perspective, PIER consists of a set of nodes
in the network that exchange \emph{tuples} of information.  Tuples
flow through the network of PIER nodes.  At each node, tuples can be
processed in various ways, including being combined into a smaller
number of tuples, or generating further tuples. 

Since PIER follows the relational model, the operations it provides
are the basis of operations in the relational algebra: projections,
selections, joins, and aggregations.  However, PIER's basic
architecture naturally accommodates other operators which can serve the
basis of more sophisticated distributed algorithms. 

\Lrp will recast the basic ideas of PIER in a networking context.  The
project has three main, overlapping directions. 

\subsubsection*{A dataflow protocol definition for the Internet}

\Lrp will define a set of protocols which capture the
communication between nodes in the dataflow network.  Previous work in
distributed query processing for wide-area
systems~\cite{astrolabe,Gibbons2003,Yalagandula2004}, including PIER, has 
focused on implementation without providing protocol definitions.

Clearly a protocol definition is essential for uptake by the
community, but more importantly (from a research perspective) a
protocol-oriented approach forces us to consider questions of
federation and the need to defend against badly behaved, buggy,
non-compliant and/or malicious hosts at the protocol level

The \Lrp protocol suite is likely to have at least two parts.  The first
is a \emph{signaling protocol} which is used by users and
participating nodes to set up and tear down query state in the system.
This amounts to a signaling protocol for setting up dataflow within
the information plane.  Key issues in the design of this protocol
include an external  representation of partial query plans.

The second is a \emph{tuple transfer protocol} which is used for
exchange of data between nodes as part of a dataflow.  A major part of
this protocol is an external representation for tuples, which must not
only include the data in the tuple, but enough tuple lineage
information to be useful to multiquery optimization mechanisms and
auditing functions. 

\subsubsection*{An implementation of the protocol}

Secondly, \Lrp will at the same time deliver an implementation
of these protocols, that is to say, the code which runs on the nodes,
together with clients of the system, management applications, and
libraries and tools for integrating data sources into the framework. 

Such an implementation is important to the project for several
reasons.  We must make it easy for third parties to both interface
data sources to the information plane, and to implement clients of the
infrastructure.  

An equal motivation, however, is as a research vehicle.  We will
develop the implementation and the protocol simultaneously: history
shows that protocols specified without the benefit of experience
rarely survive implementation.

\subsubsection*{Taxonomy of distributed algorithms}

Thirdly, the project will produce a taxonomy of distributed
network data analysis algorithms.  The goal here is to identify
common \emph{patterns of communication} between different algorithms.
It is our intuition that a relatively small number of inter-node
communication patterns capture the majority of algorithms one might
wish to implement in this framework.  

We can already give two examples of such patterns.  One is the use of
aggregation trees in systems like Astrolabe~\cite{astrolabe} and
IrisNet~\cite{Gibbons2003}.  A second is 
communication using a Chord finger tables, which turns out to
fit the pattern of messages required to calculate a Haar wavelet
transform. \note{Joe, can you give a reference for this?  Actually, I'd
  like to read it myself -- Mothy}.

By identifying such patterns we can reduce the work required to
implement a new algorithm, since we can abstract and reuse the
communication code.  

More importantly, by constraining network communication to these
patterns we can make tractable the problem of limiting the network
resources used by an algorithm, or a particular instantiation of it in
the network.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Challenges}

\Lrp will build on the experience the group has with the PIER query
processor.  In moving from PIER to \Lrp, we see three principle
challenges in the project, which we describe here.  We do not discuss
any of the familiar problems common to all planetary-scale distributed
systems, except where there is twist on the problem particular to \Lrp.

\subsubsection*{Distributed multiquery optimization}

While PIER scales well to a large number of data sources, it does not
handle a very large number of simultaneous queries, and consequently
clients.  Our goal is to have \Lrp scale to millions of clients.  To
achieve this, there must be considerable sharing of both communication
and computation between queries.  This problem is known in database
terminology as multiquery optimization. 

Multiquery optimization has recently been explored in centralized
query processors, but not in a distributed context.  Our approach is
to extend \emph{tuple lineage}~\cite{madden_cacq} to the distributed case.  
In the centralized case, each tuple is annotated with two bit vectors,
a \emph{completion vector}, which has a bit set for each outstanding
query in the system that this tuple might be a partial result for, and
a \emph{steering vector}, which has a bit set for each dataflow
operator that the tuple has yet to traverse.  Eddies~\cite{avnur00eddies} are
used to steer tuples to operators, and tuples flow through the system
until they either form a final result to a set of queries, or the
completion vector becomes zero, whereupon the tuple is discarded. 

This technique is inadequate for a distributed dataflow engine since
the vector size scales linearly with the number of queries.  We will
apply a combination of tuple lineage and static planning to resolve
this problem: common parts of each abstract query plan will be
identified and shared, and it is these shared components which will be
indexed by the completion and steering vectors. 

\subsubsection*{Security and Fidelity}

Ensuring that the results delivered to an end system by \Lrp can be
relied upon, in the presence of compromised or otherwise malicious
nodes, is perhaps the major challenge in the project.  As much as
anything, addressing this issue will entail defining a threat model
that has a tractable solution. 

However, even at this early stage we can identify some promising
approaches.  In \emph{Spot-checking} and \emph{early
commitment}~\cite{Ergun1998}, nodes are randomly sampled for
correctness.  The nodes commit cryptographically to a given set of
inputs, and the corresponding output can then be verified.  This
technique has already been investigated in the context of sensor
networks by Dawn Song and Adrian Perrig at CMU; we have discussed
collaborating on similar techniques in \Lrp.

\textit{Homomorphic encryption} (used in the design of secure
anonymous voting systems) can also be applied to the calculation 
of, for example, aggregates like COUNT.  The basic idea is to map data
values via a homomorphism into an encrypted domain and carry out
in-network calculations in this domain.  This makes it extremely
difficult for malicious nodes to bias the results predictably.  We are
exploring a collaboration with Doug Tygar of UC Berkeley on this
topic.

As a final example, the ``influence'' that a malicious node can exert
over the results of a distributed computation like the queries that
\Lrp will process is often critically dependent on its position in the
topology of the computation (i.e. the particular communication
patterns induced by the query).  By choosing topologies so as to
spatially distribute influence evenly throughout the network, the
influence of malicious nodes can be limited to the same level as node
failures. Moreover, inducing churn in the system has the effect of
temporally distributing influence evenly. 

\subsubsection*{Algorithm taxonomy}

We have already discussed our aim to classify distributed data
reduction and analysis algorithms according to patterns of
communication.  Our approach will be empirical: we will implement 
(and, if necessary, devise) a small set of point solutions for a
number of algorithms, and work from there.  

In addition to the relational aggregation operators from the
literature, our initial work will focus on existing techniques members
of the group have explored offline for centralized analysis of network
measurement data, such as dimensionality reduction via principal
component analysis.  We also plan to collaborate with those working on
distributed implementations of both loopy Bayes nets and junction trees. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acquiring data}

We envision a healthy diversity of popular sensors targeted at
different network features, sharing an integrated query processing
backplane provided initially by \Lrp.
The \Sys is characteristically a shared infrastructure, and the \Lrp
project, as we have noted, does not intend to pursue measurement
techniques itself.  This raises the question of which initial data
sources the project will use. 

For reasons we have already outlined, we expect, at best, limited
cooperation from ISPs to start with.  Consequently, the \Sys will rely
at first on end-systems for both data sources and applications
(consumers of data).  A notable exception may be data from the CoMo
project~\cite{como}, and CoMo and \Lrp are to a large extent
complementary: CoMo is concerned with data acquisition on a single
node, and efficient shared filtering techniques on that data, while
\Lrp focuses on distributed processing, including correlating data
across many sources. 

Projects like NETI@home~\cite{Simpson+:pam2004} and
DIMES~\cite{dimes:website} are exploring large-scale network
measurement from end hosts, and the DShield
project~\cite{dshield:website} warehouses firewall data sent in from
many sources.  We have explored (and continue to explore)
collaboration with these projects and a similar effort at KAIST in
Korea.  Bundling a sensor with a global query visualization
like an Internet Screensaver seems like a good incentive here, with a
quid-pro-quo opt-in model: for the features you publish, you can see
distributed results involving such features. 

Moreover, another rich source of endpoints is ``dark'' IP address
space, as monitored by Network Telescopes.  While the traffic at these
addresses is idiosyncratic, it is of interest to many parties, and
could serve as ``seed'' data to populate the screensavers of early
adopters.

The \Sys must also be able to locate and interface with large
curated databases of information as well as distributed real-time
sources.  This includes slowly-changing network data (e.g., WHOIS) and
archival data warehouses of traffic information (e.g. RouteViews).

Finally, we note that the definition of end systems expands 
to include large distributed services like P2P networks and CDNs. 
Public-minded instances of these services can share interesting
traffic data with the \Sys, along the lines of
PlanetSeer~\cite{planetseer}.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evolution}

We conclude with a brief discussion of how \Lrp might continue beyond
the course of the LRP. 

The deployment of \Lrp could evolve long-term
in a number of ways.  As we have discussed, we do not foresee a
centrally administered solution succeeding.
One option is to have a consortium manage a
well-provisioned infrastructure, conceivably federated in nature like
the Internet today.  The recent success of PlanetLab is
encouraging in this regard, but it remains unclear whether a
consortium can maintain a production service like the \Sys without
sustainable, measurable benefit to the institutions hosting the
machines.

An alternative is an organic p2p deployment, with the query
processor being bundled with the sensors.  This is easier to deploy
than the ``distributed platform'' 
approach, and is in an important sense more self-sustaining: the
system remains up as long as sensors are deployed.  It has a populist
flavor that may allay some concerns about privacy and control.
Of course, robustness in the p2p approach raises the toughest
technical challenges of scale, management, and resistance to attack or
manipulation. 

\bibliographystyle{abbrv} 
\footnotesize
\bibliography{paper,rfcs}

\end{document}
