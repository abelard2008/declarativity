
Prodding note added to Evaluation.Discussion subsection:
  The second contribution of P2 mentioned in the paper is its ability
  to share work across overlays.  How many overlays are we talking
  about here?  What is P2's design point?  A few overlays, hundreds,
  etc.?  Presumably that would have some implications on your design
  and the associated bounds on performance gains to be had.

Sorta done: What is P2's target environment?  Production code or rapid
  prototyping?  If it's mainly the latter, then both performance and
  handling every possible corner case in the code seems less important
  as opposed to getting things mostly right to allow quickly comparing
  high-level design trade offs in overlays.  Further, if it is
  primarily the latter, then where does P2 fit in relative to other
  methods for exploring a design space such as simulation (specifically
  details beyond the usual sim vs. emulab vs. pl argument)?

- How are namespaces and schemas managed?  Simple example: user X
  calls his table foo with schema A, user Y calls his table foo with
  schema B.  I suspect this just means that that schema A foo's can
  talk to the schema A foo's while the schema B foo's can talk to the
  schema B foo's.  In summary, is this problem a user problem outside
  the scope of P2?

Done: I think I have a handle on the data generated and how it moves
  around with the dataflow graph with the @X, @Y notation.  But where
  are the indexes stored?  Presumably they are distributed in some
  fashion in order to get good performance.  If they are distributed,
  how does P2 address protection, isolation, corruption, etc.?  If
  they are not distributed, what are the performance implications?

Done: Tuples are "materialized for 60 seconds" What does that mean?  Each
  tuple, upon being generated, times out after 60 seconds?  If this is
  obvious, feel free to ignore this.

Done: Why is "periodic" a fundamental table that has to be there?  Why
  isn't one well-known table enough to bootstrap the system?

- How do you schedule dataflow when you have multiple producers and
  consumers?  For example, how would you deal with a node that is
  blasting another node?  Is this round-robin, something else?  I
  realize this is a problem in other systems as well, but it seems
  there are more explicit (low-level) ways to deal with this problem
  there, e.g., policies on serving sockets in the event loop, etc.

- Can you comment on the differences between the "service" versus
  "library" deployment scenarios?  Is this just whether your specific
  application wants to be part of shared comm/overlay optimizations or
  standalone?

Done: Are the core elements of P2 fixed and, if yes, are they are
  sufficient set?  Or are these elements starting point, with ability
  for users to add additional elements?  If the answer is that users
  can write new elements, then how do these elements get to all the
  nodes in the network?  What if you have conflicting implementations
  of elements with the same names?

- On the second contribution (shared work) of the paper, can you point
  to some analogous multiquery optimization problem and cite some work
  indicating that: (1) it's hard, (2) but that there is some sliver of
  hope (e.g., maybe because it's a new problem to be attacked, has
  some momentum, performance trends, etc.) of being solved?

Done: On the OverLog language, can you comment a bit on what its
  limitations are?  In particular, what is hard, impossible, and/or
  unnatural to express in OverLog?  Examples might be specific code
  constructs as related to pieces of real overlay implementations or
  entire overlays.

- If P2 isn't fast enough to have the code it generates be used in a
  production setting, what type of reuse can you get in terms of using
  it as a baseline for further optimizations?  Also, how feasible is
  it to hand optimize certain parts of the P2 generate code?  I
  guess answers to these types of questions will depend on your
  evaluation, e.g., maybe performance hints or something like that.

- What specific design choices in P2 target the shared work piece of
  P2?  In other words, one of P2's contributions is its ability to
  share work across overlays.  What were the mechanisms that you
  designed to enable that?  Is it simply reuse of common tables and
  streams?  My guess that it is more than that in order to actually
  get good performance.  Also, on the topic of relations, if I have
  multiple application son a node, who gets to update a shared local
  table?  Is there any type of protection, e.g., can I say, overlay 7
  has "read only" access to these tables?

Done: What are the X and Y's in @X, @Y?  Are these logical or physical
  addresses?  I assume logical since I saw a 160-bit type in the
  paper.  If logical, why?  And how do you map to physical IDs?

- What is the actual interface to P2?  It compiles down to what?  A
  library?  For example, say I use P2 to implement Chord.  Now I want
  to write an application that actually uses Chord, perhaps a
  decentralized instant messenger program.  How do I interface with
  the P2-generated code?  

- Lastly, how would you expect the initial P2 overlay constructed?  I
  would guess that its topology would matter for overall performance
  to the things that depend on it.  Since it is the lowest level
  thing, I'm assuming it perhaps does some sort of introspective
  performance measurements and continuously optimizes itself?  If yes,
  that along is probably an entirely separate paper I'm guessing, so
  feel free to ignore that.
