\documentclass{article}

\usepackage{amsmath}
\usepackage{epsfig}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{defn}[thm]{Definition}
\newtheorem{cor}[thm]{Corrolary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\title{Verifiable Hierarchical Aggregation}

\author{Aydan R. Yumerefendi}

\begin{document}

\maketitle

%\section{Correctness Definition}
%Let $\mathcal{D}$ be a collection of data sources and $\mathcal{A}$ be
%a population of aggregator nodes. Let $|\mathcal{D}|=d$,
%$|\mathcal{A}|=a$ ($ a \leq d$) , and $\mathcal{D} \cap \mathcal{S} =
%\emptyset$. 

%Different configurations are possible: $\mathcal{A} \subseteq
%\mathcal{D}$, $\mathcal{D} \cap \mathcal{A} = \emptyset$, or
%$\mathcal{D} \cap \mathcal{A} \neq \emptyset$. This is to say that
%some (all) aggregator nodes can also be data source nodes. Let
%$|\mathcal{D}\cup\mathcal{A}| = n$.

%Aggregator
%nodes are organized in some hierarchy that allows them to process data
%provided by the data sources. Data processing is distributed - data
%source nodes send data to aggregators and  aggregators perform local
%computation by applying a function $f()$ over all data they
%receive. The result of each local 
%computation is forwarded to other aggregator nodes, so that at the end
%of this process, one or more aggregator nodes contain a value, which
%is identical to the result of
%applying $f()$ to all data provided by the data source nodes.


%We are interested in evaluating $f() = COUNT(P)$, the number of data
%source nodes that have data matching the given predicate $P$. We can
%compute this number by setting $f() = \sum_{all inputs}$, and limiting
%the range of the value provided by each data source node: $d_i \in
%\{0,1\}$. Because of the magnitude of $d$, it is impossible
%(undesirable) to send all data generated by the data sources to each
%aggregator. Because of the scale of the system, it is also impossible
%to verify the correctness of the processing performed by each
%aggregator. At the very best, we can afford to verify the operation
%of a constant number or a small fraction of aggregators.  

%Let $S = \sum_{i=1}^{d}d_i$ be the correct sum of all values reported
%by the data source nodes, and $\widehat{S}$ be the estimated sum using the
%hierarchy of aggregators. We define $r = |S-\widehat{S}|$ to be the
%absolute additive aggregation error, and $s = \frac{|bad
%  aggregators|}{a}$ - the subversion fraction.


%If we can tolerate at most $\epsilon$ fraction of all aggregator nodes
%to be faulty, i.e. to perform their aggregation operations incorrectly,
%then ideally we would like them not to be able to offset the final
%result by more than the fraction of the data that they control,
%namely: $\epsilon \times a \times \frac{d}{a} = \epsilon d$.
 

%-- want a distributed solution: spread work
%-- want correct result
%-- but do not trust individual aggregators
%-- trust data source nodes
%-- can inspect some aggregators and verify they are work
%-- cannot inspect all
%-- what is the quality of the end result


%Choose an aggregation hierarachy that imposes strong probabilistic
%bounds on the resulting relative error.

\section{Building Blocks}

\subsection{Sampling From a Set} \label{sec:sampling}
{\bf Problem:} Let $S$ be a set of $n$ elements. Each $s_i \in S$ belongs to
one of the two classes: ``Good'' or ``Bad''. To test the properties
of $S$, we take a random sample of size $m$. How big should $m$ be, so
that if all samples are ``Good'', the fraction of 
``Good''  elements in $S$ is at least $(1-\epsilon)$ with
probability $1-c$, where $c$ is a constant, and $0<c<1/2$ ?\\

\noindent {\bf Solution:} The probability of all sampled elements being ``Good'', while
there are at least $\epsilon n$ ``Bad'' elements is at most:
$(1-\epsilon)^{m}$. We want to bound this number from above by $c$, so
that the fraction of the ``Bad'' elements is at most 
$\epsilon$ with probability at least $(1-c) > 1/2$.

\begin{eqnarray*}
(1-\epsilon)^{m} \leq c &\Leftrightarrow& m\ln{(1-\epsilon)} \leq \ln{c}\\
&\Leftrightarrow& -m\ln{(1-\epsilon)} \geq -\ln{c}\\
&\Leftrightarrow& m \geq \frac{\ln{1/c}}{-\ln{(1-\epsilon)}}  
\end{eqnarray*}

Using Taylor expansion of $\ln{(x)}$ centered at 1 $( 0 < x \leq 2)$, we get that:
\begin{eqnarray*}
\ln{(x)} &=& (x-1) - \frac{(x-1)^2}{2} + \frac{(x-1)^{3}}{3} - \ldots \\
&=& \sum_{i=1}^{\infty} \frac{(-1)^{i+1}(x-1)^{i}}{i}
\end{eqnarray*}

Substituting $(1-x)$ for $x$, we get:

\begin{eqnarray*}
\ln{(1-x)} = -\sum_{i=1}^{\infty} \frac{(x)^{i}}{i}
\end{eqnarray*}

$ \Rightarrow -\ln{(1-x)} = \sum_{i=1}^{\infty} \frac{(x)^{i}}{i} \geq x$
and as a result $\frac{\ln{1/c}}{-\ln{(1-\epsilon)}} \leq
\frac{1}{\epsilon} \ln{1/c}$. Thus by setting $m \geq 
\frac{1}{\epsilon} \ln{1/c}$, we satisfy the condition that with
probability at least $(1-c)$ $S$ contains at least $(1-\epsilon)$
``Good'' elements. By setting $c = e^{-1}$, it is sufficient to take
$\lceil 1/\epsilon \rceil$ samples to conclude with probability $1-1/e$ that
there are at least $(1-\epsilon)n$ ``Good'' elements.


\subsection{Repeating a Test with Constant Error Probability}
\label{sec:repeating} 

{\bf Problem:} Let $T$ be a test with constant error probability $c <
1/2$. Assuming independence, how many times do we need to repeat this
test to decrease the error probability to at most $\delta < c < 1/2$?\\

\noindent {\bf Solution:} If we perform the test $m$ times, $m \geq
1$, the probability of the test being wrong on all trials is $c^m$. We
want to bound this probability from above by a constant $\delta < c <
1/2$. We have:

\begin{eqnarray*}
c^m \leq \delta & \Leftrightarrow & m\ln{c} \leq \ln{\delta}\\
&\Leftrightarrow& -m\ln{c} \geq -\ln{\delta} \\
&\Leftrightarrow& m \ln{1/c} \geq \ln{1/\delta} \\
&\Leftrightarrow& m \geq \frac{\ln{1/\delta}}{\ln{1/c}}
\end{eqnarray*}

If $c = e^{-1}$, then by repeating $T$ $\lceil \ln{1/\delta} \rceil$
times we can ensure that the test is correct with probability at
least $1-\delta$.


\subsection{Mapping Values to Keys in a DHT}
\label{sec:mapping}

Without lack of generality, let's assume that the key space of a DHT
is a ring of size $K$. Let $V$ be a set of $n$ different values that
we want to map to DHT keys. The probability of assigning any $v \in
V$ to a given DHT key is uniform (making the necessary assumptions of
the hash function used). Let $A$ be the event of a DHT key not being
assigned to a value after $n$ consecutive assignments. We have that
$P(A) = (1- \frac{1}{K})^n$.

Let's partition the key ring into $a > 0$ equal size ranges. Let
$B$ be the event that a range constructed in this way is empty,
i.e. it contains no key that is mapped to any $v \in V$. The size of
each range is $\frac{K}{a}$. Therefore:

\begin{eqnarray*}
P(B) = ((1-\frac{1}{K})^n)^{\frac{K}{a}} =
(1-\frac{1}{K})^\frac{Kn}{a} \leq e^{-\frac{n}{a}}
\end{eqnarray*}

Let $E[B]$ be the expected number of empty ranges. We have that:
\begin{eqnarray*}
E[B] = a(1-\frac{1}{K})^\frac{Kn}{a} \leq ae^{-\frac{n}{a}}
\end{eqnarray*}   

Let $C$ be the event that a given range has exactly $n/a$ keys. We
have:
\begin{eqnarray*}
P(C) &=& {{\frac{K}{a}} \choose
  {\frac{n}{a}}}
  ((1-\frac{1}{K})^\frac{Kn}{a})^\frac{K-n}{a}(1-(1-\frac{1}{K})^\frac{Kn}{a})^\frac{n}{a}\\
&=&  {{\frac{K}{a}} \choose
  {\frac{n}{a}}}(1-\frac{1}{K})^\frac{Kn(K-n)}{a^2}(1-(1-\frac{1}{K})^\frac{Kn}{a})^\frac{n}{a}
\end{eqnarray*}


Let $X$ be a random variable describing the number of keys in a given
range that are mapped to some values in $V$. $X$ has binomial
distribution:
\begin{eqnarray*}
P[X=i] = {\frac{K}{a} \choose i}p^iq^{\frac{K}{a}-i}
\end{eqnarray*}
where $q = (1-\frac{1}{K})^n$ and $p=1-q$.

For the expectation of $X$ we have:
\begin{eqnarray*}
E[X] &=& \frac{K}{a}p = \frac{K}{a}(1-(1-\frac{1}{K}^n) =\\
&=& \frac{K}{a}(\frac{1}{K}(1+(1-\frac{1}{K}) + (1-\frac{1}{K})^2 +
\ldots + (1-\frac{1}{K})^{n-1})=\\
&=& \frac{(1+(1-\frac{1}{K}) + (1-\frac{1}{K})^2 +
\ldots + (1-\frac{1}{K})^{n-1})}{a}
\end{eqnarray*}

A typical value of $K$ is $2^{160}$ for which $\frac{1}{K} \approx 0$
and $E[X] = \frac{n}{a}$.

For the variance of X we have:
\begin{eqnarray*}
Var[X] &=& \frac{K}{a}pq = \frac{K}{a}\frac{(K-1)^n}{K^n}\frac{K^n -
  (K-1)^n}{K^n} =\\
&=& \frac{K(K-1)^n}{aK^n}\frac{K^{n-1} + K^{n-2}(K-1) + \ldots +
  (K-1)^{n-1}}{K^n} \leq \\
&\leq& \frac{K(K-1)^nnK^{n-1}}{aK^{2n}} = \frac{n}{a}(\frac{K-1}{K})^n
  \leq \frac{n}{a}
\end{eqnarray*}


\subsection{Selecting a Sub-set of Application Hosts}
\label{sec:selecting}
{\bf Problem:} Given a set of $n$ application hosts organized in a DHT
circular key-space of size K, choose with high probability a sub-set
of different application host of size $\frac{m^k-1}{m-1}$, where $k$
is maximal.\\

\noindent {\bf Solution:} To select $p$ (not necessary different)
application  hosts we can use the following algorithm. Split the
key-space into $p$ equal size ranges. For each range select the node
successor(beginning of range). The advantage of this scheme is that it
is efficient and convenient: to access the $i^{th}$ selected host, simply
compute successor(start of $i^{th}$ range). However, it is possible to
obtain less than $p$ different physical hosts, since some ranges can
be empty. In Section~\ref{sec:mapping} we calculated the probability
of a single range being empty to be at most $e^{-\frac{n}{p}}$ and
the expected number of empty ranges to be at most
$pe^{-\frac{n}{p}}$. Therefore, to solve the problem, we can use the
simple selection mechanism with the largest value of $p$ for which the
expected number of empty ranges becomes close to zero. This procedure
will give us with high probability $p$ different application hosts.

\subsection{Estimating the Number of Application Hosts}
\label{sec:estimating}
{\bf Problem:} Let there be $n$ application hosts organized uniformly in a
circular key-space. Devise an algorithm to estimate $n$, with limited
communication and computation.

\noindent {\bf Solution:} Let the key-space be of size $K$. Let $X$ be
a random variable equal to the distance (in key-space) between two
consecutive application hosts. We can estimate $E[X]$ by using the
observation that the $n$ nodes partition the key-space in $n$ disjoint
intervals, the sum of which is $K$. Therefore the average interval
size is: $E[X] = \frac{K}{n}$. Therefore, given $E[X]$, we have $n =
\frac{K}{E[X]}$.

We can approximate $E[X]$ by using sampling. Let $\overline{X}$ be the
mean of the observed $m$ samples. We know that $E[\overline{X}] = E[X]$
and $Var[\overline{X}] = \frac{Var[X]}{m}$. (Need to compute
$Var[X]$). From the Central Limit Theorem, we have that $\overline{X}$
is normally distributed. Therefore, $|X-\overline{X}| \leq
\Phi(t)\sqrt{\frac{Var[X]}{m}}$ with probability $t$, where $\Phi(t)$
is the abscissa of the vertical line, which cuts an area of size
$\frac{1-t}{2}$ from the unit normal curve. If $\widehat{n}$ is the
estimated value of $n$ using a sample of size $m$, we have with
probability $t$ that:
\begin{eqnarray*}
\frac{K}{\frac{K}{n} + \Phi(t)\sqrt{\frac{Var[X]}{m}}} \leq \widehat{n} \leq
  \frac{K}{\frac{K}{n} - \Phi(t)\sqrt{\frac{Var[X]}{m}} }
\end{eqnarray*}


\subsection{Building a Multicast Tree}
\label{sec:building}
{\bf Problem:} Build a spanning tree of degree $m$ of all application
hosts registered in a circular key-space.\\

\noindent {\bf Solution:} Let $v$ be the root of the spanning
tree. We define the visited list $L$ as the list of all nodes along a path
of the spanning tree visited by going top-down from the root. The root
of the tree adds itself to the visited list and partitions the key space
into $m$ equal size key 
ranges. For each key range it computes $v_i =
successor(beginning of range)$. If $v_i \notin V$, $v$ forwards the
visited list to $v_i$. Upon receiving the message, $v_i$ adds itself
to the visited list, calculates the boundaries of its range, splits it
into $m$ equal sizes, finds the successors of the beginning of
each sub-interval, and forwards the visited list to the successors not
yet in it. Using this recursive scheme the process terminates when all
application hosts have been visited. Using the uniformity assumption,
we can show that the average distance from the root to a leaf in the
tree is $\log_m(n)$, where $n$ is the number of application hosts. The
variance of this estimate is within small bounds (need to quantify it).



\subsection{Estimating the Cardinality of a Set}
Flajolet and Martin propose an algorithm to estimate the cardinality
of a set with customizable precision. The core idea of the algorithm
is to use randomization: each distinct element can contribute in a
deterministic way and the rate of the contribution can be exploited to
compute the cardinality of the set. A very useful property of the
algorithm is that it is insensitive to the underlying
distribution, as well as to repetition and multiple counting.

The algorithm works as follows. Let $B$ be a bitvector of size $r$, so that $r
> \log_{2}{n}$, where $n$ is the maximal possible cardinality, and let
$H$ be a hash function defined on the domain of possible set elements
with range $[0, 2^r-1]$. Define $\rho(\alpha)$ to be the position of
the left-most 1 bit in the binary representation of $\alpha$. For each
set element $x$, compute $\rho(H(x))$ and set the corresponding bit of
$B$ to 1. Using this approach on the average every second element sets
the bit at position 0, every fourth - the bit at position 1, etc. We
can compute the cardinality of the set as two to the power the position of the
left-most consecutive 1 bit. Flajolet and Martin derive more precise
formulas for the final conversion that correct also for the small
incurred bias.

It is possible to improve the standard error of the above aproach by
repeating it multiple times and taking the average result. Flajolet
and Martin show that 64 repetitions result in precision of
approximately 10\%.

Mayank Bawa presented a small modification to the above
algorithm. Bawa's proposes to use coin flipping to compute $\rho$. For
each element key flipping coins intil the first time the flip results
in heads and use the trial number to mark the corresponding bit in 1. 

The Flajolet Martin algorithm gives in naturally to distributed
computation: if $B_1$ and $B_2$ are the bitvectors obtained by running
the algorithm on set $A_1$ and $A_2$ respectively, than the
cardinality of $A_1 \cup A_2$ can be determined from $B = B_1$ OR
$B_2$. 

\subsection{Estimating the Sum of a Set}
It is possible to estimate the sum of set by using the techniques for
set size estimation: transform the original set to a set, the
cardinality of which equals the sum of all set elements. Another
approach is to use repeated coin flipping as if each element consists
of a number of virtual elements equal to the magintude of its value.

Instead of flipping a coing multiple times, it is possible to flip a
biased coin and achieve the same result. Bawa describes the desired
biased coin probabilities.


\subsection{Sampling from a Tree}
Let $S$ be a set of data organized hierarchically in an m-way
tree. For simplicity let us assume that the set elements are stored at
the leaves of the tree and the internal tree nodes are simply used for
navigation. In many scenarios it is necessary to be able to obtain a
random uniform sample from $S$. If the underlying tree is a full tree
of $k$ levels, then any random root-to-leaf walk will generate a
random sample of a leaf node. However, if the tree is not full, some
walks will be shorter than others and as a result the probability of
visiting a leaf node will not be uniform.

To sample from a tree, we need to ensure that all walks give paths of
equal length. This means that the underlying tree has to be a full
m-way tree of maximal height. We can construct such a tree by placing
more than one element in a tree leaf. For an m-way tree, we can store
up to m elements in each tree leaf. If we use this organization, each
root-to-leaf walk will visit leaf nodes with uniform probability. To
ensure that all elements are visited with the same probability, we
need to inspect every single element stored in a given tree leaf.


It is interesting whether we can ensure that the tree structure is
maintained correctly by an untrusted entitity. We can define the tree
stucture more strictly as follows:
\begin{enumerate}
  \item All tree leaves are at the same level
  \item Each tree leave stores up to m elements
  \item If there are a total of $n$ leaves, all leaves up to $l_k$ ($0
  \leq k < n$) store $m$ elements, at most one stores more than 1 and
  less them m, and the remaining leaves store exactly 1.
\end{enumerate}

When we take a random walk, we can examine the path length of each
walk and the number of elements in each leaf. When we inspect we make
sure that:

\begin{enumerate}
  \item All path lengths are the same
  \item The three path is in sorted-order
  \item If two leaves, have different number of elements,  then they
  are at the correct location
  \item (when m> 2, at most one has more than 1 and less than m
  elements)
\end{enumerate}

If we take $1/\epsilon$ walks and ensure that they all stop on level
$l$ of the tree, what can we say about the tree?

The tree has $m^l$ nodes on level $l$. Let $n$ be the total number of
leaves in the tree. We have that at most $\epsilon m^l$ of the nodes
on level $l$ are not leaves: either lead to a leaf at a lower level or are the
continuation of a path that finishes at a leaf on a higher level.

$\Rightarrow n > (1-\epsilon)m^l$.

The problem here is that there can be a small number of heavy
sub-trees rooted at level $l$ and as a result the probability of
visiting a leaf in the heavy sub-tree is much smaller. To accomodate
for this the data structure maintainer needs to provide the upper
bound explicitly.

Tree commitment schemes using Merkle trees maintained by the prover
cannot be used to sample with uniform probability even if we try to
impose some predefined tree structure: we cannot impose an upper bound on the
number of nodes in the tree: the bound will have to be provided by the
prover.

Another problem with trees for sampling is repeating elements? 



note: there are two questions here: I commit to this set of data and
cannot add/remove any item at a later time. But i need not commit to
all. So if we want to make sure that i commit to all, we need some
oracle access to all.





\section{Secure Set Cardinality Estimation}
In this section we propose an extension of the traditional Flajolet
and Martin algorithm for set size estimation. The goal of this
extension is to distribute the computation of the bitvectors among
different untrusted entities and to ensure correctness bounds on the
final result.






\section{Single Tree Aggregation}
\label{sec:aggregation}
Let $T$ be a full \emph{m-way} tree with a total
of $a$ nodes. We will refer to $T$ as the aggregation tree and to each
node of $T$ as an aggregator. Let the number of data sources be $n$
($0 < a \leq n$) and aggregator nodes receive data from exactly $n/a$
data sources. Let us also assume that there exists
a deterministic mechanism to map data source nodes to a given
aggregator as well as to determine the location of each aggregator in
the hierarchy. Finally, we assume that each value reported by either a
data source or an aggregator carries a digital signature to certify
the the identity of its origin. 


To perform the aggregation using $T$ we proceed as follows:
\begin{enumerate}
  \item Each aggregator $a_i$ processes the $n/a$ values received from the
  corresponding data sources and computes their total $t_i$.
  \item An aggregator $a_j$, which is at the leaf level of $T$,
  forwards $t_j$ to its parent in $T$. 
  \item An aggregator $a_k$, which is an internal node of $T$, waits for its
  children to send their corresponding values, computes their sum, adds
  $t_k$, and forwards the result to its parent.
  \item The aggregator at the root of $T$ reports the final result $v_f$.
\end{enumerate}

\begin{defn}
A \emph{correct} aggregator is one that:
\begin{enumerate}
  \item Sums correctly all $n/a$ data source values
  \item Sums correctly all children aggregators' values (if not a leaf)
  \item Reports to its parent the exact value it computed
  \item Checks each reported value for proper magnitude. For
  performing COUNT(), each data source node reports a value in $\{0,
  1\}$. Since, there are a fixed number of aggregators in a given
  aggregator sub-tree, there is also an upper bound on the value
  reported by any aggregator node.
\end{enumerate}
\end{defn}

Using the above definition for correctness we can design a test to
verify the correctness of a given aggregator node. Assume that we know
the value $v_i$ reported by aggregator $a_i$. To verify the correctness
of $a_i$ we ask it to provide all data source values that it has
received. If the aggregator is an internal node of $T$, it also sends
the values that it received from its children in the aggregation
tree. Having obtained all values used in the aggregation, we verify
their integrity and origin by inspecting their digital
signatures. Having established the identity of the source of each
value we verify that this host was indeed supposed to report to the
given aggregator. In the next step of the verification process we
inspect the magnitude of each value. Having passed this test, we sum
all values and compare them to the already known value $v_i$. 

We can use this verification procedure to take a random walk in the
aggregation tree and verify the correctness of each aggregator node
along the path. We start from the root of the tree and know $v_f$. We
use the correctness test to inspect  the root. Having inspected the
root, we choose uniformly at random one of its 
children. Since we already know its value, we can proceed with the
correctness test. We continue recursively until we reach a leaf in the
aggregation tree. If we perform multiple random walks, making
independent random decisions, we obtain a sampling mechanism that
selects nodes on a given tree level independently and with uniform
probability. Therefore, performing $1/\epsilon$ random root-to-leaf walks will
ensure with probability at least $1-1/e$ that at most $\epsilon$
fraction of all aggregator nodes on a given tree level are incorrect
(Section~\ref{sec:sampling}). Using the result from
Section~\ref{sec:repeating} we can repeat the $1/\epsilon$ random
walks $\ln(1/\delta)$ times to achieve probability of at least
$1-\delta$.  

\section{Single Tree Aggregation Analysis}
\label{sec:analysis}
In this section we analyze the security and correctness guarantees of
the aggregation procedure described in
Section~\ref{sec:aggregation}. This procedure guarantees that with
probability $1-\delta$ there are at most $\epsilon$ faulty nodes on a
given tree level.

The aggregation tree $T$ is a full \emph{m-way} tree with a total of
$a$ nodes. Let $l$ be the height of $T$. We have:

\begin{eqnarray*}
a &=& m^0 + m^1 + \ldots + m^{l-1}\\
a &=& \frac{m^l -1}{m-1}\\
l &=& \log_m(am-a+1)
\end{eqnarray*}


\begin{lem}\label{lem:offset}
A faulty aggregator that is $k$ levels higher than a leaf node ($0 \leq k
\leq l-1$) can report an aggregation result that is at most 
$\frac{n}{a}\frac{m^{k+1} -1}{m-1}$ higher(lower) than the correct one.  
\end{lem}

\begin{proof}
A leaf node in the aggregation tree controls $n/a$ data values. Each
value can be ether 0 or 1. Therefore, each leaf aggregator can offset
the correct result by $\frac{n}{a} = \frac{n}{a}\frac{m^{0+1}-1}{m-1}$.
Let for some $p \geq 0$ we have that an aggregator node at a distance
$p$ levels from the leaves of the aggregation tree can offset the
result by at most $\frac{n}{a}\frac{m^{p+1}-1}{m-1}$. An aggregator
node, one tree level higher, receives data from $n/a$ data source
nodes, as well as $m$ aggregators from the lower level. Since, each of
the lower level aggregators can offset the result by at most
$\frac{n}{a}\frac{m^{p+1}-1}{m-1}$, we have that the higher level
aggregator can offset the result by at most:

\begin{eqnarray*}
m\frac{n}{a}\frac{m^{p+1}-1}{m-1} + \frac{n}{a} &=&
\frac{n}{a}\frac{m^{p+2}-m+m-1}{m-1}\\
&=& \frac{n}{a}\frac{m^{p+2}-1}{m-1}
\end{eqnarray*}
\end{proof}


\noindent From the principle of induction it follows, that an aggregator $k$
levels higher than a leaf node in the aggregation tree can report an
aggregation result that is at most $\frac{n}{a}\frac{m^{k+1} -1}{m-1}$
higher(lower) than the correct one 

\begin{thm} 
\label{thm:faulty}
If at most $\epsilon$ fraction ($0 \leq \epsilon \leq 1$)
  of the aggregators on a given aggregator tree level are faulty,
  cumulatively  they can offset the correct aggregation result by at
  most  $\epsilon n$.
\end{thm} 

\begin{proof}
Using our notation, we have that the aggregation tree is a
full \emph{m-way} tree with height $l$ and a total of $a$ tree nodes, where
$l = \log_m(am-a+1)$. An aggregation tree level that is $k$ ($0 \leq
k \leq l-1$) levels higher than the leaf level, has $m^{l-1-k}$ nodes. We
have that at most $\epsilon$ fraction of them are faulty, giving us at
most $\epsilon m^{l-1-k}$ faulty nodes on the level. From
Lemma~\ref{lem:offset} we have that each of the faulty nodes can
offset the result by at most  $\frac{d}{a}\frac{m^{k+1} -1}{m-1}$. Let
$F$ be the cumulative error caused by all faulty nodes. We have
that:

\begin{eqnarray*}
F &\leq& \epsilon m^{l-1-k} \frac{n}{a}\frac{m^{k+1}-1}{m-1}=\\
&=& \epsilon \frac{n}{a}\frac{m^l}{m^{k+1}}\frac{m^{k+1}-1}{m-1}=\\
&=& \epsilon \frac{n}{a}\frac{am-a+1}{m^{k+1}}\frac{m^{k+1}-1}{m-1}=\\
&=& \epsilon n\frac{am^{k+2} - am^{k+1} + m^{k+1} - am + a
  -1}{am^{k+2} - am^{k+1}}=\\
&=& \epsilon d (1 + \frac{m^{k+1}-am + a -1}{am^{k+2} - am^{k+1}})
\end{eqnarray*}

\noindent Let's examine the conditions under which the second summand
is at most zero:

\begin{eqnarray*}
\frac{m^{k+1}-am + a -1}{am^{k+2} - am^{k+1}} \leq 0 &\Leftrightarrow&
m^{k+1}-am + a -1 \leq 0\\
&\Leftrightarrow& a(m-1) \geq m^{k+1}-1\\
&\Leftrightarrow& a \geq \frac{m^{k+1}-1}{m-1}
\end{eqnarray*}

\noindent But we also have that $0 \leq k \leq l-1$ and $a =
\frac{m^l-1}{m-1}$. Therefore, the above condition is true for all
valid values of $k$.

$\Rightarrow F \leq \epsilon n$.
\end{proof}

\begin{thm} The hierarchical aggregation algorithm produces with
  probability at least $1-\delta$ a result
  that is within $\epsilon n$ from the correct result.
\end{thm}

\begin{proof}
Each incorrect node that is undetected after the sampling is the root
of a sub-tree, no node of which has been traversed during
selection. All faulty nodes in $T$ can be organized into a set of
subtrees $K = \{T_1, T_2,\ldots T_k\}$, so that the root of $T_i$ is
the incorrect node $n_i$ and all ancestors of $n_i$ are correct. Clearly all incorrect
nodes in $T$ fall in one of the $T_i$


Moreover, any node in $T_i$
can be incorrect: sampling did not visit the root of $T_i$. By
sampling the tree, we know that with probability at least $1-\delta$,
there are no more than $\epsilon a$ incorrect aggregator nodes.

$\Rightarrow \sum_{i=1}^{k}|T_i| \leq \epsilon a$ with probability at
least $1-\delta$. Since each of the
faulty aggregators controls $n/a$ data source values, the total error
will be at most $\epsilon a \times n/a = \epsilon n$, with probability
$1-\delta$.
\end{proof}

\section{???}

\subsection{The Basics}
All application hosts are running on top of a Distributed Hash Table
(DHT). We make use of a publicly available DHT managed by a single
trusted authority. Clients of the DHT can install application code
in any DHT node and make use of the DHT public API. The main
characteristic of our platform is that application code has no access to
the DHT code and as a result cannot influence the correctness of the
DHT primitives. Therefore, in our work we assume that the DHT API are
trusted and hence always correct. However, application code is
vulnerable and is untrusted to perform its operations
correctly. Finally, there can be multiple applications running on
each DHT node, and not all DHT nodes are required to run the
same set of applications.

The DHT provides two basic primitives. The {\tt get(key)} primitive
retrieves the value stored under the specified key, and the
{\tt put(key,value)} primitive allows a client to store data under a
given key. There are two basic modes of servicing {\tt put()}
requests. In the \emph{overwrite} mode the new value overwrites any 
existing data stored under the key. In the \emph{append} mode values
are added to the already existing data. The DHT annotates each
stored value with the mode used in the corresponding {\tt put()}
operation.

\subsection{Application Hosts Registration}
All application hosts register themselves as members of the
application. Since the main target of our work is applications with
large number of hosts, centralized and broadcast registration are not
viable options. Our system requires a distributed membership
management scheme with provable correctness: if a malicious
application host decides to remove some other application hosts from
the system, it should be possible to detect this with high
probability. 

The ReDir protocol, provides a distributed membership management
mechanism. (Describe ReDir). 

A malicious host that uses the ReDir protocol, can remove
already registered application hosts by using put() in overwrite
mode. However, we can always detect this action by examining a given key
and inspecting the DHT annotations for each value. Should there be a
value with overwrite annotation, there has been some potentially
malicious action. While we can always detect malicious actions,
currently we have no mechanism to avoid them.

Another problem associated with the ReDir protocol is the ability of
an application node to put any identity during the registration phase. A
malicious host can put an identity under the wrong key, or can simply
insert an invalid identity. We can easily prevent against the first
threat by verifying that each identity we use falls in the current
range. The second threat, though, can cause significant problems
because it makes Sybil attacks possible: a single application host can
forge multiple identities and concentrate them in a single range of
the key-space, thus violating the uniformity of key
distribution. Uniform key distribution is important for the
performance and load balancing of our system.

To protect against Sybil attacks it is necessary to verify the
identity that an application host uses during ReDir registration. We
introduce a new put mode that treats the value parameter of the
put operation as the IP address of the client performing the
operation. To verify this IP address, the DHT establishes a handshake
with the client to protect against clients using spoofed IP
addresses. Other verification mechanisms based on public certificates
are also possible. 

Using the additional put mode, we can ensure that an attacker cannot
concentrate its presence in a given range of the key space and violate
the uniformity of key distribution. 

\subsection{Asking a Query}
We currently support queries counting
how many application hosts have data matching a given
predicate. We can support any general predicate that can be
applied to the data stored at each application host. When asking the
query, the client specifies the following parameters:
\begin{enumerate}
\item {\bf P} - the predicate to be execute at each data source node
\item {\bf s} - a random number uniformly chosen in the interval
  $[0,2^{160}-1]$ 
\item {\bf m} - the degree of the aggregation tree
\item {\bf a} - the number of aggregators to use in the evaluation of
  the query result
\end{enumerate}

To make query evaluation more resilient to long-term adversary attacks,
it is important to introduce variability in the aggregation hierarchy
used to evaluate the query. With static hierarchy, a long term
adversary can continuously corrupt aggregator nodes and obtain
significant presence. We introduce non-deterministic variability in
the aggregation hierarchy by using a random seed to determine its
root. 

The degree of the aggregation tree determines the amount of work
performed by each aggregator node, as well as the amount of work
performed in the query result verification phase. Larger values of $m$
translate into more work for each aggregator and more work to verify
the correctness of an individual aggregator. At the same time larger
values of $m$ translate into smaller tree height. (This is not
necessary the case, as depending on the value of $m$, we might
actually choose more aggregators).

Estimating the number of aggregator nodes to use has the goal of
choosing an optimal aggregation tree size that will balance the work
evenly among all involved aggregator nodes. The higher the number of
the aggregator nodes, the smaller is the average number of application
data that they receive, and consequently the smaller the cost of
auditing a single aggregator node. Therefore, we want to chose the
largest possible number of aggregators so that the 
amount of work performed during the result verification phase is as small
as possible. However to choose an acceptable number of aggregators it
is necessary to have an estimate of the number of application hosts. 

There are various ways to obtain the number of application
hosts. Centralized solutions can give very accurate estimates but are
too costly for networks of large sizes. Having each node maintain a
membership information about all application hosts is another
alternative with the same problems. For applications with large number
of hosts, we need a lightweight approach to estimate the network size
with as high accuracy as possible. 

Viceroy described a mechanism to estimate $n$, by using the distance
(in key-space) between two consecutive hosts. In
Section~\ref{sec:estimating} we present the approach together with
some error bounds. Viceroy also concluded that taking a sample of size
at least $\log(n)$ gives asymptotically tight results. Symphony also
uses the same mechanism, but their simulations show that even a sample
of size 3 gives reasonable results (for their particular application).

Using the estimate for $n$ we can use the approach described in
Section~\ref{sec:selecting} to choose a maximal set of
aggregators. The query initiator does not perform the actual selection
-- she simply determines the values of $a$ and $m$, which together
with the random seed $s$, completely define the aggregation hierarchy.

Having gathered all necessary query parameters, the query initiator
selects a random application host and sends the query to this host. To
select a random application host, the query initiator chooses
uniformly at random a number $x$ in the range of the key-space, and uses
the ReDir protocol to discover the node $v=successor(x)$.   

\subsection{Query Dissemination}
When an application host receives a query from a query
initiator, it initiates the process of query dissemination. Ideally,
we would like application hosts to receive the query, so that the
final result can be an accurate representation of the data in the
whole system. Alternatively, we can combine a best effort
dissemination approach with sampling to ensure that no more than
$\epsilon$ fraction of the application hosts have not received the
query. The 
advantage of sampling  is that we can use any best effort
approach to send the query and still get a good guarantee of the
correctness of the dissemination process. If sampling is successful, then
probabilistically the query dissemination has succeeded. A
disadvantage of this approach is its probabilistic nature, the
possibility of $\epsilon$ fraction of the application hosts being
unaware of the query, and the susceptibility to denial of service
attacks.

To disseminate the query we can use any existing multicast mechanism
such as Scribe, ReDir, or the simple scheme presented in
Section~\ref{sec:building}. 


\subsection{Aggregation Tree Construction}
We construct the aggregation tree using the simple selection mechanism
described in Section~\ref{sec:selecting} with the requirement that the
first key range starts at the random seed given in the query.

Each application host determines in which part of the key space it is
located by using its own identifier and the random seed $s$. Having
done this it sends a message to the successor of the beginning of its
range. The message contains the result of executing the predicate (0
or 1) and the hash of the query all signed with the key of the host. If
the node is not the successor of the beginning of its range, its part
of the process is over. If the node is the successor of the beginning
of its range, it starts a timer within the expiry of which it  has to
receive data from its children that are not aggregator nodes. Having
received the data, it verifies that they indeed are part of its range
and computes the sum. 

If the node is a leaf in the
aggregation tree (can be determined by the index of its range), then
it simply forwards the result with the hash of the query (digitally
signed) to its parent (can also be determined from the index of the
nodes' range). If the node is an internal node of the aggregation
hierarchy, it starts another timer and waits for its child aggregator
nodes to report their values. The duration of this timer depends on
the height of the tree and is proportional to the number of nodes in
the aggregator's sub-tree. Once all aggregator children report their
value, the node sums the values, adds the sum of the values of the
hosts in its range and forwards the result to its parent. 

\section{Verification}
The goal of query verification is to establish probabilistic bounds on
the quality of the aggregation result. Using verification we want to
ensure that the dissemination phase of query execution has reached at
least $(1-\epsilon)$ fraction of all application hosts. This is an
important requirement since it guarantees that the end result is
computed over a substantial number of the system hosts. The second
goal of verification is to ensure that at most $\epsilon$ fraction of
all nodes on each aggregation tree level are incorrect. We know from
Section~\ref{sec:analysis} that this assurance translates into the
final result being off from the correct one by at most $\epsilon n$. 

We outline some modifications to the verification procedure outlined
in Section~\ref{sec:aggregation}. The need for modification arises
from the fact that unlike the situation described in
Section~\ref{sec:aggregation} in the P2P setting some aggregators will
receive data from more than $n/a$ hosts in their range, while others will
receive from less. In Section~\ref{sec:mapping} we estimated the
variance of the number of received values to be at most
$\frac{n}{a}$. This observation makes it possible for an aggregator
node to omit the value reported by a system host and we need a mechanism
to verify probabilistically whether there are more than $\epsilon$
fraction of omitted nodes. We can treat these omitted nodes as nodes
who did not receive the query.

We can inspect for omitted values by adding an additional verification
procedure. While inspecting an aggregator, choose uniformly at random
an application host that falls within its interval, and make sure that
its value is included. Using this technique, we can guarantee that at
most $\epsilon$ fraction of all application hosts that fall on a given
aggregation tree level have not received the query (and have not been
included in the result). Summing across all levels, we obtain the
required guarantee that at most $\epsilon$ fraction of all application
hosts have not received the query (not been included in the result). 


%NOTE: Somewhere we have to say something about virtual-physical
%nodes. The fact that more than one virtual nodes can be mapped to one
%physical node does not affect the correctness of the solution but
%affects the load distribution.



\section{Count by Set Size Estimation}
Let's assume that we have an algorithm $A$ that can estimate the size
of a set with some customizable prcision.. How can we use this algorithm to
estimate the result of a COUNT() aggregation operation? For this
purpose let us introduce some notation. Let $X$ be the correct set of
all data source values that have a value of 1, $Y$ be the correct set
of all data source values that have a value of 0, and $Z = X \cap Y$
be the set of all data source nodes. Let also $X'$ be the set of data
source values that are claimed to be 1, $Y'$ be the set of the
datasource values that are claimed to be 0, and $Z' = X' \cap
Y'$. Finally, we will use $\widetilde{X}$ to denote the estimate of
$X$ obtained by running the algorithm for set size estimation. 

NOTE: (1) The text below needs to be revised/reorganized.\\

      (2) We assume that the data sources are correct. This means that
          when they receive a query they do send a corresponding
          value. The problem with using set size estimation directly
          is that even with perfect set size estimation, we do not
          know if all data sources have received the query. If we
          sample the data sources to see if they received the query,
          then we can ensure that no more than $\epsilon$ fraction of
          the data sources have not received the query. This will
          directly translate into getting a relative error in terms of
          the number of data sources, not the number of data sources
          matching the predicate. 


Clearly, if we have direct access to $X$, we can compute its size and
solve the problem. However,  in practice we have access to only $Z$, $X'$,
and $Y'$.

{\bf Solve the problem by using $Size(Z)$}\\
One can compute count using the following approach:
\begin{enumerate}
  \item Compute the size of $Z$
  \item Compute the size of $X'$
  \item Compute the size of $Y'$
\end{enumerate}

If we assume that there are no failures during the process, we can
expect that $Z = X' + Y'$.


We would like to use $X'$ to compute the aggregation result, but
we need to ensure that:

\begin{enumerate}
  \item $X'$ does not contain too many falsely claimed 0s.
  \item $X'$ does not exclude too many element of $X$.
\end{enumerate}

We can ensure the first guarantee by sampling $X'$. We can examine at
random $1/\epsilon$ elements of $X'$ and ensure that they all have 1
values. This will guarantee that $X' \leq (1+\epsilon) X$ with probability
$1-1/e$. To satisfy the second requirement, though, we need to sample
elements from $X$. However we do not have access to $X$. We can sample
elements from $Z$ and stop after we collect the desired number of
samples from $X$. The number of elements of $X$ that we need to
examine before we obtain the desired sample is dependent on the
selectivity of the predicate ($0\leq s \leq 1$) and its expected value
is $\frac{1}{s\epsilon}$. For predicates with relatively low
selectivity this number can be significant.

We can avoid the need to sample from $X$ by using $Y'$ in the
computation of the aggregate. We can sample $Y'$ to ensure that $Y'
\leq (1+\epsilon)Y$. We can then sample from $Z$ and make sure that each
sampled element is in either $X'$ or in $Y'$. If we take $1/\epsilon$
samples we know that $X' + Y' \geq (1-\epsilon)(X+Y)$. We can continue
sampling until we collect $1/\epsilon$ samples from either $X$ or
$Y$. In the worst case this process will require at most $2/\epsilon
-1$ samples. If we were lucky to collect the samples from $X$, then we
have $X' \geq (1-\epsilon)X'$ and together with the earlier result of
$X' \leq (1+\epsilon) X$, we have that $(1-\epsilon)X \leq X'(1+\epsilon)X$. 

If we were unlucky and selected more elements from $Y$, then $Y' \geq
(1-\epsilon)Y$. We have that:
\begin{align*}
  & X' + Y' \geq (1-\epsilon)(X+Y)\\
  \Leftrightarrow & X' \geq (1-\epsilon)(X+Y)-Y'\geq
  (1-\epsilon)(X+Y)-(1+\epsilon)Y = (1-\epsilon)X -2\epsilon Y\\
  \Leftrightarrow & X' \geq X -\epsilon(X+2Y)
\end{align*}

%$\Rightarrow (1-\epsilon)X \leq X' \leq (1+\epsilon)X$.



\section{Design Space}

(Mothy's Suggestion)

\begin{table}[htpb!]
  \centerline{
  \begin{tabular}{|c|c|c|}
    \hline
    Data Sources & Aggregators & Routers \\
    \hline
    T & T & T\\
    \hline
    U & * & *\\
    \hline
    T & T & U\\
    \hline
    T & U & U\\
    \hline
    T & U & T\\
    \hline
  \end{tabular}}
  \caption{\label{tab:scenarios} Possible Trust Scenarious}
\end{table}
  
We can describe the potential trust scenarious using
Table~\ref{tab:scenarios}. The system in question consists of three
main components. Data Sources provide data, Aggregators perform
computation over the data, and Routers supply aggregators with
data. Each of the system participants can be trusted (T) or untrusted
(U). Traditionally various research projects have studied the scenario
in which all system participants are trusted (row 1). These projects
have laid the foundation for out work. However, many realistic
scenarios not all system components can be trusted to perform their
operations correctly.

On one extreme of the possible spectrum are environments in which the
data source nodes cannot be trusted to provide correct data. This is
an extremely serious problem, because no matter whether the rest of
the system components are trusted or not, it is impossible to ensure
any correctness of the end result. This situation can be improved if
there exist mechanisms to verify the correctness of individual data
sources. However such mechanisms need to make use of historic
information and internal state transitions. Trust but Verify~\cite{}
is a project aimed at solving this particular part of
problem.

Moving further, we have three different scenarios which have not been
addressed so far. Our work focuses on the scenario corresponding to
the last row of the table. 
  

(Our Classification)
\begin{table}[htpb!]
  \begin{tabular}{|c|c|c|c|}
    \hline
    & No Trust & Trust but Verify & Full Trust\\
    \hline
    Querier Computes & Q1 & Q2 & Q3\\
    Network Computes & N1 & N2 & N3\\
    \hline
  \end{tabular}
\end{table}

There are two main approaches to compute an aggregate---have the
querier perform all computation (first table row), or have the network do
all computation. There are three main trust categories. No trust
describes situations in which no party is trusted to perform any
computation correctly, Trust but Verify describes situation in which
actors can be trusted but their work is subject to verification, and
full trust describes scenarios in which everyone is trusted to perform
computations correctly.

Querier Computation Based Solutions:\\
Solutions in this part of the design space have the querier to be the
only entity performing computation over all (some) of the data. There
are two main approaches in this class: make use of all data (provides
an absolute result), or utilize only a sample of the data (provides
relative result). Each of the two approaches is associated with
different cost and the sampling one is generally less expensive, but
also less accurate. There are different mechanism to collect the data
to be used in the computation. In the No Trust region (Q1) the
querier personally fetches the desired data, while in Q2 the querierier
specifies the desired size of the data and verifies that the delivered
data satisfies its requirements. 


Network Computation Based Solutions:\\
Solutions in this part do not involve the querier in the computational
process. In the No-Trust scenario, the querier uses redundancy to
obtain higher confidence about the correctness of the result that the
network provides. Redundancy can be expressed as either running the
same algorithm multiple times, or trying different algorithms. Of
particular interest is the last design space region N2. Solutions in
this region have the network compute the desired aggregation but
involve some kind of protocol in which the querier verifies the result
and asserts its correctness. 


Solutions by range and their results:

\begin{enumerate}
  \item [Q1] Querier Computes all, No Trust
    \begin{enumerate}
      \item Establish a connection with each data source, obtain its
      data, compute the result. Messages: $O(n)$, Computation: $O(n)$.
      
      \item Establish a connection with a predermined number of
      randomly chosen data source nodes. Extrapolate the result from
      the observed values: $O(sample size)$, Computation: $O(sample size)$.
      
    \end{enumerate}
  \item [Q2] Querier Computes all, Trust but Verify
    \begin{enumerate}
      
      \item The network collects all data and delivers it to the
      querier.

    \item The network collects a random sample and delivers it to
      the querier
    \end{enumerate}
  \item [N1] Network computes all, No Trust
    \begin{enumerate}
      \item Run the same algorithm multiple times and compare results
      \item Run different algorithms and compare results
    \end{enumerate}
  \item [N2] Network Computes all, Trust but Verify
    \begin{enumerate}
      \item Single Tree Aggregation with summing and sampling
      \item Single Tree Aggregation with modified Flajolet Martin
        and Sampling
      \item Multiple trees with no overlap
      \item Multiple trees with overlap
      \item Itermediary verification
    \end{enumerate}  
\end{enumerate}


Techniques:
\begin{enumerate}
  \item Redundancy
  \item Spot Checking
  \item Data Authentication
  \item Intermediate verification
  \item Different Heuristics
\end{enumerate}


\section{Butterfly Aggregation Hierarchy}
Instead of using a single aggregation hierarchy, it is possible to use
multiple hierarchies and redundancy to improve the quality and
reliability of aggregation. Let $a=m^l$. We can construct an m-way
Butterfly network by placing all aggregator nodes at each level of the network
and connecting each node on a given level to $m$ nodes on the level above
it, such that one of the nodes is the same node, and the other $m-1$
nodes are at specific distance, depending on the Butterfly level. Let
the bottommost Butterfly level be level 0. For $m^l$ nodes on 
every level, we have a Butterfly of a total of $l+1$ levels. 

We can perform aggregation using the Butterfly network. At level 0,
each aggregator node receives $n/a$ values from the corresponding data
sources and computes their sum. Having computed the sum, the
aggregator sends the result to the aggregator nodes on level 1 that it
is connected to. An aggregator at level 1, waits for its inputs to
arrive, computes their sum and forwards them to the next level. 

\begin{lem} If all aggregations have been performed correctly, the
  values reported by the butterfly nodes on the last level are the
  same and are equal to the total sum of all data source nodes.
\end{lem}

\begin{proof}
  An aggregator at level 0 computes the count of $d/a$ distinct data source
  nodes. An aggregator at level 1 receives data from  $m-1$ level 0
  aggregations to compute a sum over $mn/a$ distinct data source
  nodes. In the same way the aggregators on the last level ($l$)
  compute a sum over $m^ln/a = an/a = n$ distinct data source nodes.
\end{proof}


\begin{defn}
{\it Safe Cheating} - malicious actions of a single aggregator nodes
that affect in the same way the final values of all nodes at the
last Butterfly level. Safe cheating is indistinguishable be inspecting
only the values produced at the last Butterfly level.
\end{defn}

\begin{lem}
The result reported by an aggregator at level $i$ of the Butterfly
affects the outputs of $m^{l-i}$ aggregator nodes on the last
Butterfly level.
\end{lem}

\begin{proof}
At level $0$ we can reach any node on the last Butterfly level. This
follows from the construction of the Butterfly network - at each level
we shorten the distance by $1/m$ and in $l = \log_m{a}$ steps we can
reach the desired destination. Using the same reasoning, once we
are on level $1$, we have access to only $1/m$ fraction of the nodes
on the last Butterfly level. Using induction we can prove the claim of
the lemma.
\end{proof}

\begin{cor}
Cheating on level 0 is safe
\end{cor}


\begin{cor}\label{cor:no_safe}
Cheating on a level other than 0 is not safe.
\end{cor}

From Corrolary~\ref{cor:no_safe} it follows that if a node wants to
cheat at a level $i > 0$ it needs to co-operate with other nodes on
the same level. At level $i$, this process requires $m^i$ accomplices
at fixed positions on level $i$. Let there be $m^k$ malicious nodes
and let's assume that they are located on level $k$ in such a way so
that if they all decide to cheat, cheating will be safe. If the
cheating on level $k$ is safe, then so is cheating on any level $0 \leq i \leq k$.

When we are doing count, the last observation does not give additional
strength to the attacker. With count each data source reports a value
in $\{0, 1\}$, thus at each level of the aggregation hierarchy we have
fixed bounds on the size of the intermediary results. The fact that
$m^k$ accomplices can cheat on all levels $0 \leq i \leq k$, does  not
give the attackers additional advantage: if cheating takes place on
level $j < k$, with the maximum possible amount, then no cheating is
possible on the levels higher than $j$.   

%Figure~\ref{fig:inv.butterfly} 
%
%\begin{figure*}[htpb!]
%\centering
%\includegraphics[clip=true,width=8cm]{figures/inv.butterfly.eps} 
%B\caption{\small \label{fig:inv.butterfly} Request Execution. }
%\end{figure*}



\end{document}
