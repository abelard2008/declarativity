\documentclass{article}

\title{Accountable Aggregation}

\author{Aydan Yumerefendi}

\begin{document}

\section{General Setting}
Asuume a public trusted infrastructured Distributed Hash Table
(DHT). The DHT is managed by a central authority that controls the
code and the correctness of each opearation. The infrastructure
guarantees that no message is deliberately delayed, spoofed, or
modified---if A wants to send a message to B, the DHT will protect the
integrity of the message and will do a best effort to deliver it to
the end host. Additionally, the DHT can provide a short-lived trusted
write-only storage---clients can publish a data item under a given key
and will not be allowed to change its value until a certain time
interval has elapsed.

Assuming that an infrastrcuture with the above properties exists, it
is interesting to examine what kind of applications can be built on
top of it. In particular, we are interested in distributed data processing: a
large number of data sources have some data under their
control. Individual entities can request some operations to be
performed over the data. All these operations can be executed by
supplying the data to the client requesting the operation. However,
this approach is quite inefficient and does not scale. Ideally, we
would like to distribute the work done to execute the operation over
data. By having multiple nodes perform the same opeartion over smaller
data sizes, it is possible to scale to large data volumes. This
efficiency comes at a price---we can no longer trust the end result
and the results at each intermediary step.

While it is possible for applications to provide incorrect results,
the centralized control over the underlying infrastructure provides some
powerful building blocks that could potentially be used to ensure
bounds on the correctness of applications built on top of the
infrastructure. With proper mechanisms in place it might be possible
to verify the correctness of individual operations. Deterministic
verification is the ideal goal, but for a large number of applications
even probabilistic guarantees are sufficient. The key to probabilistic
guarantees is the use of randomization during the process of
verification to thwart determininistic attacks and behavior.


\section{Terminology}
In our environment there a large number of \emph{data sources} that
control some sets of data. These nodes can be simple data storage
nodes with limited resources. \emph{Aggregator} nodes receive data
from data sources or other aggreagator nodes and perform the desired
operation over the obtained data. Having performed the desired
operation, they forward the result further in the
hierarchy. A \emph{client} sends a request to all 
nodes, asking for the execution of a given function over all existing
data. Results flow from the data sources to the client through a hierarchy of
aggregators.

\section{Threats}
The above described model of execution faces a number of threats:
\begin{itemize}
  \item Aggregators can omit data received from other nodes
  \item Aggregators can include invalid data in the computation
  \item Aggregators can report invalid results
\end{itemize}


\section{Basic Tools}
Hash trees
Spot-Checkers

\section{General Model of Aggregation}
The client distributes the query to all active data sources. During
the distribution process, randon nodes are elected as
aggregators. Once the query reaches a data source, the data source
sends to its aggreagtor the data that it stores (or the result of
applying the desired function over it). As of now assume that the data
source is always correct. Once the aggregator receives the data from
all of its child nodes (within a given time bound), it first commits
to the received data. To commit to the received data, an aggregator
organizes data into a hash tree using node id as a key. Once the root
of the tree is constructed, the aggregator publishes the authenticator
of the tree in the DHT. 

Once an aggregator publishes its authenticator, the nodes, which sent
data to it decide randomly whether to verify the correctness of the
committment. The random decision is such so that
$O(\frac{1}{\epsilon})$ nodes on the average decide to query the
committment. If any of the verifications fails, the corresponding node
raises an alarm. This process ensures that at least $(1-\epsilon)$
fraction of all values are included by the aggregator. With functions
that can be incrementaly computed by executing the over each
individual argument (e.g. sum, min, max), it is possible to augment
the tree to proove that the reported data was indeed included in the
tree. We still need to see what other functions fall in this category.

With hierarchical aggregation, aggregators that are located more than
one level above the data sources receive data from the aggregators
bellow them. To prevent aggregators from sending different results to
their parent and children, higher level aggregators read their inputs
from the DHT. In this scenario the lower level aggregators participate
in the same protocol as the data source nodes to ensure that the
higher level aggregator includes all data. 

To complete the process it is neccessary to prevent aggreagtors from
including in their computation invalid data, not received from a valid data source or
aggreagator. To do this, higher level aggregators, choose random
committed elements for each of their lower level aggregators and
inspects them for validity. Take $O(\frac{1}{\epsilon})$ samples










\end{document}