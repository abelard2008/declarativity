
\subsubsection{Evaluation Results}

%\subsection{Magic Sets and Predicate Reordering}
%\label{subsec:expr:caching}

\begin{figure}
  \centering
  \includegraphics[width=2.6in]{graphs/queryBW_random100}
  \caption{\label{ms-bw}\emph{\small Aggregate communication overhead (MB) with and
    without magic sets and caching}.}
\end{figure}


We study the effectiveness of combining the use of magic sets and
predicate reordering for lowering communication overhead when the
queries are constrained by randomly chosen sources and
destinations. Our workload consists of queries that request
source-to-destination paths based on the {\em Hop-Count} metric. For
each query, we execute the {\em magic-shortest-path} query
(Section~\ref{sec:magic}).

Figure~\ref{ms-bw} shows the aggregate communication overhead as the
number of queries increases.  The {\em No-MS} line represents our
baseline, and shows the communication overhead in the absence of
rewrites (this essentially reduces to computing all-pairs
least-hop-count). The {\em MS} line shows the communication overhead
when running the optimized query with no sharing across queries. When
there are few queries, the communication overhead of {\em MS} is
significantly lower than that of {\em NO-MS}. As the number of queries
increases, the communication overhead of {\em MS} increases linearly,
exceeding {\em No-MS} after $170$ queries.


We start from the base rules NR1 and NR2 used in our first {\em
  Network-Reachability} example from
Section~\ref{sec:firstExample}. That example computes {\em all-pairs
  paths}. In practice, a more common query would compute {\em
  all-pairs shortest paths}. By modifying NR2 and adding rules BPR1
and BPR2, the following {\em Best-Path} query generalizes the
all-pairs shortest paths computation, and computes the best paths for
any path metric {\em C}:

\vspace{2pt}
{\small
\noindent{\bf {\#include(NR1)}} \\
{\bf NR2: } path(\underline{S},D,P,C) :-
  link(\underline{S},Z,C$_{1}$), \\
  \datalogspace path(\underline{Z},D,P$_{2}$,C$_{2}$), {\bf C = f\_compute(C$_{1}$,C$_{2}$)}, \\
  \datalogspace P =
  $f\_concatPath$(link(\underline{S},Z,C$_{1}$),P$_{2}$), \\
{\bf BPR1:  bestPathCost(\underline{S},D,AGG$<$C$>$) :- path(\underline{S},D,P,C).} \\
{\bf BPR2:  bestPath(\underline{S},D,P,C) :-
  bestPathCost(\underline{S},D,C), \\
\datalogspace path(\underline{S},D,P,C).} \\ 
{\bf Query: } bestPath(\underline{S},D,P,C).
}
\vspace{2pt}
 
We have left the aggregation function $(AGG)$ unspecified. By changing
$AGG$ and the function $f\_compute$ used for computing the path cost
{\em C}, the {\em Best-Path} query can generate best paths based on
any metric including link latency, available bandwidth and node
load. For example, if the query is used for computing the shortest
paths, $f\_sum$ is the appropriate replacement for $f\_compute$ in
rule BPR1, and $min$ is the replacement for $AGG$.  The resulting
bestPath tuples are stored at the source nodes, and are used by
end-hosts to perform source routing.

The two added rules BPR1 and BPR2 do not result in extra messages
being sent beyond those generated by rules NR1 and NR2.  This is
because path tuples computed by rules NR1 and NR2 are stored at the
source nodes, and bestPathCost and bestPath tuples are generated
locally at those nodes. Instead of computing the best path between any
two nodes, this query can be easily modified to compute {\em all}
paths, {\em any} path or the {\em Best-k} paths between any two nodes.

Similar to the {\em Network-Reachability} example, we can add an extra
predicate $f\_inPath(P_{2},S) = false$ to rule NR2 to avoiding
computing best paths with cycles. We can further extend the rules from
the {\em Best-Path} query by including constraints that enforce a QoS
requirement specified by end-hosts. For example, we can restrict the
set of paths to those with costs below a loss or latency threshold
{\em k} by adding an extra constraint {\em C$<$k} to the rules NR1 and
NR2.


\subsubsection{Dynamic Source Routing}
\label{sec:dsr}

All of our previous examples use what is called {\em right} recursion,
since the recursive use of $path$ in the rule (NR2, DV2) appears to
the right of the matching $link$. The query semantics do not change if
we flip the order of $path$ and $link$ in the body of these rules, but
the execution strategy does change. In fact, using {\em left
  recursion} as follows, we implement the Dynamic Source Routing (DSR)
protocol~\cite{dsr}:

\vspace{2pt}
{\small
\noindent{{\bf {\small \#include(NR1)}}} \\
{\bf DSR1: } path(\underline{S},D,P,C) :- {\bf
  path(\underline{S},Z,P$_{1}$,C$_{1}$), link(\underline{Z},D,C$_{2}$),} \\
\datalogspace  P = $f\_concatPath$(P$_{1}$, link(\underline{Z},D,C$_{2}$)), \\
\datalogspace C = C$_{1}$ + C$_{2}$. \\
{\bf Query: } path(\underline{S},D,P,C).
}
\vspace{2pt}

Rule NR1 produces new one-hop paths from existing link tuples as
before. Rule DSR2 matches the destination fields of newly computed
path tuples with the source fields of link tuples. This requires newly
computed path tuples be shipped by their destination fields to find
matching links, hence ensuring that each source node will recursively
follow the links along all reachable paths. Here, the function
$f\_concatPath(P,L)$ returns a new path vector with {\em L} appended
to {\em P}. These rules can also be used in combination with BPR1 and
BPR2 to generate the best paths. By adding two extra rules not shown
here, we can also express the logic for sending each path on the
reverse path from the destination to the source node.



%\subsubsection{Magic Sets with Caching}
%\label{subsec:expr:magic}

In addition, Figure~\ref{ms-bw} also illustrates the effectiveness of
caching (Section~\ref{subsec:multiQuerySharing}). The {\em MSC} line
shows the aggregate communication overhead for magic sets with
caching. For fewer than $170$ queries, there is some overhead
associated with caching. This is due to false positive cache hits,
where a cache result does not contribute to computing shortest
paths. However, as the number of queries increases, the overall cache
hit rate improves, resulting in a dramatic reduction of bandwidth.
When limiting the choice of destination nodes to $30$\% ({\em
MSC-30\%}) and $10$\% ({\em MSC-10\%}), the communication overhead
levels of at $1.8$ MB, and $1$ MB, respectively. The smaller the set
of requested destinations, the higher the cache hit rate, and the
greater the opportunity for sharing across different queries.  These
results are consistent with the results obtained by Loo
\textit{et~al.}~\cite{declareRoute} in a similar experiment, using the
PIER~\cite{pierCidr} simulator.


%\subsection{Language Features}

\eat{

An \Overlog program is largely composed of table declaration statements
and rules; we consider each in turn. In \Overlog, all input relations
and rule derivations are stored in {\em materialized} tables. Unlike
Datalog, tables must be defined explicitly in \Overlog via
\nd{materialize} statements, which specify constraints on the size and
lifetime of tuple storage -- any relations not declared as tables are
treated as named {\em streams} of tuples. Each \nd{materialize(name,
  lifetime, size, primary keys)} statement specifies the relation name,
lifetime of each tuple in the relation, maximum size of the relation,
and fields making up the primary key of each relation\footnote{We have a
  convention of starting the offset by 1 in the \Sys system, as 0 is
  reserved in the implementation for the table name.}. If the primary
key is the empty set \nd{()}, then the primary key is the full set of
fields in the relation.  For example, in the {\em Shortest-Path-Hop}
\Overlog program, all the tables are specified with infinite sizes and
lifetimes.

The execution of \Overlog rules will result in the derivation of tuples
that are stored in materialized tables. For the duration of program
execution, these materialized results are incrementally recomputed as
the input relations are updated. For example, the update of \nd{link}
tuples will result in new derivations and updates to existing \nd{path},
\nd{spCost} and \nd{shortestPathHop} tuples. In addition, if an \Overlog
rule head is prepended with an optional keyword {\em delete}, the
derived tuples are used to delete an exact match tuple in its relation
instead.




}



\eat{
Let $FP_{S}(p)$ and $FP_{P}(p)$ denote the result set for $p$ for
using SN and PSN respectively. We show that:

\vspace{1pt}
\noindent{\bf Theorem~\ref{theorem:nonLinearEq}: } {\em $FP_{S}(p)=FP_{P}(p)$}\\
\noindent{\bf Theorem~\ref{theorem:dupnl}: } {\em There are no
  repeated inferences in computing $FP_{P}(p)$.} 
\vspace{1pt}

In order to compute rules with aggregation (such as SP3), we utilize incremental
fixpoint evaluation techniques~\cite{rossAggregate} that are amenable to
pipelined query processing. These techniques can compute {\em monotonic
  aggregates} such as $min$, $max$ and $count$
incrementally based on the current aggregate and each new input tuple. 
We omit the details for lack of space.
}




\eat{In Algorithm~\ref{alg:ruleLocal}, we summarize the general rewrite
technique for an input set of link-restricted rules R. In the
pseudocode, for simplicity, we assume that the location specifiers of all the body
predicates are sorted (@S followed by @D); this can be done as a
preprocessing step. The algorithm as presented here assumes that all links are
bidirectional, and may add a \link(@D,@S) to a rewritten rule to
allow for backward propagation of messages.

\eat{
\vspace{2pt}
{\scriptsize
\begin{Algorithm}[ht]
  \begin{programbox}
    \PROC |RuleLocalization| (R)
     \WHILE \exists | rule r |\in R|: |h(@L,...) :- |\link(@S,@D,...),|
     |\manyquads p|_{1}|(@S,..),..,p|_{i}|(@S,...),|
     |\manyquads p|_{i+1}|(@D,...),..,p|_{n}|(@D,..)| 
%     |\manyquads | \wedge | (| @L=@S | | \vee | | @L=@D )
            R.remove(r)	   
	    R.add(hS(@S,@D,..) :- |\link(@S,@D,..),..,p|_{i}|(@S,..).)|
	    R.add(hD(@D,@S,..) :- hS(@S,@D,..).)
	    \IF @L=@D 
	    \THEN R.add(|h(@D,..) :- hD(@D,@S,..),|
            |\manyquads p|_{i+1}|(@D,..),..,p|_{n}|(@D,..).|)
	    \ELSE R.add(|h(@S,..) :- \link(@D,@S),hD(@D,@S..),|
               |\manyquads p|_{i+1}|(@D,..),..,p|_{n}|(@D,..).|) 
%	       \IF \link.bidirectional
%	       | |\ELSE
%	       error(``illegal rule'')	       
%	       \FI
%            \FI
%     \END
%    \END      
\end{programbox}
\caption{Rule Localization Rewrite}
\label{alg:ruleLocal}
\end{Algorithm}
}
}
\vspace{2pt}

\begin{Claim}\label{claim:ruleLocal} Every link-restricted \Dlog program, when rewritten using
  Algorithm~\ref{alg:ruleLocal}, produces an equivalent program where
  the following holds:
\begin{enumerate}
\item The body of each rule can be evaluated at a single node.
\item The communication required to evaluate a rule is limited to
	sending derived tuples over links from a link relation.
\end{enumerate}
\end{Claim}

The equivalence statement in the above claim can be easily shown,
by examining the simple factoring of each removed rule into two parts. The
remainder of the claim can be verified syntactically in the added rules.
% 
%   because the algorithm generates rewritten
% rules whose body predicates have the same location specifier, and all
% non-local rules generates a tuple that is communicated forwards
% (\link(@S,@D)) or backwards (\link(@D,@S)) based on the input link
% predicate of the input rule. 
}



\eat{

}



The execution model of declarative networks is based on a distributed
variant of the standard evaluation technique for Datalog programs that
is commonly known as {\em semi-\naive} (SN)
evaluation~\cite{declareNetworks}, with modifications to enable
pipelined asynchronous evaluation suited to a distributed setting.
Reference~\cite{declareNetworks} provides details on the implementation
and execution model of declarative networking.

Unlike traditional semi-\naive\ evaluation, PSN does not require
computations in synchronous rounds (or iterations), a prohibitively
expensive operation in distributed settings.  We consider the following
recursive Datalog rule:

% \begin{equation}\label{eqn:central}

%\vspace{-0.1in}
\begin{center}$d $ :- $ d_{1}, d_{2}, ..., d_{n}, b_{1}, b_{2}, ..., b_{m}$\end{center}
%\vspace{-0.1in}

\noindent where there are $n$ derived predicates ($d_{1},...,d_{n}$),
and $m$ {\em base predicates} ($b_{1},...,b_{m}$) in the rule body.
Derived predicates refer to intensional relations that are derived
during rule execution, and may be mutually recursive with $d$.  Base
predicates refer to extensional (stored) relations whose values are not
changed during rule execution.  In PSN, a delta rule is generated for
each derived predicate, where the $k^{th}$ {\em delta rule} is of the
form:
%\vspace{-0.1in}
\begin{center}$\triangle d$ :- $ d_{1},..,d_{k-1},\triangle d_{k}, d_{k+1},..,d_{n},
b_{1},b_{2},...,b_{m}$\end{center}

\noindent where $\triangle d_{k}$ denotes a tuple $t_{k} \in d_{k}$ that is used as
input to the rule for computing new $d$ tuples.

In the simplest version of PSN with no buffering, tuples are processed
tuple-at-time in a pipelined fashion.  Each node maintains a FIFO queue
(ordered by arrival timestamp) of new input tuples.  Each new tuple is
dequeued and is used as input to its respective a delta rule.  The
execution of a delta rule may generate new tuples which are either
inserted into the local queue or sent to a remote node for further
execution.  Duplicate evaluations are avoided using local arrival
timestamps, where each new tuple is only processed with tuples with
older timestamps.


\reminder{Add PSN, semi-naive evaluation, and localization rewrite via
  exmaple. Proofs in paper.}


r1 path(S,D,D,C) :- link(S,D,C).
r2 path(S,D,Z,C) :- link(S,Z,C1), path(Z,D,Z2,C2), C = C1 + C2.
r3 spCost(S,D,min<C>) :- path(S,D,Z,C).
r4 shortestPathHop(S,D,C) :- spCost(S,D,C), path(S,D,Z,C).
Query shortestPathHop(S,D,Z,C).
\caption{{\small Shortest-Path-Hop Datalog program.}\label{fig:background:shortestPath}}



In this section, we will provide a brief overview of declarative
networking.  Declarative networks are implemented using {\em Network
  Datalog} (\Dlog), a distributed logic-based recursive query language
first introduced in the database community for querying network graphs.


We first provide a short review of Datalog, following the conventions in
Ramakrishnan and Ullman's survey~\cite{ramakrishnan93survey}. A Datalog
program consists of a set of declarative {\em rules}.  Each rule has the
form \nd{p :- q1, q2, ..., qn}., which can be read informally as
``\nd{q1} and \nd{q2} and \nd{...}  and \nd{qn} implies \nd{p}''.  Here,
\nd{p} is the {\em head} of the rule, and \nd{q1}, \nd{q2},...,\nd{qn}
is a list of {\em literals} that constitutes the {\em body} of the rule.
Literals are either {\em predicates} with {\em attributes} (which are
bound to variables or constants by the query), or boolean expressions
that involve function symbols (including arithmetic) applied to
attributes.  In Datalog, rule predicates can be defined with other
predicates in a cyclic fashion to express recursion.  The order in which
the rules are presented in a program is semantically immaterial;
likewise, the order predicates appear in a rule is not semantically
meaningful.  Commas are interpreted as logical conjunctions (\nd{AND}).
The names of predicates, function symbols, and variable names begin with
an upper letter, while constants names begin with an lowercase letter.
An optional \nd{Query} rule specifies the output of interest (i.e.
result tuples).


\subsection{Overlog}

%\reminder{Should we talk about link restricted rules here? I would think
%  not.}
We illustrate \Dlog using a simple example of two rules that computes
all pairs of reachable nodes:

\begin{NDlog}
r1 reachable(@S,N) :- link(@S,N).
r2 reachable(@S,D) :- link(@S,N), reachable(@N,D).
Query reachable(@S,D).
\end{NDlog}


The rules \nd{r1} and \nd{r2} specify a distributed transitive closure
computation, where rule \nd{r1} computes all pairs of nodes reachable
within a single hop from all input links (denoted by 
\nd{neighbor}), and rule \nd{r2} expresses that ``if there is a link
from \nd{S} to \nd{N}, and \nd{N} can reach \nd{D}, then \nd{S} can
reach \nd{D}.''  
% By modifying this simple example, we can construct
% more complex routing protocols, such as the distance vector and path
% vector routing protocols.

\Dlog supports a {\em location specifier} in each predicate, expressed
with the \nd{@} symbol followed by an attribute. This attribute is
used to denote the source location of each corresponding tuple. For
example, all \nd{reachable} and \nd{link} tuples are stored based on
the \nd{@S} address field. The output of interest is the set of all
\nd{reachable(@S,D)} tuples, representing reachable pairs of nodes
from \nd{S} to \nd{D}.

\subsection{Link-restricted Rules}

Used in network  Datalog. 


\subsection{Soft-state Storage Model}
\label{sec:dn:softstate}

Declarative networking incorporates support {\em
  soft-state}~\cite{raman99model} derivations commonly used in
networks. In the soft state storage model, all data (input and
derivations) has an explicit ``time to live'' (TTL) or lifetime, and all
tuples must be explicitly reinserted with their latest values and a new
TTL, or they are deleted.

To support this feature, an additional language feature is added to the
\Dlog language, in the form of a
\nd{materialized}~\cite{declareOverlays} keyword at the beginning of
each \Dlog program that specifies the TTL of predicates. For example,
the definition \nd{materialized(link, \{1,2\}, 10)} specifies that the
\nd{link} table has its primary key set to the first and second
attributes (denoted by \nd{\{1,2\}})\footnote{Following the conventions
  of the \Pitu declarative networking system, attribute 0 is reserved
  for the predicate name.} and each \nd{link} tuple has a lifetime of 10
seconds.  If the TTL is set to infinity, the predicate will be treated
as {\em hard-state}.

The soft-state storage semantics are as follows. When a tuple is
derived, if there exists another tuple with the same primary key but
differs on other attributes, an {\em update} occurs, in which the new
tuple replaces the previous one. On the other hand, if the two tuples
are identical, a {\em refresh} occurs, in which the existing tuple is
extended by its TTL.

For a given predicate, in the absence of any \nd{materialize}
declaration, it is treated as an {\em event} predicate with zero
lifetime. Since events are not stored, they are primarily used to
trigger rules periodically (via a special \nd{periodic} predicate) or in
response to network events. For example, utilizing \Sys's~\cite{p2}
built-in \nd{periodic} keyword , node \nd{X} periodically generates a
\nd{ePing} event every 10 seconds to its neighbor \nd{Y} denoted in the
\nd{link(@X,Y)} predicate:

\begin{NDlog}
ePing(@Y,X) :- periodic(@X,10), link(@X,Y).
\end{NDlog}

\eat{
\alignauthor
Raghu Ramakrishnan\\
       \affaddr{Institute}\\
       \affaddr{Address}\\
       \email{Email}

\alignauthor
Timothy Roscoe\\
       \affaddr{Institute}\\
       \affaddr{Address}\\
       \email{Email}

\alignauthor
Ion Stoica\\
       \affaddr{Institute}\\
       \affaddr{Address}\\
       \email{Email}
\numberofauthors{6} %  in this sample file, there are a *total*
% of SIX authors and all of them fit neatly on the first page.
% As said, all authors get 'equal billing' and you should fit all of them on the opening page
% in the 'byline'. The production/editorial-staff will 'separate' names from their affiliations, leaving
% author names beneath the title (in the byline), and moving the affilations/contact information to an area
% after the references at the back of the article.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Boon Thau Loo\\
       \affaddr{Institute}\\
       \affaddr{Address}\\
       \email{Email}
% 2nd. author
\alignauthor
Tyson Condie\\
       \affaddr{Institute}\\
       \affaddr{Address}\\
       \email{Email}

\alignauthor
Minos Garofalakis\\
       \affaddr{Institute}\\
       \affaddr{Address}\\
       \email{Email}

\alignauthor
Minos Garofalakis\\
       \affaddr{Institute}\\
       \affaddr{Address}\\
       \email{Email}

\alignauthor
Joseph M. Hellerstein\\
       \affaddr{Institute}\\
       \affaddr{Address}\\
       \email{Email}

\alignauthor
Petros Maniatis\\
       \affaddr{Institute}\\
       \affaddr{Address}\\
       \email{Email}
}


}
