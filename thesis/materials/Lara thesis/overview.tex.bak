\chapter[Dissertation Overview]{Dissertation Overview}

Data communication, whether over space (e.g. Ethernet, satellite
broadcasting) or over time (e.s. storage applications) is inherently
noisy. The goal of the channel coding theory is to provide
systematic ways of introducing a controlled amount of redundancy
into the transmitted data in a form of a channel code to
successfully overcome the noise in the channel \cite{wicker}.
Channel coding theory is traditionally concerned with constructing
good codes capable of correcting additive errors, since the noise in
the channel is almost always taken to be additive.

A good performance metric is that of the minimum distance of a code,
which is the minimum separation of the codewords in the given code,
since it determines how large the additive noise in the channel can
be while communication still remains reliable. Thus, a good code
traditionally has large minimum distance.

In addition, good channel codes should also be equipped with simple
and realizable encoding and decoding algorithms to make them useful
in practice. In particular, good decoding algorithms should mimic
the performance of the optimal maximum likelihood decoding, but do
so with much lower complexity than the exponential complexity of the
highly impractical maximum likelihood decoding algorithm. %, whose
%complexity makes it highly
%impractical. %Iterative decoding algorithms are an example of low
%complexity decoding algorithms that (empirically) perform well.

 In this dissertation we
develop novel coding theoretic approaches for two somewhat
non-standard yet relevant problems for modern communication systems.
In the first part of the thesis we address the issue of reliable
communication under varying sampling rate, while in the second part
we focus on understanding the performance of low density parity
check (LDPC) codes in the low bit error rate (BER) region under
iterative decoding. The underlying theme of this work is that the
notion of the minimum distance of a code needs to be rethought when
the standard assumptions on the communication no longer hold.

In particular, when one can no longer assume perfect signal
synchronization and use the code and its decoding algorithm solely
to overcome additive errors, the notion of the relevant code metric
needs to be redefined. A simple example is that of a code consisting
of only two codewords of equal length $n$, the codeword $c_1$ being
a string of alternating 0's and 1's and the codeword $c_2$ being a
string of alternating 1's and 0's. These two strings have maximum
possible additive (Hamming) distance, since they differ in every
position. However, when this code is used over a channel that
introduces a repetition error, their post-repetition distance drops
to only 2, since, for example, a repetition in the first bit of
$c_1$, and in the last bit of $c_2$, produces strings that differ in
only two positions, thus making the original code not nearly as
effective for communicating over a channel that introduces additive
as well as a repetition error.

%This point of view inspired some of the results presented in the
%first part of the dissertation, as we now further discuss.

More generally, current trends in many emerging applications require
timing recovery to be performed under increasingly stringent
constraints. In digital data storage applications the continuously
increasing user  demand requires higher storage capacity and higher
data rates while keeping the disk size the same.  While this can be
accomplished using advanced coding and signal processing techniques
\cite{vasic:05}, this leads to the timing recovery block consuming
an increasing large fraction of on-chip resources. Low power
wireless applications also demand that accurate synchronization be
performed under limited power and constrained chip area
\cite{ammer}.

Sampling errors caused by poor timing recovery, such as repetitions
or deletions of symbols, severely impact the decoder's performance
and can undermine the benefits of other system components, since
other components are traditionally not designed to deal with
synchronization errors. As an alternative to developing more complex
and more expensive timing recovery schemes, we adopt a coding
theoretic point of view in addressing this problem.
Chapter~\ref{intro1} elaborates on the relevant background and
introduces the set-theoretic viewpoint in modelling synchronization
errors. We then investigate how to modify additive error correcting
codes, such as Reed-Muller codes, to ensure that they would, in
addition to excellent additive error correction properties, be
equipped with good synchronization error correction capabilities.

We focus on the first order Reed-Muller codes as a representative of
a class of highly structured additive error correcting codes with
large minimum distance. For the Reed-Muller codes, we first
establish and prove several novel properties of the run-length
structure of this code. Chapter ~\ref{reed-muller-struc} contains
several structural properties of these codes, which are then used in
Chapter~\ref{reed-muller-perfm} in a systematic analysis of the
performance of these codes in channels which in addition to additive
errors also permit a repetition or a deletion of a symbol. The
runlength properties of Reed-Muller codes proved in this thesis may
be also of independent interest. Chapter~\ref{reed-muller-perfm}
explains how to systematically thin the original Reed-Muller code to
substantially improve the performance of these codes when the
inadequate synchronization causes a repetition or a deletion of a
coded bit, while only losing one information symbol. We also derive
appropriate post-deletion and post-repetition distance and propose a
modified bounded distance decoding algorithm suitable for decoding
thinned Reed-Muller codes transmitted over the channels that
introduce additive noise and a synchronization error. This algorithm
is a variant of the Hadamard transform-based bounded distance
decoding algorithm traditionally used to decode Reed-Muller codes.
It also features the same complexity as this traditional algorithm,
while also being able to correct for a synchronization error.

Motivated by the problem of communicating in the presence of
repetition errors, in Chapter~\ref{numbertheory} we focus on
explicit number-theoretic constructions of a set of strings immune
to multiple repetitions. In particular, we propose an explicit
number-theoretic construction of a set of strings immune to a single
repetition, and then subsequently, a construction of a set of
strings immune to multiple repetitions. The former construction is
asymptotically optimal, whereas the latter is within a constant of
the upper bound on the cardinality of such a set, and improves on
the previously best known construction due to Levenshtein
\cite{lev:66a}.

Using the number-theoretic construction of
Chapter~\ref{numbertheory}, we then in Chapter~\ref{prefixing}
propose a general method for transforming a code, as an arbitrary
collection of binary strings of equal length, by prepending each
string in this collection with a  prefix, carefully chosen to
improve the immunity to synchronization errors. The prefix itself is
chosen based on the number-theoretic methods of
Chapter~\ref{numbertheory} and is shown to have length that is only
logarithmic in the length of the strings in the original collection.
The proposed method therefore provides a general way to reach
guaranteed immunity to repetition errors with asymptotically
negligible redundancy. We also provide a decoding algorithm, as a
variant of a message passing algorithm, capable of decoding additive
as well as repetition errors. The proposed algorithm does so without
the increase in complexity over the traditional message passing
decoding algorithm designed to handle additive errors only. This
result concludes the first part of the dissertation.


In the second part of the dissertation we turn to a different
problem where we study the performance of low density parity check
(LDPC) codes in the low BER regime under iterative decoding. LDPC
codes are a class of additive error correcting codes that operate
close to the Shannon limit, which is the absolute limit on the rate
that can be achieved over a noisy channel. LDPC codes were invented
by Gallager \cite{gallager} in the 1960's, but then were largely
forgotten until early 1990's. Their rediscovery \cite{mackay96},
\cite{foss01} sparked research interest in LDPC codes, as well as
their wide consideration for many modern applications.

LDPC codes have a convenient graphical representation which makes
them particularly suitable for low complexity, iterative decoding.
Finite length LDPC codes and their iterative decoding algorithms
have found tremendous success when used in the moderate BER region,
of say $10^{-6}$ and above, and vast empirical evidence supports
this finding \cite{lincostello}. However, the suboptimal nature of
these highly efficient decoding algorithms on non-tree graphs (as
arise from the graphical representation of most LDPC codes) has also
been troublesome when LDPC codes are considered for applications
that need to operate at very low bit error rates.

Namely, some LDPC codes that have excellent performance in the
moderate BER region exhibit a change in the slope of the BER vs.
signal to noise ratio (SNR) curve in the very low BER region, a
region which is of interest for many practical applications,
implying that a significant increase in the signal power is needed
for only a marginal improvement in the bit error rate. This
``error-floor" phenomenon is particularly worrisome for low BER
applications since this region is out of reach of pure software
simulations and there is a lack of appropriate analytic tools needed
to address, predict and improve the low BER performance of LDPC
codes. As a consequence, the deployment of LDPC codes in
applications requiring low BER guarantees has not quite met the
original promise of these powerful codes. Nonetheless, with their
unprecedented coding gains, LDPC codes remain strong contenders for
many applications, witnessed by their recent adoption into the
digital broadcasting DVB-S2 standard \cite{dvbstandard} for
satellite communication as well as 10Gb/s standard for Ethernet
\cite{802standard}. Chapter~\ref{iterativeBG} contains relevant
background on LDPC codes and surveys existing work related to the
``error floor" problem.

Motivated by earlier experimental observations that certain
structures intrinsic to the parity check matrix of a given code are
the dominant causes of the errors in the very low BER region
\cite{richardson},\cite{zhang06}, we introduce the notion of a
combinatorial structure in the graphical representation of an LDPC
code, which we call an absorbing set. Chapter~\ref{iterativeBG} also
contains a formal definition of absorbing and fully absorbing sets.
Absorbing sets (fully absorbing sets) are combinatorial objects that
exist in the Tanner graph associated with the parity check matrix of
a given code and have the property that bits in the absorbing set
(and bits outside the absorbing set) have strictly more satisfied
than unsatisfied checks. In particular, fully absorbing sets are
stable under the bit-flipping algorithm, which is the simplest form
of the message passing based decoding of LDPC codes. If there are
absorbing sets smaller that the minimum distance of the code, the
decoder is likely to converge to these objects. As a result, under
iterative decoding, the low BER performance will be dominated by the
number and the size of dominant absorbing sets, rather that the
number of minimum distance codewords and the minimum distance
itself, which is considered to be the performance metric under the
maximum likelihood decoding and the key property of a code.

To demonstrate the importance of studying absorbing sets, we then
provide a detailed analysis of the  minimal absorbing sets and
minimal fully absorbing sets of the high rate array-based LDPC codes
in Chapter~\ref{arrayabs}. Minimal (fully) absorbing sets are the
sets of the smallest size and  are shown experimentally to dominate
low BER performance for several LDPC codes. Array-based LDPC codes,
as an instance of regular LDPC codes, were chosen for this analysis
due to their high performance in the moderate BER regime, and the
structure of the parity check matrix that is amenable for many
high-throughput applications. In particular, we analytically
describe minimal absorbing sets and minimal fully absorbing sets for
 $\gamma=2,3,4$  where $\gamma$ refers to the column
weight of the array-based LDPC code. We also compute the number of
(fully) absorbing sets and show how it scales with the codeword
length. For $\gamma=2$, the smallest (fully) absorbing sets are
actually minimum distance codewords whereas for $\gamma =3,4$ and
large enough parity check matrix, the smallest (fully) absorbing
sets have the Hamming weight that is strictly smaller than the
minimum distance of the code. In the latter case, the minimal
absorbing sets, rather than the minimum distance codewords, dominate
the low BER performance. This claim is also supported by the
experiments carried out on an emulation platform that demonstrate
complete agreement with the theoretical prediction.

Lastly, Part ~\ref{last} summarizes main contributions and proposes
potential future extensions of the work presented here.
