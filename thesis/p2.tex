\chapter[P2: A Logical Beginning]{P2: A Logical Beginning}
\label{ch:p2}

Our journey begins with the Berkeley Declarative Networking Project, which
introduced our declarative language {\em \OVERLOG} and its runtime {\em P2}.
The project aim was to make it easy to implement and deploy overlay networks by
allowing specifications in a high-level declarative language to be directly
executed on nodes that span the Internet.  These overlay specifications,
expressed in \OVERLOG rules, contained {\em orders of magnitude} fewer lines of
code than corresponding overlay implementations written in an imperative
language (e.g., C/C++).  We implemented and deployed a Narada-style mesh
network~\cite{chu00case}, using only 12 rules, and the Chord structured
overlay~\cite{chord} in only 35 rules~\cite{p2:sosp}, versus thousands of lines
of code for the MIT Chord reference implementation.  Our experience with
overlay implementations has shown that relations, together with a recursive
query language, can fairly naturally represent the persistent routing state of
the overlays we considered~\cite{loo-sigmod06, p2:sosp}.

The \OVERLOG language is an extension of Datalog, which we introduce in
Section~\ref{ch:p2:sec:datalog}.  In Section~\ref{ch:p2:sec:overlog} we
describe the \OVERLOG language, which extends Datalog in three main ways: it
adds notation to specify the location of data, provides some SQL-style
extensions such as primary keys and aggregation, and defines a model for
processing and generating changes to tables.  Section~\ref{ch:p2:sec:p2}
reviews the P2 runtime responsible for compiling and executing \OVERLOG
programs on a set of distributed nodes.  The design of P2 was inspired by prior
work in both databases and networking.  It is based in large part upon a
side-by-side comparison between the PIER peer-to-peer query
engine~\cite{pier-cidr05} and the Click router~\cite{click-tocs}.  Like PIER,
P2 can manage structured data tuples flowing through a broad range of query
processing elements, which may accumulate significant state and perform
substantial asynchronous processing.  Like Click, P2 stresses high-performance
transfers of data units, as well as dataflow elements with both ``push'' and
``pull'' modalities.

This chapter is an overview of some of the many contributions made during the
course of the Declarative Networking project~\cite{boon-thesis}.  In
Section~\ref{ch:p2:sec:datalog}, we review the Datalog language, which is a
logic-based language that is suitable for database systems and it provides the
foundation for our \OVERLOG language.  Section~\ref{ch:p2:sec:overlog} provides
our first look at the \OVERLOG language, and describes the extensions that it
made to Datalog.  In Section~\ref{ch:p2:sec:p2}, we introduce the P2 runtime
system that we developed for executing \OVERLOG programs in a networked
environment.  We conclude in Section~\ref{ch:p2:sec:summary} with a summary of
the many contributions made during the Declarative Networking project before
presenting the final chapter of this project in Chapter~\ref{ch:evita}.

\section{Introduction to Datalog}
\label{ch:p2:sec:datalog}

We begin with a short review of Datalog, following the conventions in
Ramakrishnan and Ullman's survey~\ref{deductive-database}.  Datalog drew
inspiration from the Prolog language~\cite{prolog}, which was one of the first
logic programming languages.  Both Datalog and Prolog consists of a set of
declarative {\em rules} and an optional {\em query}.  A rule has the form $p :-
q_1, q_2, \ldots, q_n$, which consists of a disjunction of literals.
Informally, a Datalog rules reads ''{\bf if} $q_1$ and $q_2$ and $\ldots$ and
$q_n$ is true {\bf then} $p$ is true``.  Literals are either {\em predicates}
over {\em fields} (variables and constants), or function symbols applied to
fields.  The predicate appearing to the left of the \ol{:-} symbol is the
head predicate, and those to the right are body predicates or ``subgoals.'' 
Recursion is expressed by rules that refer to each other in a cyclic
fashion. That is, the head predicate also appears as a subgoal in the rule.

Datalog has two notions of predicates appearing in a program.  A predicate
whose relation is stored in the database is called an extensional database
(EDB) relation, while those that are defined by logical rules are called
intensional database (IDB) predicates.  In loose terms, the EDB represents the
input to the Datalog program, and the IDB is its output.  Datalog, unlike
Prolog, evaluates its rules in a bottom up fashion, starting with the facts the
EDB, and deriving new facts through rule deductions.  A key consequence of a
bottom-up evaluation strategy is that it can efficiently handle ``large data''
sets, whose size exceeds the capacity of a machine's main memory~\ref{ullman}.
Prolog on the other hand is evaluated tuple at a time, which precludes the
use of efficient relational operators (i.e., select, project, join) in
the evaluator.

\begin{figure*}
\ssp
\begin{boxedminipage}{\linewidth}
{\bf link}("localhost:10000", "localhost:10001"). \\
{\bf link}("localhost:10001", "localhost:10002"). \\
\\
R1 {\bf path}(X, Y,cons(X,Y), C) :- \\
\datalogspace {\bf link}(X, Y, C). \\
\\       
R2 {\bf path}(X,Y,cons(X,P2),C1+C2) :- \\
\datalogspace {\bf link}(X, Z, C1), {\bf path}(Z, Y, P2, C2), \\
\datalogspace $f\_contains(X,P2) == false$. \\

Query {\bf path}(``localhost:10000'', Y, P, C).
\end{boxedminipage}
\caption{\label{ch:p2:fig:datalogPath}Path program written in Datalog.}
\end{figure*}

Figure~\ref{ch:p2:fig:datalogPath} provides our first look at a program
expressed as a set of Datalog rules.  The program derives all reachable paths
from a set of known links.  The first two {\bf link} terms represent facts in
the underlining EDB.  Any predicates that appears in the head of a rule (e.g.,
{\bf path} in rules~\ol{R1} and \ol{R2}) are part of the IDB.  The first two
\ol{link} predicates in Figure~\ref{ch:p2:fig:datalogPath} represent facts that
are part of the EDB.  These \ol{link} tuples are used in rule~\ol{R1} to derive
an initial set of \ol{path} tuples.  The rule reads ``if there exists a
\ol{link} from $X$ to $Y$ at cost $C$, then there exists a \ol{path} from $X$
to $Y$ consisting of nodes $X, Y$ at cost $C$.'' Subsequent to that,
rule~\ol{R2} performs a transitive closure over the set of \ol{link} and
\ol{path} tuples.  Rule~\ol{R2} reads ``if there is a link from $X$ to $Z$ at
cost $C$, and there is a \ol{path} from $Z$ to $Y$ over $P2$ at cost $C2$, then
there is a path from $X$ to $Y$ through $X, P2$ at cost $C1+C3$.'' A query
predicate is also present at the bottom of Figure~\ref{ch:p2:fig:datalogPath}
that asks for all paths that start from ``localhost:10000.''

\subsection{Safety Constraint}

There are constraints that must be in place for a Datalog program to make
sense as operations on finite relations. A ``safe'' Datalog rule is one
that restricts the range of all variables appearing in the head by ensuring
that each such variable be associated with some predicate in the rule body.
For example, the following rule is not safe since it does not restrict the
$P$ variable in the \ol{path} head predicate.

\begin{minipage}{\linewidth}
\ssp
R3 {\bf path}(X,Y,P,C) :- \\
\datalogspace {\bf link}(X, Y, C). \\
\end{minipage}

Rule~\ol{R3} defines an infinite number of \ol{path} tuples since
we can substitute $P$ with any path. The ``safety'' restriction of 
Datalog, and its restriction to set semantics, is crucial to the
termination of a set of rules when applied to a finite EDB. The reader
can assume these restrictions throughout this thesis.

\subsection{Evaluation}

We now turn to the evaluation of a set of Datalog rules, which is performed in
a bottom-up fashion, starting with a set of baseline facts.  There are two
popular approaches to evaluating a set of Datalog rules.  The first approach is
called ``Naive Evaluation'', which is an iterative algorithm that repeatedly
applies all known facts to the program rules.  It starts with the facts
contained in the EDB and, by applying those facts to the rule bodies, derives
the initial set of IDB facts.  It repeats the process of deriving new IDB facts
by applying {\emph all} known facts (both EDB and IDB) to rule bodies.  The
evaluation terminates when it reaches a ``fixed point'', which occurs when no
new facts can be inferred.

\begin{figure*}
\ssp
\begin{boxedminipage}{\linewidth}
    \begin{algorithmic}[1]
      	\STATE Empty all IDB facts
	\STATE /* Base case: initialize IDB */
        \STATE Evaluate rules with subgoals that involve EDB predicates only
	\STATE Initialize $\delta$-IDB relations to be corresponding IDB relations
	\STATE /* Repeat while new tuples exist in any $\delta p$ relation in $\delta$-IDB */
	\WHILE{$\delta$-IDB != $\emptyset$}
        \FORALL{predicate $\delta p$ in $\delta$-IDB}
        	\FORALL{rules~$r$ that reference $p$ as the head predicate}
                	\STATE /* Compute new $\delta p$ as follows */
			\FORALL{subgoals $delta G_i \in \delta$-IDB from the body of $r$}
				\STATE Fix other $k$ subgoals $G_{k\ != i}$ to regular IDB relations
				\STATE Evaluate rule relative to the tuples in $delta G_i$ 
				\STATE Add all derivations to $\delta p$
			\ENDFOR
        	\ENDFOR
		\STATE Remove from $\delta p$, all facts already present in $p$ IDB relation
        \ENDFOR
	\ENDWHILE
    \end{algorithmic}
\end{boxedminipage}
\caption{\label{ch:p2:fig:seminaive}Seminaive Evaluation over a set of Datalog rules.}
\end{figure*}

The second approach, which is also optimal, adds a condition to the iteration
loop of the ``Naive Evaluation'' algorithm.  ``Seminaive Evaluation'' is based
on the principle that if a fact is derived during round~$i$ then it must have
been inferred from a rule in which one or more subgoals were instantiated with
facts that were inferred in round~$i-1$.  Figure~\ref{ch:p2:fig:seminaive}
describes the algorithm that performs a seminaive evaluation over a set of
Datalog rules.  In the first step, it evaluates the rules against the EDB facts
to derive an initial set of IDB facts, which is subsequently used to represent
the $\delta$-IDB.  We then enter an iteration loop, which repeatedly derives
new facts in the IDB relations mentioned in $\delta$-IDB.  We start by
considering a single $\delta p$ predicate in $\delta$-IDB that contains the
facts derived in the previous iteration, and therefore contained within the IDB
relation $p$.  To derive new facts for $\delta p$, we consider those rules~$r$
that refer to predicate $p$ in the rule head.  These rules are evaluated using
only subgoals mentioned in $\delta$-IDB.  Following the evaluation of each such
rule, we remove all tuples from $\delta p$ that existed in the previous
iteration.  The algorithm terminates when no new derivations were made to any
IDB relations.


\subsection{Fixed Point Semantics}

Datalog is monotonic since it does not contain the notion of deletions.  The
evaluation of a program proceeds as a series of deductions to the IDB.  Since
Datalog is ``set oriented'', only new deductions will be added to the IDB, and
therefore it is guaranteed to terminate when no new deductions can be made.  A
Datalog program is said to be at a ``fixed point'' when no further deductions
can be made relative to the EDB and IDB relations.  In the absence of negated
subgoals, the derivations made during the evaluation represent a {\emph unique
minimal model}.  They are unique in the sense that we will always derive the
same IDB tuples given the same EDB tuples.  The result represents a minimal
model since we cannot add or take away any fact and still have a model
consistent with the database.

\subsection{Negation and Stratification}

The last subject we will cover in this section is handling negated
subgoals in a Datalog rule. There is a large body of work on this subject
that we will not address here, since it is not necessary in our context.
We will discuss the notion of stratified negation, which ensures that
a set of Datalog rules with negated subgoals will reach a minimal fixed point
evaluation. But first we review the issues surrounding negations in rule
bodies.

\begin{figure*}
\centering
\ssp
\begin{boxedminipage}{\linewidth}
R4 {\bf path}(X,Y,P,C) :- \\
\datalogspace {\bf link}(X, Y, C), \\
\datalogspace not {\bf detour}(X, Y). \\

R5 {\bf detour}(X, Y) :- \\
\datalogspace {\bf link}(X, Y, C), \\
\datalogspace \ldots 
\end{boxedminipage}
\caption{\label{ch:p2:fig:negation}Negated Datalog rule.}
\end{figure*}

Consider rule~\ol{R4} in Figure~\ref{ch:p2:fig:negation}, which formulates a
\ol{path} from a \ol{link} if $X$ and $Y$ does not represent a detour.
Unfortunately, the complement of the \ol{detour} relation is not a well-defined
term since we do not know its domain of possible values.  Moreover, we cannot
specify its relation prior to seminaive evaluation, since rule~\ol{R5}
references the \ol{detour} predicate in the head.  

If we were to evaluate the rules in Figure~\ref{ch:p2:fig:negation}
then we could end up with \ol{path} tuples that represent
detours. To see this, lets assume that we start by evaluating rule~\ol{R4}
with the \ol{link} relation. The plan for the negated \ol{detour} is models
an anti-join operation, where tuples from \ol{link} pass if they do not exist
in the \ol{detour} relation. Since we have not evaluated rule~\ol{R5},
all tuples in \ol{link} will produce a deduction to the \ol{path} relation
in rule~\ol{R4}. Subsequently evaluating rule~\ol{R5} would derive our
\ol{detour} tuples, but it would be too late in the sense that we have already
made incorrect deductions via rule~\ol{R4}.

The reader may realize that by evaluating rule~\ol{R5} first, we will get the
correct deductions in rule~\ol{R4}.  This is indeed correct, and this ordering
of predicates deductions forms the basic idea behind stratified Datalog.
Before we get to that definition, lets first understand how the dependencies of
a Datalog program is represented graphically.

\begin{figure*} 
\ssp
\begin{center}
\includegraphics[scale=1]{figures/dependency-graph}
\caption{\label{ch:p2:fig:dependency}Dependency graph for predicates 
appearing in Figure~\ref{ch:p2:fig:negation}.}
\end{center} 
\end{figure*}

Figure~\ref{ch:p2:fig:dependency} contains the dependency graph for the predicates
appearing in the rules of Figure~\ref{ch:p2:fig:negation}. Constructing this graph
is a straightforward application of the following two rules.
\begin{enumerate}
  \ssp
  \item Add $p \rightarrow q$ dependency if there is a rule with head predicate $p$ and subgoal $q$.
  \item Add $p \rightarrow q$ dependency labeled $\neg$ if there is a rule with head predicate $p$ and negated subgoal $q$.
\end{enumerate}
From Figure~\ref{ch:p2:fig:negation}, rule~\ol{R4} forms the $path \rightarrow
link$ and $path \rightarrow detour$ dependencies, while rule~\ol{R5} forms the
$detour {\neg \atop \rightarrow} link$ negated dependency. 

We now formally define the stratum of an IDB predicate to be the largest number
of $\neg$ on a path from that predicate, in the dependency graph. The dependency
graph in Figure~\ref{ch:p2:fig:dependency} places predicates \ol{detour} and \ol{link}
in the lowest stratum~$0$, while the \ol{path} predicate is in stratum~$1$. If all
IDB predicates have a finite stratum, then the Datalog program is stratified. If any
IDB predicate has an infinite stratum, then the program is unstratified. An IDB
predicate is assigned an infinite stratum if it is on a path that contains a cycle 
(due to recursion), and there exists a negated edge along that path. 

We evaluate a stratified Datalog program using the seminaive algorithm of
Figure~\ref{ch:p2:fig:seminaive} but with a slight twist -- we order the
$\delta$-IDB predicates by their assigned stratum.  If the program is
stratified then any negated subgoal (e.g., \ol{detour}) has already had its
relation fully evaluated.  The result of this evaluation is a {\emph stratified
model}. We revisit the notion of stratified Datalog throughout this thesis. It turns
out that the P2 system does not supported stratified Datalog, which significantly
complicated made many of our \OVERLOG programs mentioned in Chapters~\ref{ch:evita}, 
\ref{ch:magic} and~\ref{ch:opt}.


\section{\OVERLOG: The P2 Query Language}
\label{ch:p2:sec:overlog}

\OVERLOG is based on the traditional recursive query language, Datalog.  As in
Datalog, an \OVERLOG~{\em program} consists of a set of deduction {\em rules}
that define the set of tuples that can be derived from a base set of tuples
called {\em facts}.  Each rule has a {\em body} on the right of the \texttt{:-}
divider, and a {\em head} on the left; the head represents tuples that can be
derived from the body.  The body is a comma-separated list of {\em terms}; a
term is either a {\em predicate} (i.e., a relation), a {\em condition} (i.e., a
relational selection) or an {\em assignment}~\footnote{\OVERLOG's assignments
are strictly syntactic replacements of variables with expressions; they are
akin to ``\#define'' macros in C++.}.  An example \OVERLOG program is shown in
Figure~\ref{ch:p2:fig:overlogSP}.  \OVERLOG introduces some notable extensions
to Datalog, which we review before describing the P2 runtime.

\begin{figure*}
\ssp
\begin{boxedminipage}{\linewidth}
{\bf materialize}(link,infinity,infinity,keys(1,2)). \\
{\bf materialize}(path,1,infinity,keys(1,2,3)).  \\
{\bf materialize}(shortestPath,1,infinity,keys(1,2,3)). \\
\\
{\bf link}("localhost:10000", "localhost:10001"). \\
{\bf link}("localhost:10001", "localhost:10002"). \\
\\
R1{\bf path}(@X, Y, P, C) :- \\
\datalogspace {\bf link}(@X, Y, C), P := f\_cons(X, Y). \\
\\       
R2 {\bf path}(@X,Y,P,C) :- \\
\datalogspace {\bf link}(@X, Z, C1), {\bf path}(@Z, Y, P2, C2), \\
\datalogspace $f\_contains(X,P2) == false$, \\
\datalogspace P := f\_cons(X,P2), C := C1 + C2. \\ 
\\      
R4 {\bf minCostPath}(@X, Y, a\_min$<$C$>$) :-  \\
\datalogspace {\bf path}(@X, Y, P, C). \\
\\
R5 {\bf shortestPath}(@X, Y, P, C) :- \\
\datalogspace {\bf minCostPath}(@X, Y, C), {\bf path}(@X, Y, P, C).\\
\end{boxedminipage}
\caption{\label{ch:p2:fig:overlogSP}Shortest path program in \OVERLOG. \ol{a\_}
prefixes introduce aggregate functions and \ol{f\_} prefixes introduce
built-in functions.}
\end{figure*}

\subsection{Horizontal partitioning}

\OVERLOG's basic data model consists of relational tables that are partitioned
across the nodes in a P2 network.  Each relation in an \OVERLOG rule must have
one attribute that is preceded by an ``@'' sign.  This attribute is called the
{\em location specifier} of the relation, and must contain values in the
network's underlying address space (e.g., IP addresses for Internet settings,
802.13.4 addresses for sensor networks, hash-identifiers for code written atop
distributed hash tables, etc.).  Location specifiers define the horizontal
partitioning of the relation: each tuple is stored at the address found in its
location specifier attribute.  At a given node, we call a tuple a {\em local
tuple} if its location specifier is equal to the local address.  Network
communication is implicit in \OVERLOG: tuples must be stored at the address in
their location specifier, and hence the runtime engine has to send some of its
derived tuples across the network to achieve this physical constraint.  Loo, et
al.  provide syntactic tests to ensure that a set of rules can be maintained
partitioned in a manner consistent with its location specifiers and network
topology~\cite{loo-sigmod06}.


\subsection{Soft State and Events}

Associated with each \OVERLOG table is a ``soft-state'' lifetime that
determines how long (in seconds) a tuple in that table remains stored before it
is automatically deleted.  Lifetimes can vary from zero to infinity.
Zero-lifetime tables are referred to as {\em event} tables, and their tuples
are called \emph{events}; all other tables are referred to as {\em
materialized} tables.  \OVERLOG contains a \ol{materialize} declaration that
specifies the lifetime of a materialized table.  At any instant in time, at any
given node in the network, the contents of the local \OVERLOG ``database'' are
considered to be: (a) the local tuples in materialized tables whose lifetime
has not run out, (b) at most one local event fact across {\em all} event
tables, and (c) any derived local tuples that can be deduced from (a) and (b)
via the program rules.  Note that while (b) specifies that only one event fact
is considered to be live at a time per node, (c) could include {\em derived}
local events, which are considered to be live simultaneously with the event
fact.  This three-part definition defines the semantics for a single P2 node at
a snapshot in time.  P2 has no defined semantics across time and space (in the
network); we describe the relevant operational semantics of the prototype in
Section~\ref{sec:eventloop}.
     
\subsection{Deletions and Updates}

\OVERLOG, like SQL, supports declarative expressions that identify tuples to be
deleted in a deferred manner after query execution completes.  To this end, any
\OVERLOG rule in a program can be prefaced by the keyword \ol{delete}.  The
program is run to fixpoint, after which the tuples derived in {\tt delete}
rules -- as well as other tuples derivable from those -- are removed from
materialized tables before another fixpoint is executed.  It is also possible
in \OVERLOG to specify updates, but the syntax for doing so is different.
\OVERLOG's {\tt materialize} statement supports the specification of a primary
key for each relation.  Any derived tuple that matches an existing tuple on the
primary key is intended to {\em replace} that existing tuple, but the
replacement is separated into an insertion and a deletion: the deduction of the
new fact to be inserted is visible within the current fixpoint, whereas the
deletion of the original fact is deferred until after the fixpoint is computed.

\subsection{A Canonical Example}
\label{ch:p2:sec:declnet}

To illustrate the specifics of \OVERLOG, we review the shortest paths example
in Figure~\ref{ch:p2:fig:overlogSP}, which is similar to that
of~\cite{loo-sigmod06}, but with fully-realized \OVERLOG syntax that runs in
P2.  The three \ol{materialize} statements specify that \ol{link}, \ol{path}
and \ol{bestpath} are all tables with infinite lifetime and infinite storage
space\footnote{The third argument of P2's table definition optionally specifies
a constraint on the number of tuples guaranteed to be allowed in the relation.
The P2 runtime replaces tuples in ``full'' tables as needed during execution;
replaced tuples are handled in the same way as tuples displaced due to
primary-key overwrite.}.  For each table, the positions of the primary key
attributes are noted as well.  Rule \ol{r1} can be read as saying ``if there is
a link tuple of the form \ol{(X,Y,C)} stored at node \ol{X}, then one can
derive the existence of a path tuple \ol{(X,Y,P,C)} at node \ol{X}, where
\ol{P} is the output of the function \ol{f\_cons(X,Y)} -- the concatenation of
\ol{X} and \ol{Y}.'' Note that rule \ol{r1} has the same location specifiers
throughout, and involves no communication.  This is not true of the recursive
rule \ol{r2}, which connects any \ol{link} tuple at a node \ol{X} with any path
tuple at a neighboring node \ol{Z}, the output of which is to be stored back at
\ol{X}.  As described in the earlier work on
P2~\cite{loo-sigcomm05,loo-sigmod06} such rules can be easily rewritten so that
the body predicates all have the same location specifier; the only
communication then is shipping the results of the deduction to the head
relation's location specifier.

\section{The P2 Runtime Engine}
\label{ch:p2:sec:p2}

The P2 runtime is a dataflow engine that was based on ideas from relational
databases and network routers; its scheduling and data hand-off closely
resemble the Click extensible router~\cite{click-tocs}.  Like Click, the P2
runtime supports dataflow {\em elements} (or ``operators'') of two sorts:
pull-based elements akin to database iterators~\cite{graefe-survey}, and
push-based elements as well.  As in Click, whenever a pull-based element and a
push-based element need to be connected, an explicit ``glue'' element (either a
pull-to-push driver, or a queue element) serves to bridge the two.  More
details of this dataflow coordination are presented in the original P2
paper~\cite{p2:sosp}.

\subsection{Dataflow Elements} 

The set of elements provided in P2 includes a suite of operators familiar from
relational query engines: selection, projection, and in-memory indexes.  P2
supports joins of two relations in a manner similar to the symmetric hash join:
it takes an arriving tuple from one relation, inserts it into an in-memory
table for that relation, and probes for matches in an access method over the
other relation (either an index or a scan).  The work described in
Chapter~\ref{ch:evita} extended this suite to include sorting and merge-joins,
which allowed us to explore some traditional query optimization opportunities
and trade-offs (Section~\ref{ch:evita:sec:systemr}).

P2 currently has no support for persistent storage, beyond the ability to read
input streams from comma-separated-value files.  Its tables are stored in
memory-based balanced trees that are instantiated at program startup;
additional such trees are constructed by the planner as secondary indexes to
support query predicates.

P2 also provides a number of elements used for networking, which handle issues
like packet fragmentation and assembly, congestion control, multiplexing and
demultiplexing, and so on; these are composable in ways that are of interest to
network protocol designers~\cite{condie-hotnets05}.  The basic pattern that the
reader should assume is that each P2 node has a single IP port for
communication, and the dataflow graph is ``wrapped'' in elements that handle
network ingress with translation of packets into tuples, and network egress
with translation of tuples into packets.

\subsection{The P2 Event Loop}
\label{sec:eventloop}

The control flow in the P2 runtime is driven by a fairly traditional event loop
that responds to any network or timer event by invoking an appropriate dataflow
segment to handle the event.

The basic control loop in P2 works as follows:
\begin{CompactEnumerate}
    \item An event is taken from the system input queue, corresponding to a single newly-arrived tuple, which is either an {\em insert} tuple (i.e., the result of a normal deduction) or a {\em delete} tuple (the result of a \ol{delete} rule or a primary-key update).  We will refer to this tuple as the {\em current tuple}.
    \item The value of the system clock is noted in a variable we will call the {\em current time}.  This is the time that will be used to determine the liveness of soft-state tuples.  
    %(Note that any event tuples that arrived previously will no longer be live in any event table, which guarantees the single-event semantics described above.)
    \item The current tuple is, logically, appended to its table.
    \item If the current tuple is an insert tuple, the dataflow corresponding to the \OVERLOG program is initiated and run to a local fixpoint following traditional Datalog semantics, with the following exception: during processing, any non-local derived tuples are buffered in a {\em send queue}, as are any derived tuples to be deleted.
    \item If, instead, the current tuple is a delete tuple, the dataflow
    is run to a local fixpoint, but newly-derived local tuples
    (including the current tuple) are copied to a {\em delete queue},
    and newly-derived non-local tuples are marked as delete tuples
    before being placed in the send queue so as to cascade the deletions
    to remote nodes' databases.
    \item All tuples in the delete queue are deleted from their associated tables, and the delete queue is emptied.
    \item The send queue is flushed across the network, with any local updates inserted into the local input queue.
\end{CompactEnumerate}

Unlike Datalog, \OVERLOG must run in the continuous processing context of
networking, over streams of tuples representing system events.  This inherently
requires more than the single computation of a fixpoint as described in the
Datalog literature.  P2 has modified its handling of this issue since the
initial paper~\cite{p2:sosp}.  P2 nests a fairly traditional declarative
Datalog fixpoint execution within an operationally defined local event loop at
each node.  An input queue is kept at each P2 node, to hold tuples that
correspond to network messages and clock interrupts.  Each tuple in the queue
is tagged with the name of a relation in the schema of the Datalog database.
The loop begins by noting the local wall-clock time, and deleting from all
tables any tuples whose soft-state lifetime has expired; this includes event
tuples from the previous iteration of the loop.  At that point, a tuple is
dequeued from the input queue and inserted into its associated table.  At that
point, the \OVERLOG program is run to fixpoint atomically, nearly as if it were
a traditional single Datalog program.  One exception to traditional Datalog is
the handling of derived tuples with remote location specifiers; these are
placed directly into network queues for subsequent processing.  Another
exception involves rules that have {\em actions} in the head -- these actions
can be table insertion or deletion; derived tuples in such rules are also
enqueued for subsequent processing.  When fixpoint is reached, the queued
network messages are sent to their destinations, and the table actions are
carried out on the database.  This completes one iteration of the event loop.

From this perspective, the P2 runtime looks quite a bit like an
Event-Condition-Action system with dataflow underneath: events are provided by
the clock and network, conditions are checked via the dataflow engine for
matches, which are then converted into actions: network messages to be sent, or
table updates to be performed.

\section{Summary}

While ostensibly a network protocol engine, architecturally P2 resembles a
fairly traditional shared-nothing parallel query processor, targeted at both
stored state and data streams.  The P2 runtime at each node consists of a
compiler---which parses programs, optimizes them, and physically plans them---a
dataflow executor, and access methods.  Each P2 node runs the same query
engine, and, by default, participates equally in every ``query.'' In parallel
programming terms, P2 encourages a Single-Program-Multiple-Data (SPMD) style
for parallel tasks, but also supports more loosely-coupled (MPMD) styles for
cooperative distributed tasks, e.g.  for communications among clients and
servers.


% \jmh{This is probably too long.  Also, we need to purge text that was recycled from SOSP.  }
% The design of P2 was inspired by prior work in both databases and
% networking. It is based in large part upon a
% side-by-side comparison between the PIER peer-to-peer query
% engine~\cite{pier-cidr05} and the Click router~\cite{click-tocs}. Like
% PIER, P2 can manage structureddata tuples flowing through a broad
% range of query processing elements, which may accumulate significant
% state and perform substantial asynchronous processing.  Like Click, P2
% stresses high-performance transfers of data units, as well as dataflow
% elements with both ``push'' and ``pull'' modalities. 
% 
% At a coarse grain, P2 in its current state consists of (1) an \OVERLOG
% parser, (2) an Planner that translates \OVERLOG to a runtime dataflow
% plan, and (3) a runtime plan executor.  The
% life of a query is simple: the query is parsed into an internal
% representation, the planner constructs a corresponding dataflow graph
% of elements, and the graph is executed by the runtime until it is
% canceled.  We proceed to overview the components bottom-up; more
% details are given in the P2 SOSP paper~\cite{p2:sosp}.
% 
% Processing in P2 is handled with a dataflow model inspired by Click
% and PIER.  As in Click, nodes in a P2 dataflow
% graph can be chosen from a set of C++ objects called
% \textit{elements}.  In database systems these are often called
% \textit{operators}, since they derive from logical operators in the
% relational algebra.  Elements have some number of input and output
% \emph{ports}.  An arc in the dataflow graph is represented by a
% binding between an output port on one element and an input port on
% another.  Tuples arrive at the element on input ports, and elements
% emit tuples from their output ports. Handoff of a tuple between two elements takes one
% of two forms, \emph{push} or \emph{pull}, determined when the elements
% are configured into a dataflow graph.   
% 
% P2 provides a number of built in dataflow elements that allow it to
% implement networking and query processing logic.  This includes
% elements for the streaming relational query operators found in most
% database systems, e.g., selection, projection, join, and aggregation.
% It also includes networking elements responsible for socket handling,
% packet scheduling, congestion control, reliable transmission, data
% serialization, and dispatch.  P2 has elements to store incoming tuples in tables, 
% iteratively emit tuples in a table matching a filter expression, and {\em listener}
% elements that are notified whenever a tuple is added or deleted from a
% table. Finally, like Click, P2 includes a collection of general-purpose
% ``glue'' elements, such as a queue, a multiplexer, a round-robin
% scheduler, etc.
% 
% Storage in P2 is currently via a main-memory relational Table
% implementation, named using unique IDs that can be shared between
% different queries and/or dataflow elements.  In-memory indices
% (implemented using standard balanced binary trees) can be attached to
% attributes of tables to enable quick equality lookups.  The current
% in-memory implementation serves the system requirements for implementing
% network overlays and streaming query applications, all of which tend
% to expire tuples from memory rather than accumulating them
% indefinitely.  P2's event-driven, run-to-completion model obviates the
% need for locking or transaction support, and relatively simple indices
% suffice to meet performance requirements.  We plan additional
% table implementations that use stable storage for persistent data
% storage; that engineering task is relatively straightforward, but not
% within the scope of this paper.
% 
