\chapter[Dissertation Overview]{Dissertation Overview}
\label{ch:overview}

There has been renewed interest in recent years in applying declarative
languages to a variety of applications outside the traditional boundaries of
data management.  Examples include work on compilers~\cite{lam05context},
computer games~\cite{white-sigmod07}, security protocols~\cite{li-padl03}, and
modular robotics~\cite{ashley-iros07}.  Our work in this area began with the
{\em Declarative Networking} project, as instantiated in the {\em P2} system
for Internet overlays~\cite{p2:sosp, loo-sigmod06}.  This thesis represents the
final chapter of that project and introduces the {\em Berkeley Orders Of
Magnitude} (BOOM) project, which moves up from the networking layer, and into
the data center system software layer.  A number of complex issues arise at
this layer, such as resource scheduling, the enforcement of distributed
invariants (e.g., safety and liveness), consistency, availability, and
fault-tolerance.  In this thesis, we focus on one issue -- resource scheduling,
and how it can be expressed compactly via a high-level declarative query
language.  We also touch on an initial investigation of fault-tolerance in the
context of another high-level dataflow language -- MapReduce, and like a
declarative language, it too abstracts away low level mechanistic details
behind a programming model.  

Our goal here is to explore systems programming in a high level declarative
language.  This effort is rooted in the {\em Declarative Networking}
project~\cite{boon-thesis}, which ignited the research direction of using a
declarative language to develop distributed software, specifically network
layer protocols, for the next generation of computing architectures.  In
Chapter~\ref{ch:p2}, we review this influential work on constructing networking
protocols in a declarative language because it sets the stage for much of the
work described in this dissertation.  Specifically, the declarative language
\OVERLOG (used throughout my work here) was developed during this era.
\OVERLOG is a Datalog-like language with distributed extensions that will be
fully described in Chapter~\ref{ch:p2}.  

The Declarative Networking project also developed a runtime for the \OVERLOG
language called P2, which automatically compiled \OVERLOG programs into a
dataflow-oriented runtime system.  Although P2 itself was used for only part of
this dissertation~\footnote{Chapter~\ref{ch:evita} marks the final chapter in
the P2 Declarative Networking project.}, we retain the use of the \OVERLOG
language in many subsequent projects, two of which are described in
Chapter~\ref{ch:boom} and Chapter~\ref{ch:hop}.

The contributions presented by this dissertation begin in
Chapter~\ref{ch:evita}, where we describe our first declarative system
component -- Evita Raced, which is the P2 declarative meta-compiler.  Evita
Raced itself runs as a regular query within the P2 query processing engine, and
it executes the task of query compilation for the P2 system.  An Evita Raced
compilation task takes the form of a query written in the same declarative
language (\OVERLOG) used to submit user queries, such as various networking
protocols from Loo, et al.~\cite{loo-sigmod06, p2:sosp}.  

The P2 compiler was engineered to compile query code into a relational format,
thereby providing compilation tasks (written in \OVERLOG) access to the logical
query plan, and allowing the ability to query and update the logical query plan
during compilation.  We show that many traditional database optimizations, like
the System R optimization algorithm and the magic-sets rewrite, can be easily
expressed as \OVERLOG queries.  Expressing the optimization algorithms as
\OVERLOG queries led to a more concise representation of the algorithm and a
dramatic reduction in the overall coding effort.
 
Following Evita Raced, we turned our attention to another system that has
gained popularity in the early $21^{st}$ century -- Hadoop~\cite{hadoop}.
Hadoop is a rather large piece of open source software that implements the
MapReduce programming model~\cite{mapreduce-osdi}.  We tackle this elephant
system in two main efforts.  We first look at the scheduling component that
is housed within the centralized coordinator (master) of Hadoop.  It is written
in a (relatively) low-level language called Java~\cite{java}.  Java is a
popular imperative language used today in many cloud
applications~\cite{pig-sigmod, hive-vldb, zookeeper, hbase, cassandra}.
It was built in $1991$, however, for interactive television~\cite{java-history},
and became the language of the web development shortly thereafter.  Today it is
used to write large distributed applications that provide the world with its
daily fix of data.  Unfortunately, these systems take time to get right, and once
written, are incredibly difficult to extend with new features.  We believe a
declarative approach is in order.

The remaining chapters of this dissertation focus on the contributions made
during the early stages of the {\em Berkeley Orders Of Magnitude} (\BOOM)
project, which evaluates the use of a declarative language in building
distributed system software for today's data center architecture (clusters of
commodity machines).  As we have already alluded, building and debugging
distributed software remains extremely difficult in today's language of choice
-- Java.  We conjecture that by adopting a \emph{data-centric} approach to
system design and by employing \emph{declarative} programming languages, a
broad range of distributed software functionality can be recast naturally in a
data-parallel programming model.  Our hope is that this model can significantly
raise the level of abstraction for programmers, improving code simplicity,
speed of development, ease of software evolution, and program correctness.

To evaluate this conjecture, we used the \OVERLOG language to implement an
API-compatible version of the Hadoop MapReduce with competitive performance,
and then extended it by adding advanced fault-tolerance and scale features that
are typical for cloud computing environments.  In Chapter~\ref{ch:hadoop}, we
provide some background material on MapReduce, which has emerged as a popular
programming model that allows imperative languages (i.e., C++, Java) to harness
the power of large clusters of computers.  In Chapter~\ref{ch:boom}, we
describe our rewrite of the Hadoop MapReduce scheduling engine in a declarative
language and show that equivalent performance, fault-tolerance, and scalability
properties can be achieved in orders-of-magnitude less code.  In
Chapter~\ref{ch:hop}, we look at moving beyond a batch-oriented execution model
in MapReduce to a more online execution model by pipelining data between system
operators.  This extension brings with it a number of scheduling challenges,
which we solve using our declarative scheduling framework.  Finally, we
conclude in Chapter~\ref{ch:conclusion} with a discussion of future directions.





