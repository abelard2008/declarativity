\chapter[Dissertation Overview]{Dissertation Overview}
\label{ch:overview}

There has been renewed interest in recent years in applying declarative
languages to a variety of applications outside the traditional boundaries of
data management.  Examples include work on compilers~\cite{lam05context},
computer games~\cite{white-sigmod07}, security protocols~\cite{li-padl03}, and
modular robotics~\cite{ashley-iros07}.  Our work in this area began with the
{\em Declarative Networking} project, as instantiated in the {\em P2} system
for Internet overlays~\cite{p2:sosp, loo-sigmod06}.  This thesis represents the
final chapter of that project and introduces a new beginning, where we express
layers of the distributed system stack in a declarative language.  A number
of complex issues arise at this layer, such as resource scheduling, the
enforcement of distributed invariants (e.g., safety and liveness), consistency,
availability, and fault-tolerance.  In this thesis, we focus on one issue --
resource scheduling, and how it can be expressed compactly via a high-level
declarative query language.  We also touch on an initial investigation of
fault-tolerance in the context of another high-level dataflow language, which
is a new ``language'' designed for the {\emph cloud}~\cite{abovetheclouds}.

Our goal here is to explore systems programming in a high level declarative
language.  This effort is rooted in the {\em Declarative Networking}
project~\cite{boon-thesis}, which ignited the research direction of using a
declarative language to develop distributed software, specifically network
layer protocols, for the next generation of computing architectures.  In
Chapter~\ref{ch:p2}, we review this influential work because it sets the stage
for this dissertation.  Specifically, the declarative language \OVERLOG (used
throughout this dissertation) was developed during this era.  

The Declarative Networking project also developed a runtime for the \OVERLOG
language called P2, which automatically compiled \OVERLOG programs into a
dataflow-oriented runtime system.  Although P2 itself was used for only part of
this dissertation~\footnote{Chapter~\ref{ch:evita} marks the final chapter in
the P2 Declarative Networking project.}, we retain its many lessons and the use
of the \OVERLOG language in the work presented in Chapter~\ref{ch:boom} and
Chapter~\ref{ch:hop}.

The primary contributions presented in this dissertation begin with
Chapter~\ref{ch:evita}, where we describe our first declarative system
component -- Evita Raced, which is the new P2 declarative meta-compiler.  Evita
Raced runs as a regular query within the P2 processing engine, and it executes
the task of query compilation for the P2 system.  Evita Raced formulates the
compiler/optimizer task as a query written in the same declarative language
(\OVERLOG) used by ``client'' queries, such as various networking protocols
from Loo, et al.~\cite{loo-sigmod06, p2:sosp}.

The P2 compiler was first engineered to compile query code into a relational
format, thereby providing compilation tasks (written in \OVERLOG) access to the
logical query plan, and allowing the ability to query and update that logical
query plan.  We show that many traditional database optimizations, like the
System R optimization algorithm and the magic-sets rewrite, can be easily
expressed as \OVERLOG queries.  Expressing the optimization algorithms as
\OVERLOG queries resulted in a more concise representation of the algorithm as
code and a dramatic reduction in the overall development effort.
 
Following Evita Raced work, we turn our attention to another system that has
gained in popularity recently~\footnote{Around $2010$, the year this thesis was
written.} -- Apache Hadoop~\cite{hadoop}.  Hadoop is an open source software
project that implements the MapReduce programming model~\cite{mapreduce-osdi}.
In our work, we investigate the Hadoop scheduler component, which is housed
within the centralized coordinator (master) of the MapReduce engine.  It is
entirely written in the (relatively) low-level Java language~\cite{java}.  Java
was developed in $1991$, and quickly became the language for the web
development~\cite{java-history}.  Today, Java is used to develop many popular
{\emph cloud} applications~\cite{pig-sigmod, hive-vldb, zookeeper, hbase,
cassandra}.  These {\emph cloud computing} applications are being used by large
Internet companies to mine information from massive amount of data, using the
computational power and storage of a data center (the cloud), at an unforeseen
scale.  The information compiled by these cloud applications translate to
bottom line returns.  Unfortunately, they take time to get right, and once
written, are incredibly difficult to extend with new features.  We believe a
declarative approach is in order.

The remaining chapters of this dissertation focus on the contributions made
during the early stages of the {\em Berkeley Orders Of Magnitude} (\BOOM)
project, which evaluates the use of a declarative language in building
distributed system software for today's data center architecture (clusters of
commodity machines).  As we have already alluded, building and debugging
distributed software remains extremely difficult in today's language of choice
-- Java.  We conjecture that by adopting a \emph{data-centric} approach to
system design and by employing \emph{declarative} programming languages, a
broad range of distributed software functionality can be recast naturally in a
data-parallel programming model.  Our hope is that this model can significantly
raise the level of abstraction for programmers, improving code simplicity,
speed of development, ease of software evolution, and program correctness.

To evaluate this conjecture, we used the \OVERLOG language to implement an
API-compatible version of the Hadoop MapReduce scheduler.  Not only did we
achieve this goal using {\emph orders of magnitude} few lines of code, our
implementation exhibits competitive performance, and extends Hadoop with
advanced fault-tolerance and scaling features that are typical for cloud
computing environments.  In Chapter~\ref{ch:hadoop}, we provide some background
material on MapReduce, which has emerged as a popular programming model for
writing data mining tasks in the cloud.  In Chapter~\ref{ch:boom}, we describe
our rewrite of the Hadoop MapReduce scheduling engine in a declarative language
and show that equivalent performance, fault-tolerance, and scalability
properties can be achieved in orders-of-magnitude less code.  In
Chapter~\ref{ch:hop}, we look at moving beyond a batch-oriented execution model
in MapReduce to a more online execution model by pipelining data between system
operators.  This extension brings with it a number of scheduling challenges,
which we solve using our declarative scheduling framework.  Finally, we
conclude in Chapter~\ref{ch:conclusion} with a discussion of future directions.





