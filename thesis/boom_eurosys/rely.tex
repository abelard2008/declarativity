\section{The Availability Rev}
\label{sec:rely}
% \paa{a basic substrate for a HA distributed system is a total ordering over events; cf Lamport's state machine
% approach.  this amounts to a distributed log.  Paxos is commonly used to achieve this property.
% the harvard guys showed that a logic language Paxos was possible, but did not produce a useable implementation 
% (single-decree basicPaxos, races, etc).  we needed to make it "live" and encountered many of the issues
% mentioned by the chubby authors, which are NOT treated (other than being mentioned) in the lamport literature.
% discuss the "glue" needed to Paxos-ize bfs.  putting state changes into the Paxos log is the easy part: stringify.
% trickier is crafting the rules that are contingent on reading an entry from the log; ie, user sends a "create file"
% request, we pass it through Paxos.  deltas on the "passed decrees" log trigger rules that reflect the new file in
% the current metadata, and send the response to the client.
% discussion of how reliability is tested..
% }


% In a single-node system, reliability is directly tied to recoverability, and is typically guaranteed by a 
% combination of redundant storage (i.e. RAID) and logging (i.e. WAL logs in databases and journals in file 
% systems).  

% \jmh{I have added para-by-para annotations here to see if we like the flow, and help us think about condensing/refocusing.}
% 
% \jmh{The shtick: Base implementation done, it was time for a feat of
% distributed derring-do.}
Having achieved a fairly faithful implementation of HDFS, we were ready to
explore whether data-centric programming would make it easy to add complex
distributed functionality to an existing system.  We chose what we considered a
challenging goal: retrofitting \BOOM-FS with high availability failover via
``hot standby'' {\NN}s.  A proposal for warm standby was posted to the Hadoop
issue tracker in October of 2008 (\cite{jira} issue HADOOP-4539). We felt that a
hot standby scheme would be more useful, and would more aggressively test our
hypothesis that significant distributed system infrastructure could be
implemented cleanly in a data-centric manner.

%\jmh{Background -- real heroes do it via Paxos.}
\subsection{Paxos Implementation}
Implementing hot standby replication is tricky, since replica state must remain
consistent in the face of node failures and lost messages. One solution is to
use a globally-consistent distributed log, which guarantees a total ordering
over events affecting replicated state. Lamport's Paxos algorithm is the
canonical mechanism for this feature~\cite{part-time}.

%\jmh{What were we thinking?  This para may be unnecessarily long.}
% We had two reasons to believe that we could cleanly retrofit a hot
% standby solution into \BOOM-FS.  First, data-centric programming had
% already forced us to encode the relevant \NN state into a small number
% of relational tables, so we knew what data we needed to
% replicate. Second, we were encouraged by a concise Overlog
% implementation of simple Paxos that had been achieved in an early
% version of P2~\cite{paxonp2}.  On the other hand, we were sobered by
% the fact that the Paxos-in-P2 effort failed to produce a useable
% implementation; like the Paxos implementation at
% Google~\cite{paxos-live}, they discovered that Lamport's
% papers~\cite{paxos-simple, part-time} present just a sketch of what
% would be necessary to implement Paxos in a practical environment.
% last clause of this paragraph can be removed, I think ~nrc

% \jmh{First step: simple Paxos validates data-centric language.}
We began by creating an Overlog implementation of basic Paxos, focusing on
correctness and adhering as closely as possible to the initial
specification. Lamport's description of Paxos is given in terms of ``ballots''
and ``ledgers,'' which correspond to network messages and stable storage,
respectively. The consensus algorithm is given as a collection of logical
invariants which describe when agents cast ballots and commit writes to
their ledgers.  In Overlog, messages and disk writes are represented as
insertions into tables with different persistence properties, while invariants
are expressed as Overlog rules.  Our first effort was clean and fairly simple:
22 Overlog rules in 53 lines of code, corresponding nearly line-for-line with
the invariants from Lamport's original paper~\cite{part-time}.  Since our entire
implementation fit on a single screen, we were able to visually confirm its
faithfulness to the original specification.  To this point, working with a
data-centric language was extremely gratifying, as we further describe
in~\cite{netdb-declare}.

%\jmh{Now, get real.  Not quite as pretty.}
Next, we needed to convert basic Paxos into a working primitive for a
distributed log.  This required adding the ability to efficiently pass a series of log
entries (``Multi-Paxos''), a liveness module, and a catchup algorithm. While the
first was for the most part a simple schema change, the latter two caused our
implementation to swell to 50 rules in roughly 400 lines of code. Echoing the experience of
Chandra et al.~\cite{paxos-live}, these enhancements made our code considerably
more difficult to check for correctness. The code also lost some of its pristine
declarative character; we return to this point in Section~\ref{sec:lessons}.

%%This was due in part to the evolution of the Paxos research
%%papers: while the original Paxos was described as a set of invariants
%%over state \jmh{say this in previous paragraph!}, most of the optimizations were described as transition
%%rules in state machines.
%\jmh{Is there a pithy example of this?} 
%%Hence we found ourselves translating state-machine pseudocode back into logical invariants, and it took some time to gain confidence in our code.  The resulting 
%%implementation is still very concise relative to a traditional programming language, but it highlighted the difficulty of using a data-centric programming model for 
%%complex tasks that were not originally specified that way. \jmh{Is this about data-centric or about logic?} 

\subsection{\BOOM-FS Integration}
\label{sec:paxos-integration}
%\jmh{Integration is simple, though.}
Once we had Paxos in place, it was straightforward to support the replication of
file system metadata. All state-altering actions are represented in the revised
\BOOM-FS as Paxos decrees, which are passed into the Paxos logic via a single
Overlog rule that intercepts tentative actions and places them into a table that
is joined with Paxos rules. Each action is considered complete at a given site
when it is ``read back'' from the Paxos log (i.e., when it becomes visible in a
join with a table representing the local copy of that log). A sequence number
field in the Paxos log table captures the globally-accepted order of actions on
all replicas.

%%\nrc{Kill this paragraph, or move to previous section.}
%%We also had to ensure that Paxos state was durable in the face of crashes.
%%Lamport's description of Paxos explicitly distinguishes between transient and
%%durable state. Our implementation already divided this state into separate
%%relations, so we simply marked the appropriate relations as durable.

%\jmh{
% hard to read!
% For atomic, unconditional actions, the Paxos layer merely chooses a serialization order. In the more common case, however, a submitted action must be validated against the current application state.  In this case, an action is accepted by a replica only if it represents a transition to a new consistent state.  Because Paxos guarantees that all replicas see the same set of transitions in the same order, if a tentative action is refused on one replica it will be refused everywhere.  A series of actions that must be treated as an atomic unit may be converted into a``multi-op'' \cite{paxos-live}
% our Paxos rules made use of a guard clause and a set of actions to be executed upon the success or failure of the guard. <-- huh?  I don't really understand.} \rcs{multi-op?  what's that?  is this something we do?  Also, the stasis description means it no longer fits here.} 
% \paa{
%     we can omit the preceeding.  that paxos enforces an order, and that all replicas make the same set of transitions, is the whole point of using a consensus protocol.  the paxos made live authors discuss the usefulness of the multi-op primitive; we don't use it in bfs so there's no point discussing it here.
% }
% Finally, to implement Paxos we had to ensure that the appropriate
% tables were stored persistently, and restored to consistency after a
% failure. As for \BOOM-FS itself, we used the 
% Finally, to implement Paxos reliably we had to add a disk persistence mechanism to \JOL, a feature that was not considered in P2.  We chose to use the Stasis storage library~\cite{stasis}, which provides atomicity and durability for concurrent transactions, and handles physical consistency of its indexes and other internal structures. 
% Unlike many transactional storage libraries, Stasis does not provide a default mechanism for concurrency control.  This suited our purposes well, since Overlog's timestep semantics isolate the local database from network events, and take it from one consistent fixpoint to another.
 % However, it does not provide consistency or isolation over ``application'' state, such as the tuples stored in \JOL's tables.  By leaving concurrency control to client code, Stasis is able to make use of deadlock avoidance instead of deadlock detection, and will never force \JOL to roll-back transactions at runtime.  Since \JOL does not contain a notion of transactional abort, recovering from storage deadlocks would have complicated our design considerably.  For Stasis recovery to work correctly, \JOL must still ensure ``Consistency'' and ``Isolation'' for its ACID transactions.  By definition, Datalog fixpoints transform application data from one consistent state to another.  Also, \JOL queues up incoming and outgoing network messages during fixpoint computation, providing isolation.  Therefore, 
% We implemented each \JOL fixpoint as a single Stasis transaction.  

% \paa{ I don't agree with the point about isolation.  when I think of isolation I think of database-transaction-style solipsism: I can write a program that pretends it isn't possible that my execution could be interleaved with that of another program.  I don't believe that this is true in overlog: the programmer needs to reason about concurrency because fixpoints process batches of tuples from potentially many other nodes, and the order and timing of these events can be arbitrary.  
% }

% Many of the tables in \BOOMA represent transient in-memory data
% structures, even though they are represented as ``database'' tables.
% Hence we decided to allow \JOL programs to decide which tables should
% be durably stored in Stasis, and which should be transient.  Fixpoints
% that do not touch durable tables do not create Stasis transactions.

We validated the performance of our implementation experimentally. In the
absence of failure, replication has negligible performance impact, but when the
primary \NN fails, a backup \NN takes over reasonably quickly.  We present
performance results in the technical report~\cite{boom-techr}.

% Rusty notes:
% \begin{itemize}
% \item
%   RCS Local durability primitive: Force persistent tables at end of
%   timestamp.  (Paxos 'just works' (?)) Need datalog rewrite for 2PC's
%   prepare (ie: transactions in datalog rewrites).  2PC would be a
%   forward ref to performance section if it is here.  Splitting 2PC
%   into perf section might confuse durability story.
% 
% \item
%   Replicated HDFS master nodes ($\to$ crash recovery; assuming we have
%   enough elements in the log?)
% 
% \end{itemize}

\subsection{Discussion}
Our Paxos implementation constituted roughly 400 lines of code and required six
person-weeks of development time. Adding Paxos support to \BOOM-FS took two
person-days and required making mechanical changes to ten \BOOM-FS rules (as
described in Section~\ref{sec:paxos-integration}). We suspect that the rule
modifications required to add Paxos support could be performed as an automatic
rewrite.

% \jmh{I need help here separating out the benefits of reifying data from using
% Overlog.  However we do it we need to tone down the self-praise.  This subsection needs work.}
% The Availability revision was our first foray into ``serious''
% distributed systems programming, and we continued to benefit from the
% high-level abstractions provided by Overlog.  Most of our attention
% was focused at the appropriate level of complexity: faithfully
% capturing the reasoning involved in distributed protocols.
% \paa{not sure what to do with the previous sentence}

Lamport's original paper describes Paxos as a set of logical invariants.  This
specification naturally lent itself to a data-centric design in which
``ballots,'' ``ledgers,'' internal counters and vote-counting logic are
represented uniformly as tables.  However, as we note in a workshop
paper~\cite{netdb-declare}, the principal benefit of our approach came directly
from our use of a rule-based declarative language to encode Lamport's
invariants.  We found that we were able to capture the design patterns
frequently encountered in consensus protocols (e.g., multicast, voting) via the
composition of language constructs like aggregation, selection and join.

In our initial implementation of basic Paxos, we found that each rule covered a
large portion of the state space, avoiding the case-by-case transitions that
would need to be specified in a state machine-based implementation.  However,
choosing an invariant-based approach made it harder to adopt optimizations from
the literature as the code evolved, in part because these optimizations were
often described using state machines. We had to choose between translating the
optimizations ``up'' to a higher-level while preserving their intent, or
directly ``encoding'' the state machine into logic, resulting in a
lower-level implementation.  In the end, we adopted both approaches, giving
sections of the code a hybrid feel.

%%For example, a common optimization of basic Paxos avoids
%%the high messaging cost of reaching quorum by skipping the protocol's
%%first phase once a master has established quorum: subsequent decrees
%%then use the established quorum, and merely hold rounds of voting
%%while steady state is maintained.  This is naturally expressed in a
%%state machine model as a pair of transition rules for the same input
%%(a request) given different starting states.  
%%In our implementation,
%%we frequently found it easier in such cases to model the state as a relation
%%with a single row, allow certain rules to fire only in certain 
%%states, and explicitly describe the transitions, rather than to reformulate
%%the optimizations in terms of original logic.  Though mimicking a state 
%%machine is straightforward, the resulting rules have a 
%%hybrid feel, which somewhat compromises our high-level protocol specification.

% These are simple uses of Overlog
% persistence, but for our purposes to date this model has been
% sufficient.  Stasis was an elegant fit due to its separation of
% atomic, durable writes from traditional transactional locking for
% isolation and consistency.  However, given the modest load we put on
% Stasis, we also would probably have been fine using a traditional
% database.

%\jmh{We return to this point in Section~\ref{sec:lessons}.}
