\section{The Scalability Rev}
\label{sec:scale}
% \begin{itemize}
% \item
%   Partitioned HDFS master node (done)
% \item
%   Debugging at scale (tyson's tracker load view vs. scheduler load view).
% \item
%   There must be more things...
% \end{itemize}

HDFS {\NN}s manage large amounts of file system metadata, which are kept in
memory to ensure good performance. The original GFS paper acknowledged that this
could cause significant memory pressure~\cite{gfs-sosp}, and \NN scaling is
often an issue in practice at Yahoo!.  Given the data-centric nature of
\BOOM-FS, we hoped to simply scale out the \NN across multiple
\emph{\NN-partitions}. Having exposed the system state in tables, this was
straightforward: it involved adding a ``partition'' column to various tables to
split them across nodes in a simple way. Once this was done, the code to query
those partitions --- regardless of language in which it is written --- composes
cleanly with our availability implementation: each \NN-partition can be deployed
either as a single node or a Paxos group.

There are many options for partitioning the files in a directory tree.  We opted
for a simple strategy based on the hash of the fully-qualified pathname of each
file.  We also modified the client library to broadcast requests for directory
listings and directory creation to every \NN-partition.  Although the resulting
directory creation implementation is not atomic, it is idempotent; recreating a
partially-created directory will restore the system to a consistent state, and
will preserve any files in the partially-created directory. For all other
\BOOM-FS operations, clients have enough local information to determine the
correct \NN-partition. 

We did not attempt to support atomic ``rename'' across partitions. This would
involve the atomic transfer of state between independent Paxos groups.
We believe this would be relatively straightforward to implement --- we have
previously built a two-phase commit protocol in Overlog~\cite{netdb-declare} ---
but we decided not to pursue this feature at present.

%\jmh{Figure~\ref{fig:scaling} shows...}

\subsection{Discussion}
By isolating the file system state into relations, it became a textbook exercise
to partition that state across nodes.  It took eight hours of developer time to
implement \NN partitioning; two of these hours were spent adding partitioning
and broadcast support to the \BOOM-FS client library.  This was a clear win for
the data-centric approach, independent of any declarative features of Overlog.

Before attempting this work, we were unsure whether partitioning for scale-out
would compose naturally with state replication for fault tolerance. Because
scale-out in \BOOM-FS amounted to little more than partitioning data
collections, we found it quite easy to convince ourselves that our scalability
improvements integrated correctly with Paxos. Again, this was primarily due to
the data-centric nature of our design. Using a declarative language
led to a concise codebase that was easier to understand, but the essential
benefits of our approach would likely have applied to a data-centric
implementation in a traditional imperative language.
