\section{Introduction}
% Cloud is the next platform
Clusters of commodity hardware have become a standard architecture for
datacenters over the last decade.
% , and the major online services have
% all invested heavily in cluster software infrastructure (e.g.,
% \cite{brewer-giant,chubby,mapreduce-osdi,dynamo,gfs-sosp}).  These
% distributed systems manage difficult
% issues including parallelism, communication, failure, and system
% resizing.  They provide the foundation for basic service operation, and also serve as key software development tools for in-house developers.
The advent of \emph{cloud computing} promises to commoditize
this architecture, enabling third-party
developers to simply and economically build and host applications on managed clusters.  

Today's cloud interfaces are convenient for launching multiple independent
instances of traditional single-node services, but writing truly distributed
software remains a significant challenge.  Distributed applications still
require a developer to orchestrate concurrent computation and communication
across machines, in a manner that is robust to delays and failures.  Writing and
debugging such code is difficult even for experienced infrastructure
programmers, and drives away many creative software designers who might
otherwise have innovative uses for cloud computing platforms.


%  impedes innovative uses of the
% distributed potential of cloud platforms.

% prevents most rank-and-file programmers from making innovative use of the
% distributed potential of cloud platforms.

%%The right programming model for the cloud remains an open question.

Although distributed programming remains hard today, one important subclass is
relatively well-understood by programmers: data-parallel computations expressed
using interfaces like MapReduce~\cite{mapreduce-osdi}, Dryad~\cite{dryad}, and
SQL\@.  These programming models substantially raise the level of abstraction
for programmers: they mask the coordination of threads and events, and instead
ask programmers to focus on applying functional or logical expressions to
collections of data.  These expressions are then auto-parallelized via a
dataflow runtime that partitions and shuffles the data across machines in the
network. Although easy to learn, these programming models have traditionally
been restricted to batch-oriented computations and data analysis tasks --- a
rather specialized subset of distributed and parallel computing.

We have recently begun the \emph{BOOM} (Berkeley Orders of Magnitude) research
project, which aims to enable developers to build orders-of-magnitude more
scalable software using orders-of-magnitude less code than the state of the
art. We began this project with two hypotheses:
\begin{enumerate}
\item
  Distributed systems benefit substantially from a \emph{data-centric} design
  style that focuses the programmer's attention on carefully capturing all the
  important state of the system as a family of collections (sets, relations,
  streams, etc.)  Given such a model, the state of the system can be distributed
  naturally and flexibly across nodes via familiar mechanisms like partitioning
  and replication.
\item The key behaviors of such systems can be naturally implemented using
  \emph{declarative} programming languages that manipulate these collections,
abstracting the programmer from
  both the physical layout of the data and the fine-grained orchestration of
  data manipulation.
\end{enumerate}
Taken together, these hypotheses suggest that traditionally difficult
distributed programming tasks can be recast as data processing problems that are
easy to reason about in a distributed setting and expressible in a high-level
language.  In turn, this should provide significant reductions in code
complexity and development overhead, and improve system evolution and program
correctness.  We also conjecture that these hypotheses, taken separately, can
offer design guidelines useful in a wide variety of programming models.

%; we return to this notion in Section~\ref{sec:contributions}.

% \nrc{Add a note here that ``data-centric'' is broader than declarative, or leave
%   that for 1.2?}

% We began the work we describe in this paper with a conjecture: that {\em the
%   majority of distributed systems software can be mapped cleanly into a
%   high-level, data-centric programming model}.  If true, this suggests that
% tricky distributed programming tasks can be recast as much simpler distributed
% data processing tasks. Our confidence in this conjecture was strengthened by
% reading recent literature on datacenter infrastructure (e.g.,
% \cite{chubby,gfs-sosp,dynamo,mapreduce-osdi}).  Most of the logic in these
% efforts involves managing various forms of asynchronously-updated state
% including sessions, protocols, and storage.  Few of the ideas in these systems
% involve intricate, uninterrupted sequences of computational steps.

% If complex distributed programs can be cast as partitioned data-parallel
% computations, it should be possible to develop, debug and extend this code at a
% far higher level of abstraction than is used in current practice.  And this
% should in turn provide significant improvements in code simplicity, speed of
% development, ease of system evolution, and program correctness.

% 
% % MapReduce is an early indicator
% %% A notable counter-example to this phenomenon is 
% The MapReduce framework popularized by Google~\cite{mapreduce-osdi} and Hadoop is
% a step in this direction,  enabling a wide range of developers to easily coordinate large numbers of machines.  MapReduce raises the programming abstraction from a traditional von Neumann model to a functional dataflow model that can be easily auto-parallelized over a shared-storage architecture.   MapReduce programmers think in a {\em data-centric} fashion: they worry about handling sets of data records, rather than managing fine-grained threads, processes, communication and coordination.  
% Move SQL to related work, it's not necessary for motivation.
%MapReduce is similar in many ways to the shared-disk and shared-nothing architectures~\cite{stonebraker-sharedDB} prevalent in modern database systems, which also expose a high-level data-centric language to programmers.  Clearly neither MapReduce nor relational databases offer a general-purpose programming model for the cloud:  
% MapReduce is targeted specifically at batch processing, and database  transaction processing is too heavyweight for building general-purpose distributed programs.  But they do seem to hold the promise of a more attractive programming model for datacenters, if they can be generalized and made more nimble.
% MapReduce achieves its simplicity in part by constraining its usage to batch-processing tasks.  Although limited in this sense, it points suggestively toward more attractive programming models for datacenters.
% 

\subsection{\BOOMA}
We decided to begin the BOOM project with an experiment in construction, by implementing a substantial piece of
distributed software in a data-centric, declarative style. Upon review of recent literature on
datacenter infrastructure (e.g.,~\cite{chubby,gfs-sosp,dynamo,mapreduce-osdi}),
we observed that most of the complexity in these systems relates to the
management of various forms of asynchronously-updated state, including sessions,
protocols, and storage. Although quite complex, few of these systems involve intricate, uninterrupted
sequences of computational steps. Hence, we suspected that datacenter
infrastructure might be a good initial litmus test for our hypotheses about building distributed software.

In this paper, we report on our experiences building \emph{\BOOMA,} an
API-compliant reimplementation of the HDFS distributed file system and the
Hadoop MapReduce engine.  We named these two
components \emph{\BOOM-FS} and \emph{\BOOM-MR}, respectively.\footnote{The \BOOMA software described in this paper can be
  found at \url{http://db.cs.berkeley.edu/eurosys-2010}.} In writing \BOOMA,
we preserved the Java API ``skin'' of HDFS and Hadoop, but replaced complex
internal state with a set of relations, and replaced key system
logic with code written in a declarative language.
%  Our goal
% with \BOOMA was first to achieve a functional reimplementation of these systems
% (with competitive performance), and then to extend our work by adding advanced
% fault-tolerance and scale features that are typical for cloud computing.

% We decided to put this idea to the test with an experiment in software engineering: implementing a well-known but non-trivial piece of software infrastructure in a data-centric style, and attempting to extend it with advanced fault-tolerance and scale features that are typical for cloud computing.
% We chose Overlog not because we believe it to be inherently the ``right'' choice, but rather because it seemed like a good enough choice to gain experience.  Part of the exercise in this work is to evaluate its strengths and weaknesses, with an eye toward designing a new languge.
 % to see if a language with a very high level, data-centric abstraction could improve our productivity without introducing major computational bottlenecks.  
 % 
 % 
%We set out to convince ourselves of this idea in a realistic setting.
% Choose a language
% As starting points, we considered practical lessons from the success of MapReduce and SQL in harnessing parallelism, even though they are not general-purpose programming languages. We also examined the declarative domain-specific languages that have emerged in an increasing variety of research communities in recent years (Section~\ref{sec:relwork}).  
% %We were attracted to declarative languages -- i.e. languages based in logic -- because they are amenable not only to parallel dataflow implementations a la MapReduce, but also various guarantees via static code analysis.  
% Among the proposals in the literature, the most natural for our purposes seemed to be the Overlog language introduced in the P2 system~\cite{p2}. 
% Overlog looked promising for our setting: there is pre-existing code
% for network protocol specification, it has been shown to be useful for
% distributed coordination protocols~\cite{paxonp2}, and it offers an
% elegant metaprogramming framework for static analysis, program
% rewriting, and generation of runtime invariant
% checks~\cite{evitaraced}.  
% Choose a target app
% To evaluate the feasibility of \BOOM, 
% For this purpose, we began building {\em \BOOMA}: an API-compliant
% reimplementation of the HDFS distributed filesystem and the Hadoop
% MapReduce engine.\footnote{\BOOM stands for the {\em Berkeley Orders Of Magnitude} project that grew from this effort.  The aim of the \BOOM project is to build orders of magnitude bigger systems in orders of magnitude less code.} We named these two components {\em \BOOM-FS} and
% {\em \BOOM-MR}, respectively. In writing \BOOMA, we preserved the Java
% API ``skin'' of HDFS and Hadoop, but replaced complex internal state
% with a set of relations.  This enabled us to express
% the system logic in a declarative language.

The Hadoop stack appealed to us as a challenge for two reasons.  First, it
exercises the distributed power of a cluster.  Unlike a farm of independent web
service instances, the HDFS and Hadoop code entails coordination of large
numbers of nodes toward common tasks.  Second, Hadoop is missing significant
distributed systems features like availability and scalability of master
nodes. This allowed us to evaluate the difficulty of extending \BOOMA with
complex features not found in the original codebase.

%  This encouraged us to challenge our hypotheses further: not only would we
% try to re-implement a well-understood distributed application in a data-centric
% fashion, but also evaluate how easy it is to extend with complex features that
% had not been attempted in the original codebase.

We implemented \BOOMA using the Overlog logic language, originally developed for
Declarative Networking~\cite{p2}.  Overlog has been used with some success to
prototype distributed system protocols, notably in a simple prototype of
Paxos~\cite{paxonp2}, a set of Byzantine Fault Tolerance variants~\cite{singh-nsdi}, a suite of distributed file system consistency
protocols~\cite{pads}, and a distributed hash table routing protocol implemented
by our own group~\cite{p2}.  On the other hand, Overlog had not previously been
used to implement a full-featured distributed system on the scale of Hadoop and
HDFS\@.  One goal of our work on \BOOMA was to evaluate the strengths and
weaknesses of Overlog for system programming in the large, to inform the design of a new declarative
framework for distributed programming.

\subsection{Contributions}
\label{sec:contributions}
This paper describes our experience implementing and evolving \BOOMA, and
running it on Amazon EC2.  We document the effort required to develop \BOOMA in
Overlog, and the way we were able to introduce significant extensions, including
Paxos-supported replicated-master availability, and multi-master
state-partitioned scalability.  We describe the debugging tasks that arose when
programming at this level of abstraction, and our tactics for metaprogramming
Overlog to instrument our distributed system at runtime.

While the outcome of any software experience is bound in part to the specific
programming tools used, there are hopefully more general lessons that can be
extracted.  To that end, we try to separate out (and in some cases critique) the
specifics of Overlog as a declarative language, and the more general lessons of high-level data-centric
programming.  
%The end goal is to evaluate the benefits of data-centric
%programming for cloud infrastructure and applications.  In that regard 
The more
general data-centric aspect of the work is both positive and language-independent: many of the benefits
we describe arise from exposing as much system state as possible via collection
data types, and proceeding from that basis to write simple code to manage those
collections.%   \jmh{Try to weave this throughout, replacing points where we
%   currently talk about Overlog benefits when appropriate.  We should also have a
%   discussion of (a) ``logic vs. simple iterators'' and (b) state management and
%   temporal issues with a ref to the Dedalus TR.}

% As in any software experience paper, much of our discussion is anecdotal,
% supported only ``softly'' by the code artifacts we produced.  However, we also
% attempt to provide some quantitative evidence for the software engineering
% benefits of data-centric distributed programming.  

As we describe each module of \BOOMA, we report the person-hours we spent
implementing it and the size of our implementation in lines of code (comparing
against the relevant feature of Hadoop, if appropriate). These are noisy
metrics, so we are most interested in numbers that transcend the noise terms:
for example, order-of-magnitude reductions in code size.  We also validate that
the performance of \BOOMA is competitive with the original Hadoop codebase.



% 
% Although our focus in this paper is not on improving the performance of Hadoop, we also present performance measurements on EC2 to validate that our approach does not degrade performance relative to the Java codebase used in production.


% We found that while Hadoop's imperative implementation mixes mechanism and
% policy, our declarative approach led to a natural decoupling of the
% two.  
% In turn \BOOMA is significantly more malleable than vanilla Hadoop. 

% This 
% After twelve months of development, \BOOMA performed as well as vanilla Hadoop, and enabled
% us to easily add complex new features including Paxos-supported
% replicated-master availability, and multi-master state-partitioned
% scalability.  
% 
% Our experience implementing \BOOMA in Overlog was gratifying
% both in its relative ease, and in the lessons learned along the way:
% lessons in how to quickly prototype and debug distributed software,
% and in understanding limitations of Overlog that may contribute to an
% even better programming environment for datacenter development.

%Our principle hypothesis is that recasting the problem of programming 
%distributed systems from a model of explicit communicating processes
%to a 
% Raising the level of abstraction to parallel data manipulation eases the burden of understanding,
% constructing, and evolving such systems.  This assertion is necessarily subjective, 
% and we validate it as best we can.   We consider the reduced complexity of the
% final product through measures like lines of code, the ease of system extensibility
% by illustrating the relative independence and isolation of various modificatons, 
% and the viability of our approach for production systems through performance evaluations.


We present the evolution of \BOOMA from a straightforward reimplementation of
HDFS and Hadoop to a significantly enhanced system.  We describe how our initial
\BOOM-FS prototype went through a series of major revisions (``revs'') focused
on \emph{availability} (Section~\ref{sec:rely}), \emph{scalability}
(Section~\ref{sec:scale}), and \emph{debugging and monitoring}
(Section~\ref{sec:manage}). We then detail how we designed \BOOM-MR by replacing
Hadoop's task scheduling logic with a declarative scheduling framework
(Section~\ref{sec:mr}). In each case, we discuss how the
data-centric approach influenced our design, and how the modifications involved
interacted with earlier revisions.  We compare the performance of \BOOMA with
Hadoop in Section~\ref{sec:eval}, and reflect on the experiment in
Section~\ref{sec:lessons}.

% \jmh{Probably we want some mnemonic religious themes here that we carry through the paper. I.e.: ``In each section we reflect on the foo-ness and bar-ness associated with using a data-centric, declarative language.  Candidates include: separation of concerns via separation of data, unification of communication, transient and persistent state, unification of fine-grained monitoring, debugging and assertion checking via metaprogramming.  Also see ``Principles'' at\linebreak https://lincoln.declarativity.net/trac/wiki/LincolnContributions.''}

