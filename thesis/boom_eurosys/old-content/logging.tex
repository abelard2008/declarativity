\subsection{Logging}
Hadoop comes with fairly extensive logging facilities that can track
not only logic internal to the application, but performance counters
that capture the current state of the worker nodes.

{\TT}s write their application logs to a local disk and rely on an
external mechanism to collect, ship and process these logs; Chukwa is
one such tool used in the Hadoop community~\cite{chukwa}.  In Chukwa,
a local {\em agent} written in Java implements a number of {\em
  adaptors} that gather files (e.g., the Hadoop log) and the output of
system utilities (e.g., top, iostat), and forward the data to
intermediaries called {\em collectors}, which in turn buffer messages
before forwarding them to {\em data sinks}.  At the data sinks, the
unstructured log data is eventually parsed by a MapReduce job,
effectively redistributing it over the cluster in HDFS.
 
We wanted to prototype similar logging facilities in Overlog, not only
because it seemed an easy extension of the existing infrastructure,
but because it would close a feedback loop that --- in future ---
could allow us to make more intelligent scheduling and placement
decisions.  Further, we observed that the mechanisms for forwarding,
buffering, aggregation and analysis of streams are already available
via Overlog.

We began by implementing Java modules that read from the
\texttt{/proc} file system and produce the results as \JOL tuples. We
also wrote Java modules to convert Hadoop application logs into
tuples. Windowing, aggregation and buffering are carried out in
Overlog, as are the summary queries run at the data sinks.

In-network buffering and aggregation were simple to implement in
Overlog, and this avoided the need to add explicit intermediary
processes to play the role of collectors.  The result was a very
simple implementation of the general Chukwa idea.  We implemented the
``agent'' and ``collector'' logic via a small set of rules that run
inside the same \JOL runtime as the \NN process.  This made our logger
easy to write, well-integrated into the rest of the system, and easily
extensible.  On the other hand, it puts the logging mechanism on the
runtime's critical path, and is unlikely to scale as well as Chukwa as
log sizes increase.  For our purposes, we were primarily interested in
gathering and acting quickly upon telemetry data, and the current
collection rates are reasonable for the existing \JOL implementation.
We are investigating alternative data forwarding pathways like those
we used for \BOOM-FS for the bulk forwarding of application logs,
which are significantly larger and are not amenable to in-network
aggregation.
