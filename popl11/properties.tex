\section{Properties}

blah blah blah

\subsection{Temporal Stratification}

\wrm{begin paste -- find a new home}
We first turn our attention to the semantics of
programs with negation.  As we will see, the inclusion of time introduces a
``source of monotonicity'' in programs that allows for clean minimal model
semantics in some surprising cases, and enables purely syntactic monotonicity
checks for a broad class of temporal programs.

\begin{lemma} \label{lemma:no-neg-unique}
%
A \slang program without negation 
has a unique minimal model.
%
\end{lemma}

\begin{proof} 
%
A \slang program without negation 
is a pure Datalog
program.  Every pure Datalog program has a unique minimal model. 
%
\end{proof}

We define syntactic stratification of a \slang program the same way it is
defined for a Datalog program:

\begin{definition}
%
A \slang program is \emph{syntactically stratifiable} if there
exists no cycle with a negative edge 
in the program's
predicate dependency graph.
%
\end{definition}

We may evaluate such a program in {\em stratum order} as described in the
Datalog literature~\cite{ullmanbook}.
It is easy to see that any syntactically stratified \slang instance has a
unique minimal model because it is a syntactically stratified Datalog program.
\paa{fix}

However, many programs we are interested in expressing are not syntactically
stratifiable.  Fortunately, we are able to define a syntactically checkable
notion of {\em temporal stratifiability} of \slang programs that maps to a
subset of {\em modularly stratifiable}~\cite{modular} Datalog programs.

\begin{definition} 
%
The \emph{deductive reduction} of a \slang program $P$ is
the subset of $P$ consisting of exactly the deductive rules in $P$.
%
\end{definition}

\begin{definition} 
%
A \slang program is \emph{temporally stratifiable} if its deductive
reduction is syntactically stratifiable.
%
\end{definition}

%%\newtheorem{theorem}{Theorem}
\begin{lemma}
\label{lemma:temp-strat-uniq}
%
Any temporally stratifiable \slang instance $P$ has a unique minimal model.
%
\end{lemma} 


\begin{example}
\label{ex:stratsafe}
A simple temporally stratifiable and temporally safe \slang instance that is neither syntactically stratifiable nor safe.

\begin{Dedalus}
persist[p, 2]  
  
r1
p(A, B) \(\leftarrow\)
  insert\_p\_req(A, B);

r2  
delete p(A, B) \(\leftarrow\)
  p(A, B),
  del\_p\_req(A);

insert\_p(1, 2)@1;
\end{Dedalus}
\end{example}

In the \slang program in Example~\ref{ex:stratsafe}, 
\emph{insert\_p} and \emph{delete\_p} are captured
in EDB relations.  This reasonable program is unstratifiable because $p \succ
p\nega \land p\nega \succ p$.  But because the successor relation is
constrained such that $\forall A,B, successor(A, B) \rightarrow B > A$, any
such program is modularly stratified on \emph{successor}.  Therefore, we have
$p_{n} \not\succ^* p\_neg_{n} \not\succ^* p_{n+1}$; informally, earlier values
do not depend on later values.

Given this discussion, in practice we are interested in three asynchronous scenarios: (a) monotonic programs (even with non-monotonicity in time), (b) non-monotonic programs whose semantics guarantee monotonicity of time suffixes  and (c) non-monotonic programs where we have domain knowledge guaranteeing monotonicity of time suffixes.  Each represents practical scenarios of interest.

The first category captures the spirit of many simple distributed implementations that are built atop unreliable asynchronous substrates.  For example, in some Internet publishing applications (weblogs, online fora), it is possible due to caching or failure that a ``thread'' of discussion arrives out of order, with responses appearing before the comments they reference.  In many cases a monotonic ``bag semantics'' for the comment program is considered a reasonable interface for readers, and the ability to tolerate temporal anomalies simplifies the challenge of scaling a system through distribution.

The second scenario is achieved in \slang via the use of \dedalus{successor} for the time suffix. The asynchronous rules of \lang require additional program logic to guarantee monotonic increases in time for predicates with dependencies.  In the literature of distributed computing, this is known as a {\em causal ordering} and is enforced by distributed clock protocols.  We review one classic protocol in the \lang context in Section~\ref{sec:lamport}; including this protocol into \lang programs ensures temporal monotonicity.

Finally, certain computational substrates guarantee monotonicity in both timestamps and message ordering---for example, some multiprocessor cache coherency protocols achieve this.  When temporal monotonicity is given, the proofs of temporal stratification and Algorithm~\ref{alg:tsn} both apply.
\wrm{end paste}

\subsubsection{Excluding Time Travel}

So far, we have not restricted asynchronous rules to prevent {\em time travel} -- a condition where an asynchronous rule gives rise to derivations that precede their antecedents in time.  While time travel is acceptable for monotonic logic, it is undesirable in general, as a fact may derive its own negation (intuitively, a ``temporal paradox.'')  Thus, an additional condition is required to exclude such contradictions.  In the following section, we will see how our design goals for \lang's distributed execution dictate a particular conservative condition.

\subsubsection{Operational Semantics for Assigning Timestamps}

Consider the the following Dedalus rule, shown in unsugared form with time suffixes: 
\begin{Dedalus}
p(X,\(\DS\)) :- q(X, \(\DT\)), !r(X, \(\DT\)), \(\DS\) == \(\DT\);
\end{Dedalus}
This rule cannot concude any \dedalus{p} facts until it has information about all \dedalus{r} facts that exist at a time, for example \dedalus{\(\DT\) == 3}. Because \lang admits distribution, it is possible that \dedalus{r} facts may be derived by remote nodes by async rules.  Since facts can be arbitrarily delayed (in real-time) during transmission, receivers may need to wait an arbitrary amount of time for \dedalus{r} facts at logical time 3.

This condition is clearly \wrm{not really that clear I guess?} suboptimal, as it impedes progress; we would like to rule out the need for such waiting.  A natural optimization is to allow the evaluation to conclude that if there are immediately (in real time) no \dedalus{r} facts at logical time 3, then there will never (in real time) be any \dedalus{r} fact at logical time 3.  This optimization requires receivers to assign their own timestamps, and thus maintain their own logical clocks.

Can the maintenance of a logical clock, and assignment of timestamps be expressed in logic?  Clearly, this would involve non-Dedalus rules, as Dedalus rules prohibit arbitrary modification of the head timestamp.  Interestingly, these rules cannot be expressed in logic.  These rules would need to compute the ``next unused timestamp,'' \wrm{show this is necessary and sufficient} as new incoming facts would need to occur at a stricty higher timestamp than any existing facts.  Assume there is a rule that models the ``next unused timestamp''.  Clearly, this rule must quantify over the current timestamps of all facts in all predicates.  As a transitive consequence, this rule will derive a new fact with a higher timestamp.  This derivation must happen at a timestamp that the original rule does not quantify over, otherwise the derivation will occur only at infinity.  However, if the original rule does not quantify over all time, it cannot produce a correct value for the ``next unused timestamp.'' 
\paa{I believe that all of this is true (or mostly true) but am unsure what purpose it serves...} \wrm{i think it's extremely important.  first, if hewitt or any actor model dudes read this, they'll be like "ha! extralogical stuff! great, this corresponds with what i know to be true! and isn't this neat, they've come up with another argument for why this thing i know to be true is true. but wow, even though there's extralogical stuff happening, this is relatively elegant and surprisingly intuitive.  and ah yes, though we can't logically assign timestamps, we can logically model the possible executions within the extent of a single node." instead of "this seems fishy, where are you hiding the extralogical stuff?".  also, if i were reading this paper i'd be curious about why we're throwing in operational semantics rather than describing this in a model-theoretic way.}

Since timestamp assignment cannot be modeled in logic, we define an operational semantics of timestamp assignment: \wrm{put in fancy-pants figures?}  the receiver assigns the ``next unused timestamp'' to an incoming fact.  Of course, these operational semantics constrain the set of possible models.  We desire a model-theoretic restriction on \dedalus{@async} timestamp assignment that enables us to precisely capture these operational semantics in \lang.

%~\paa{you sort of say it in the very next section,
%but perhaps want to emphasize that this 'opsem' is always going to be contained in the model, 
%but is more restrictive}

\subsubsection{Reality Constraint}

\dedalus{@async} is clearly more liberal than our operational semantics defined above.  One of our goals is formal verification of distributed systems.  Thus, for the particular use-case of distributed execution with the above operational semantics, we need to discover a logical condition that results in a set of models that exhibit the same set of behaviors.  It is  necessary and sufficient for each fact to carry an entangled timestamp from each node.  Timestamps must be propagated through derivations \wrm{explain}.  When a node receives a fact, it will need to delay processing of the fact until its current time is greater than its entangled timestamp.  Note that this scheme employs vector clocks.

Note that the use of Lamport clocks enforces additional constraints by ruling out more possible interleavings of events than the operational semantics for timestamp assignment. \wrm{check if this is true}
\paa{we'll put the lamport clock discussion here, then?}

\subsection{Temporal Safety}

\wrm{begin paste}
In the previous section we demonstrated that \slang can capture
intuitive notions of persistence and mutability of state via a
stylized use of Datalog.  However, the alert reader will note that
even very simple \slang programs make for unusual Datalog.
%%: among other concerns, 
To begin with, persistence rules are unsafe, as they produce derivations for an infinite number
of values of the time suffix.  Traditional Datalog interpreters, which
work against static databases, would attempt to enumerate these
values, making this approach impractical.
Equally worrisome is the fact that many common patterns for state update via mutable
persistence entail unstratifiable constructs: predicates that syntactically depend on their
own (possibly transitive) negation.  

However, in the context of distributed systems and networks, the need
for non-terminating ``services'' or ``protocols'' that continually update their
state
is very common.  In this section we show that expressing distributed systems properties
such as persistence and mutable state in logic does not require
dispensing with familiar notions of safety and stratification: we take
traditional notions of acceptable Datalog programs, and extend them in
a way that admits sensible non-terminating programs.

\subsection{Temporal Safety}
Next we consider the issue of infinite results raised in the introduction to this section.
In traditional Datalog, this is a well-studied concern.
A Datalog program is considered {\em safe} if it has a finite minimal model, and hence has
a finite execution.  Safety in Datalog is traditionally ensured
through the following syntactic constraints:

\begin{enumerate}
%
\item No functions are allowed.
%
\item Variables are \emph{range restricted}: all attributes of the head goal
appear in a non-negated body subgoal.
%
\item The EDB is finite.
%
\end{enumerate}


\wrm{end paste}

Next we consider the issue of infinite results.  In traditional Datalog, a Datalog program is considered safe if it has a finite
minimal model, and hence has a finite execution. Safety in Datalog is ensured by requiring a finite universe of constants \wrm{cite}.

In Dedalus, the program's result is the limit of the program as time goes to infinity.  Nonconvergence is a special case that we handle in the style of a {\em partial fixpoint}: nonconvergent facts are considered to be false.  Other work has proposed the use of infinite objects to represent nonconvergent results \wrm{cite chomicki}.  Absent entanglement, the set of convergent facts is always finite and can be efficiently computed \wrm{cite the paper that has the algorithm for this}.  Additionaly, we presented conservative syntactic conditions in previous work to guarantee convergence for all EDBs \wrm{cite tech report}.

However, in this work, we regard the limit of the program as only those predicates declared {\em naively persistent}.  \wrm{how do we ensure finiteness????}

\subsection{Properties of Asynchronous Derivations}

um, commutativity.

when can we say that the ND choice of timestamp in async rules does not affect the outcome
of a computation?  in the general case, it almost always does (since 'outcome' implies finality,
and thus rules out the possibility of choosing a timestamp after the outcome that could affect the outcome.

we can reason, however, about order independence on a predicate-by-predicate basis, and this
can significantly simplify how we reason about the replication/distribution of state in the
global program.  informally, we say that any predicate all of whose possible derivations 
do not transit across async rules is order-independent, but this is vacuous b/c those derivations
are fundamentally unordered (well, order is hidden in the interpreter).  more substantively,
we may say that by default, all event predicates (all predicates to a first approximation) 
are order-dependent b/c the choice of timestamp determines all of their semantics.  
can we restore order independence?  yes: perist the predicate.  now any tuple inserted will
"be there at infinity" -- the only ordering detail of semantic significance is now the 1st 
timestamp of peristence.  and even this matters only when program logic is nonmonotonic.

a program with negation (aggregation, shipping, summarization) may decide at some time N
that a predicate is false, and conclude stuff from it, only to have the predicate become
true at some M > N.  this means conclusions exists that are inconsistent with valuations at
infinity.  we know precisely the circumstances under which this may occur:

consider a predicate p in a dedalus program and the subgraph of the program's RGG that 
defines the predicate.  p's semantics are order-insensitive if:

1. in the global graph from EDB facts, timers etc, there are no stratum boundaries save those encapsulated in the choice expansion.

2. following each async rule, there is a simple persistence rule.  (we can generalize this some,
but not a heck of a lot.  a predicate p is 'basically' persistent of there exists a cycle in the RGG 
with exactly one inductive edge and all other edge (possibly 0) deductive, in which the 
bindings of p are preserved from p (as premise) to p (as conclusion), and in which either no other predicates participate, or in which we can show that any participating predicate is always true.




um, idempotence.

