\section{Properties}
\label{sec:properties}

%guarantee the non-existence of
%nontrivially periodic derivations in a program for all EDBs.
%But what
%exactly is the result of a \lang program?  For Datalog programs, the result is
%the program's model.  However, in \lang, the program's model includes 

%We can apply a result of Chomicki et al. \wrm{cite} to prove that
%using traditional Datalog-style safety restrictions (no functions, no
%entanglement, finite EDB, range-restricted variables), we can always
%efficiently compute a {\em partial fixpoint} of a \lang program.  A partial
%fixpoint includes all {\em convergent} (forever persisted) derivations, and
%excludes all {\em nonconvergent} (nonpersistent, or nontrivially periodic
%derivations).  Other work has proposed the use of infinite objects to represent
%nonconvergent derivations \wrm{cite chomicki}.  Additionally, \wrm{cite}
%presents conservative syntactic conditions to guarantee the non-existence of
%nontrivially periodic derivations in a program for all EDBs.

Having established the existence and finiteness of an ultimate model, we now
study the effects of non-determinism on the ultimate model.  In particular, we
study under what conditions there exists a unique ultimate model for a \lang
instance.  One may desire a more lenient criterion on the ultimate model: for
example, a programmer may settle for a guarantee that at checkout, the shopping
cart application charges the user only once -- but possibly for a subset of the
items in her cart.  We defer the study of these advanced correctness criteria to
future work.

%\subsection{Properties of Asynchronous Derivations}

%We would like to characterize a class of
%asynchronous \lang programs that are deterministic in the face of this
%non-determinism.
%We clearly cannot rely on equality of models over all time, as models of the
%same program with different {\em async} rule applications will likely differ
%(at least) in the timestamps of tuples.  However, we would like to say that
%these differences do not matter, and define determinism based on the {\em
%eventual} equivalence of models at some timestamp.

%\begin{definition}
%Consider a stable model $M$ resulting from a fixpoint evaluation of a \lang
%program P over a finite input trace.  In other words, we may obtain the
%eventual model of a \lang execution from one of its stable models by {\em
%selecting} tuples corresponding to the ``end of time,'' and projecting out
%time.  If all stable models of a \lang program and EDB trace produce the same
%ultimate model, we say that the program and trace are \emph{trace confluent},
%and if a program is trace confluent for all EDBs we say that the program is
%\emph{confluent}.
%The \emph{eventual model} of a \lang instance is the eventual contents of the
%simply persisted relations (ignoring timestamps).
%
%\end{definition}

%Every safe \lang program has a finite eventual model, and there is a
%well-known decision procedure for calculating the model \wrm{cite chomikci,
%partial fixpoint stuff}. \wrm{In practice, due to magic sets, this isn't
%horribly inefficient}~\cite{magic}

%\jmh{I'd prefer if any other basic terminology used here was  defined with similar formality and formatting when first introduced.  It makes jumping into the formal stuff much quicker -- you can more easily find all the necessary defs from before.  So far I think all that's missing is the definition for `instance'}
\begin{definition}
%
If all traces of a \lang instance produce the same ultimate model, then we call
the instance {\em trace confluent}.
%
\end{definition}

\begin{definition}
%
If every instance of a program is trace confluent, then we say the program is
{\em confluent}.
%
\end{definition}

\noindent{}It is easy to see that every \lang program without \dedalus{async}
rules is confluent, as the trace is empty.

\begin{definition}
%
Two traces $R_1, R_2$ are in the same associativity class if $R_2$ can be
derived from $R_1$ by freely applying the operations of combining two adjacent
timestamps, and splitting a single timestamp into two adjacent timestamps.
%
\end{definition}

\begin{definition}
%
We say that a \lang program is {\em associative} if, for any instance $I$,  the
ultimate models for any two traces of $I$ in the same associativity class are
equal.
%
\end{definition}

\begin{definition}
%
We say that a \lang program is {\em commutative} if the ultimate model is
invariant under the operation of swapping timestamps on two adjacent timestamps.
%
\end{definition}

\subsubsection{Associativity}

%\begin{definition}
%
%We say that two traces $R_1, R_2$ of a \lang instance have the same
%associativity grouping if $\forall t \in$ \dedalus{time} $\forall i,j \in
%\{1,2\} \forall F \subset \mathcal{H} \exists s \in$ \dedalus{time} $: F@t
%\subset T_i \land F \subset T_i \cap T_j \implies F@s \subset T_j$. \wrm{change
%this intersection to be an intersection modulo time}
%
%\end{definition}

\begin{definition}
%
A support of fact $f$ in a \lang instance is a set of facts $E$, such
that $\forall e \in E (F \succ E_i)$. \jmh{don't you want to talk about the ``complete'' support?  I.e. define the support so that $E$ contains any fact $e$ in the incremental model such that  $f \succ e$?  Come to think of it, you probably need to scope this discussion to a particular timestep, so that you discuss facts $f_t$ at time $t$.} If all $e$ in $E$ are non-negated, then
we call $E$ a positive support.  If some $e$ in $E$ are negated, or are the
output of an aggregate, then we call $E$ a negative support.  \jmh{What is a negated fact?  Do you mean that facts derived from rules with negation include the complement of some predicate in their support?  I.e. supports can be infinite/unsafe?}
%
\end{definition}

It is clear that every fact in all incremental models of a trace of a monotonic
\lang instance has a positive support.  In a trace of a non-monotonic \lang
instance, some facts necessarily have only negative supports. \jmh{you don't mean ``only''---range restriction means that support must include positive facts.} Note that
because of the syntactic restrictions of \lang, negative supports always have a
timestamp associated with them.  \jmh{I don't see why pos support can get away without timestamps.  And you need a formalism for the idea of ``associating'' a timestamp with a support.}

\begin{lemma}
%
A monotonic \lang program is confluent if and only if it is associative.
%
\end{lemma}
%
\begin{proof}
%
It is clear that the confluence of a monotonic \lang program implies its
associativity \jmh{no it's not -- you need to knit together traces and ``histories'' to make this argument}, so we need only consider the other direction.

If a \lang instance is not trace confluent, we will show the \lang program is
not associative.  Assume there exists a non-trace-confluent \lang instance $I$
with an associative program.  
\jmh{The below is unnecessarily hard to read.  Just say that $R_1$ has the first constraint, and $R_2$ has the second.}
That is, there are two traces $R_1, R_2$ for $I$,
such that 
%$\exists i,j \in \{1,2\}: i \neq j \and \exists f \in \mathcal{U}_I(R_i): f
%\not\in \mathcal{U}_I(R_j)$.  Consider $g$, the ``first'' such $f$, that is
%the $f$ that no 
$\exists i,j \in \{1,2\}: i \neq j \land f \in \bigcup_{t \in \text{time}}
\mathcal{M}_{I,T}(R_i) \land f \not\in \bigcup_{t \in \text{time}}
\mathcal{M}_{I,T}(R_j)$.  
\jmh{"first" in what sense: lowest timestamp? the next sentence is plain hard to parse.}
Let $g$ be the ``first'' such $f$, that is the $f$
such that no transitive support of $f$ is also in the set \wrm{note that
transitive support is acyclic}.  The entire contents of $g$'s transitive
support clearly is in $\bigcup_{t \in \text{time}} \mathcal{M}_{I,T}(R_i)$
\wrm{fix this} (although possibly at different times), because if it were not,
then $g$ would not be the ``first'' such $f$.  The fact that the program is
associative is important, because of \lang's syntactic restriction that rules
must unify on time.  Thus, even under different assignment of timestamps, $g$'s
transitive support still derives $g$ in $R_i$.  Thus, we have reached a
contradiction.  \wrm{toughen up, note that we have proven something stronger
than the lemma}
%
\end{proof}

Note that a simple way to ensure associativity of a monotonic program is to
persist all relations that appear in the head of an \dedalus{@async} rule.  
This ensures that all support will always be coincidentally true \jmh{``coincidentally'' is annoyingly overloaded; say what you mean explicitly in terms of timestamps}.  However,
this does not ensure associativity of a non-monotonic program, because the
support for some derivations is necessarily not positive \wrm{explain better}.  
Another way to ensure associativity of a monotonic program is to use a
queue-like construct to only consider one fact derived in an \dedalus{@async}
predicate per timestamp.  \jmh{but then the program is not monotonic.  and why are bothering to make this point?}

%Since we do not consider the time suffix as part of the ultimate model, how can
%non-determinism in time suffixes affect the ultimate model?  In purely
%monotonic programs, the only issue that can arise is that some derivations may
%not be {\em associative} -- the assignment of the same timestamp to two facts
%may cause a different result than the assignment of a distinct timestamp to
%each fact.  Intuitively, this is because we force unification on time suffixes
%in the body.  Persistence of async relations \nrc{Don't you mean ``rules'', not ``relations''?} solves this by making all
%derivations associative: there will eventually be a timestamp where all async
%derivations ever received are coincidentally true.\nrc{Don't grok this.}

% peter got freaked out by this, so i commented it out -wrm
%\wrm{provide example here?}  Concretely, consider the committed choice
%example in Section~\ref{sec:assignment-and-committed-choice}, which
%inserts the first value of \dedalus{condition(A,\_)} into \dedalus{log}.
%Careful readers may have noticed that, if multiple conflicting
%\dedalus{condition} tuples appear in the same timestamp, multiple
%conflicting values will be inserted into the \dedalus{log} relation.
%An associativity analysis would automatically detect such bugs.
%Furthermore, non-associative rules can always be transformed \rcs{true, right?}
%into associative rules by introducing {\em tie breaking} predicates
%and aggregates; in the committed choice example, we would simply
%replace the left hand side of the second rule
%(\dedalus{log(A,B)@next}) with \dedalus{log(A,min<B>)@next}.

%joe got freaked out by this, so commenting it out -wrm
%Alternatively, we can avoid the need for such rewrites by restricting
%the set of permissible executions, just as we did to disallow temporal
%paradoxes.  From a pragmatic perspective, we have found that it is
%convenient to process each arriving event atomically before processing
%the next.  Doing so ensures that our programs are trivially
%associative; two events will never arrive simultaneously.  This
%greatly simplifies our programs.

%joe got freaked out by this, so commenting it out -wrm
%From a model theoretic perspective, we apply a compile-time rewrite
%that duplicates each \dedalus{@next} rule, leaving one copy intact,
%and replacing the head of the other with \dedalus{busy(1)@next}.
%Instead of delivering tuples directly, \dedalus{@async}'s choice
%construct inserts the tuples into a queue, as in
%Section~\ref{sec:priority-queues}.  For each timestamp \dedalus{T}
%where \dedalus{busy(1)@T} is not defined, we dequeue and deliver
%exactly one tuple.\rcs{bill had some nice things to say about this approach regarding analysability, etc...}

%joe got freaked out by this, so commenting it out -wrm
%From an operational perspective, this allows \lang programs to
%leverage large bodies of existing work in database concurrency control
%and compiler optimization techniques that automatically parallelize
%code, strengthen loops, and so on.\rcs{cite two textbooks?}  \rcs{Naturally, programs that make use of imperative constructs will complicate such techniques; such issues existed in overlog, and led to semantic ambiguities that complicated earlier research.  We believe that \lang's foundation in logic will largely avoid such issues.  -- (Not sure we want so say this, or that we can back it up)}



%We can immediately identify a subset of the class of confluent \lang programs:
%the class $D$ of monotonic \lang programs, where all predicates have simple
%persistence rules.  Intuitively, any instance of a program in $D$ is trace
%confluent because its ultimate model corresponds exactly to the model of the
%Datalog program obtained by converting all temporal and async rules into
%deductive rules and evaluating a single Datalog fixpoint.  Deductions via
%async rules introduce nondeterminism into the time suffixes of predicates.
%However, the simple persistence of all predicates ensures that for
%some value of the time suffixes, all deductions that can be made will be made.

\subsubsection{Commutativity}

\jmh{Too much prose before the example.  Pop the example in the middle (``consider the following example'') and then discuss it.  Or put it up in a Figure.}
It is easy to see that non-monotonic programs are not necessarily confluent,
even when async predicates are simply persisted.  In addition to associativity,
{\em commutativity} is now an issue: if we swap the timestamps of two messages,
we might swap the order of existential and universal quantification, as
exhibited by the instance below, where the 
%quantification over a set occurs before the membership of the set is known,
%and results of this ``incomplete'' quantification may be forever persisted.
nondeterminism implied by the async definitions of \dedalus{p} and \dedalus{q}
leads to two different ultimate models for this instance.  All traces in which
\dedalus{p} is assigned a lower timestamp than \dedalus{q} lead to an ultimate
model including \dedalus{r(1,2)}.  However, if \dedalus{q} precedes
\dedalus{p}, or is co-incident with \dedalus{p}, then \dedalus{\(\lnot\) q(A)}
is false, and the ultimate model excludes \dedalus{r(1, 2)}.

\begin{Dedalus}
persist[r,2]
persist[p,2]
persist[q,1]

r1
r(A, B) \(\leftarrow\) 
    p(A, B), \(\lnot\) q(A);

q(A)@async :- e(A);
p(A, B)@async :- f(A, B);

e(1)@10;
f(1, 2)@4;
f(3, 4)@11; 
\end{Dedalus} 

\begin{lemma}
%
A \lang program is confluent if and only if it is associative and commutative.
%
\end{lemma}

\begin{proof}
%
Again, the forward direction is clear, so we restrict our proof to the
backward direction.

If a \lang instance is not trace confluent, we will show the \lang program is
not associative or not commutative.  Assume there exists a non-trace-confluent 
\lang instance $I$ with an associative and commutative program.  \jmh{Again, simplify the below to just defint $R_1$ and $R_2$}.  That is, there
are two traces $R_1, R_2$ for $I$, such that
%$\exists i,j \in \{1,2\}: i \neq j \and \exists f \in \mathcal{U}_I(R_i): f
%\not\in \mathcal{U}_I(R_j)$.  Consider $g$, the ``first'' such $f$, that is
%the $f$ that no 
$\exists i,j \in \{1,2\}: i \neq j \land f \in \bigcup_{t \in \text{time}}
\mathcal{M}_{I,T}(R_i) \land f \not\in \bigcup_{t \in \text{time}}
\mathcal{M}_{I,T}(R_j)$.  Let $g$ be the ``first'' such $f$, that is the $f$
such that no transitive support of $f$ is also in the set \wrm{note that
transitive support is acyclic}.  The entire contents of $g$'s transitive
support clearly is in $\bigcup_{t \in \text{time}} \mathcal{M}_{I,T}(R_i)$
\wrm{fix this} (although possibly at different times), because if it were not,
then $g$ would not be the ``first'' such $f$.  The fact that the program is
associative is important, because of \lang's syntactic restriction that rules
must unify on time.  The fact that the program is commutative is also important
-- this means that universal quantification at any time allowed by data
dependencies is fine.  \wrm{this is a sketch but not a real proof.  fix tmrw
when i get some inspiration}
%Thus, even under different assignment of timestamps, $g$'s
%transitive support still derives $g$ in $R_i$.  Thus, we have reached a
%contradiction.  \wrm{toughen up, note that we have proven something stronger
%than the lemma}
%
\end{proof}
\jmh{wasn't that a verbatim copy of the previous proof?}

\jmh{the next paragraph needs more time ... introduce the concern more carefully, and I think it's worth formally making the argument as a lemma, since the mode of reasoning here is important to a lot of our notions of fine-grained monotonicity.  and I don't think it's ``easy to see'' since we never explained the Sacca/Zaniolo choice thing in detail.}
At this stage, the careful reader may be concerned that our ``monotonic''
programs permit async rules, as async rules use the \dedalus{choice} construct,
which makes use of negation.  However, it is easy to see that the universal
quantification in \dedalus{choice} happens over the \dedalus{time} set, which
is always fully defined.  In other words, universal quantification of
\dedalus{time} commutes with any other operation in the program.

%In either case, nonmontonic \wrm{need to define} reasoning (which is easily
%spotted in a logic program) entails applying the \emph{closed world
%assumption} \wrm{need to define} to a predicate which, if it transitively
%depends upon an async derivation, is never really closed \wrm{need to
%clarify}.

\subsection{Identifying Points of Order}
\jmh{tighten up this para}
The previous section provided a conservative test for confluence: all monotonic
programs with simply persisted asynchronous predicates are confluent.  In
practice, however, programs that monotonically accumulate information and never
perform some non-monotonic operation on it are rare in the distributed systems
domain.

%Non-monotonic operations in general imply universal quantification, such as
%aggregation (e.g., computing a sum) and negation (determining the
%non-existence of a fact).  Universal quantification allows the non-determinism
%of asynchronous systems to influence the ultimate model, because the universal
%quantification may occur before all messages have been received, and results
%of this ``incomplete'' quantification may be forever persisted.  Reduction in
%general implies universal quantification, whether it involves aggregation
%(e.g., computing a sum), negation (e.g., asserting that %some fact does
%\emph{not} hold because the fact does not exist in our model) or shipping a
%``final'' version of a relation to another location, and in \lang, such
%constructs can only be expressed with syntactically transparent nonmonotonic
%constructs.  

How do we maintain confluence in the presence of non-monotonicity?  We must
introduce points of order into our program. 
%---logic that preserves commutativity even in the face of non-monotonicity.
%One simple type of coordination involves guarding the universal quantification
%with a data dependency that cannot be fulfilled until the entire set
%quantified over has been computed.  This notion of ``waiting for a data
%dependency to be satisfied'' is similar to the ordered application of rules
%required for stratification.  
\jmh{You already defined the PDG (rather too tersely) in the section on Temporal Strat}
Fortunately, we can leverage the concept of a program's {\em predicate
dependency graph} (PDG) from the Datalog literature to construct an analysis
that can tell a programmer where points of order may be required to ensure
confluence.  A PDG is a directed graph that has one node per predicate in the
program.  A PDG has an edge from \dedalus{p} to \dedalus{q} if $p \succ q$.
The edge has three possible annotations: $+$ indicating that $p \succ q$ only in
\dedalus{@next} rules, $\lnot$ indicating that $p \succ q$ in at least one rule
where $q$ appears negated in the body, and $@$, indicating that $p \succ q$
only in \dedalus{@async} rules.

\begin{definition}
%
A monotonic component is a maximal connected subgraph with no $\lnot$ edges,
and all \dedalus{async} predicates simply persisted.
%
\end{definition}

Points of order can only ever be necessary on edges between monotonic
components, because a monotonic program, considered as a \lang program, is
confluent.  We have expressed this test \jmh{What test?!  You skipped the main point!  Please highlight the specific contribution here!  The next sentence is also too terse and insider (meta-model is not common parlance, I don't think)}  in our \lang interpreter, using a
meta-model to treat rules as data.  Later, we present graphs of programs that
identify monotonic components.  The data for these graphs was generated by our
\lang code that expresses this test.

%First, some
%dependency graph of the program or instance, such as a PDG, is analyzed to
%identify and collapse {\em monotonic components}---subgraphs that contain no
%negated edges.  The Datalog program is evaluated in the topological order of
%its collapsed dependency graph.  Later monotonic components ``wait'' for
%computation to be completed in earlier monotonic components.  

%In \lang, we can leverage this same analysis to identify all {\em loci of
%coordination}---all places where the programmer may need to instrument his code
%with coordination to ensure confluence.  These are exactly the edges connecting
%monotonic components.  Note that this analysis ignores the non-monotonicity in
%\dedalus{choice} for the reasons outlined above.  Note also that we still
%maintain the requirement that async relations are simply persisted to ensurei
%associativity. \wrm{talk about relaxing this?}

%\wrm{write a paragraph extolling the virtues of logic programming here, and
%explaining why this test is good.  that paragraph isn't coming to me now...} 

%\paa{need a formal description of the new (or special-cased) stratification algorithm.
%regular strat, except any async rule is a stratum boundary \emph{unless} the 
%target predicate is naively persisted, with ``p is naively persisted'' conservatively defined
%as $\exists$ a rule with $p$ in the head,  p in the body with 'the same bindings', and the rest of the body always satisfied or empty}

%Centralized evaluation of a logic program ensures confluence considering the
%program's PDG, and collapsing {\em monotonic components} -- subgraphs of the
%PDG with no negation -- and evaluating the program in topological order of the
%resulting graph. This ensures that the evaluator defers universal
%quantification over a subset of a relation until all facts in the subset are
%known.  This deferal happens precisely between monotonic components.  Note that
%we can ignore the negation in \dedalus{choice}, for the reasons described
%above.

%The utility of syntactic monotonicity checks lies in the ability to identify
%monotonic \emph{components} of a nonmonotonic program.  Stratification already
%divides a Datalog program into an ordered list of such monotonic components,
%separated by nonmonotonic constructs like negation.  Within each such
%component or \emph{stratum}, rules may be evaluated without ordering or
%coordination, but the strata must be evaluated in order and \emph{sealed}
%before downstream strata that depend nonmonotonically on them may begin
%evaluation.  

%Our use of choice in asynchronous rules uses (ambiguous) negation to represent
%indeterminacy in timestamp assignment by entailing a multiplicity of models,
%and hence on the surface always represents a stratum boundary in the global
%program.  Under this naive interpretation, communication always occurs
%between, and never within, monotonic program components: that is to say,
%communication always occurs at a coordination boundary, and it is not safe to
%process messages ``off the wire'' unless the computation that produced them is
%complete.  

%To avoid this undesirable barrier, and allow pipelining of the results of an
%ongoing computation across network (and hence temporal) boundaries, we propose
%an extension of stratification analysis that identifies when monotonic program
%components cross network boundaries. 

%\paa{the idea is simple: perform a normal stratification analysis, then
%collapse pairs of strata that are separated only b/c of an async rule (and
%it's implied NM choice expansion), when we can show that the async
%consequences are "protected" by a persistence rule.  the result is a new
%strata graph with fewer vertices, and some of whose vertices tolerate fully
%pipelined communication in addition to parallel/unordered evaluation.}

%In \lang, a distributed system is evaluated by many different nodes.  In a
%horizontally-partitoned distributed system, deferring quantification requires
%{\em coordination} between some subset of the nodes, as multiple nodes may provide
%for a subset to be quantified over.  \wrm{connect to coordination in imperative languages.}
%Within a given monotonic component (between universal quantifications), rules may be evaluated without ordering or
%coordination.  One neat upshot is that a Dedalus program can completely identify places in the program that
%might need coordination -- precisely those places in the program (PDG) with negation.  Note that we still need to persist async predicates.


%\paa{below are notes}

%when can we say that the ND choice of timestamp in async rules does not affect the outcome
%of a computation?  in the general case, it almost always does (since 'outcome' implies finality,
%and thus rules out the possibility of choosing a timestamp after the outcome that could affect the outcome.
%\wrm{huh?  the ND choice of timestamp doesn't affect the outcome if it ``doesn't affect the outcome'' (i.e. regardless of the choice value, i still get the same set of facts in the naively persistent relations, for all EDBs)  are you looking for some conservative syntactic condition?}

%we can reason, however, about order independence on a predicate-by-predicate basis, and this
%can significantly simplify how we reason about the replication/distribution of state in the
%global program.  informally, we say that any predicate all of whose possible derivations 
%do not transit across async rules is order-independent \wrm{a queue doesn't involve async, but it's order dependent}, but this is vacuous b/c those derivations
%are fundamentally unordered (well, order is hidden in the interpreter).  more substantively,
%we may say that by default, all event predicates (all predicates to a first approximation) 
%are order-dependent b/c the choice of timestamp determines all of their semantics.  
%can we restore order independence?  yes: persist the predicate.  now any tuple inserted will
%"be there at infinity" -- the only ordering detail of semantic significance is now the 1st 
%timestamp of persistence.  and even this matters only when program logic is nonmonotonic. \wrm{you also need that the inputs are either transitively dependent on only EDB, or are persisted, right?}

%a program with negation (aggregation, shipping, summarization) may decide at some time N
%that a predicate is false, and conclude stuff from it, only to have the predicate become
%true at some M > N \wrm{don't see a problem here}.  this means conclusions exists that are inconsistent with valuations at
%infinity \wrm{don't know what this means}.  we know precisely the circumstances under which this may occur:

%consider a predicate p in a dedalus program and the subgraph of the program's RGG that 
%defines the predicate.  p's semantics are order-insensitive if:

%1. in the global graph from EDB facts, timers etc, there are no stratum boundaries save those encapsulated in the choice expansion.

%2. following each async rule, there is a simple persistence rule.  (we can generalize this some,
%but not a heck of a lot.  a predicate p is 'basically' persistent of there exists a cycle in the RGG 
%with exactly one inductive edge and all other edge (possibly 0) deductive, in which the 
%bindings of p are preserved from p (as premise) to p (as conclusion), and in which either no other predicates participate, or in which we can show that any participating predicate is always true.

%we can see that these conditions imply order-insensitivity wrt ``in-flight'' tuples bound for p.
%any pair of tuples will end up in p and be there ``at some time'' if the network quiesces, at 
%which time we may consider the timestamp irrelevant.  and p is order-insensitive in this respect,
%though operations including join, till the next stratum boundary.

%but can we weaken the conditions?  in general it looks hard.  take (1): if we have indeed crossed a stratum boundary, we have in essence decided ``at some time'' that some predicate is closed to insertion, by universally quantifying over it (to assert its complement, summarize it, ship it, whatever).  recall p() from above.  a rule that negates p() in a subgoal could succeed between 
%two async insertions of p() tuples, which thus clearly do not commute.  the 'committed choice'
%pattern is a rudimentary illustration of 'sealing' a nonmonotonic consequence: log is defined 
%in terms of its own negation.  the 'assignment' pattern is a rudimentary example of... um, I guess a rule that isn't persistent (violates 2) b/c notin condition is clearly not "always true."  ditto the
%mutable state pattern.

%um, idempotence.
\wrm{idempotence}
um, efficient lazy evaluation?  \cite{magic}
