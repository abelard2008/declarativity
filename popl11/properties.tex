\section{Properties}

%guarantee the non-existence of
%nontrivially periodic derivations in a program for all EDBs.
%But what
%exactly is the result of a \lang program?  For Datalog programs, the result is
%the program's model.  However, in \lang, the program's model includes 

%We can apply a result of Chomicki et al. \wrm{cite} to prove that
%using traditional Datalog-style safety restrictions (no functions, no
%entanglement, finite EDB, range-restricted variables), we can always
%efficiently compute a {\em partial fixpoint} of a \lang program.  A partial
%fixpoint includes all {\em convergent} (forever persisted) derivations, and
%excludes all {\em nonconvergent} (nonpersistent, or nontrivially periodic
%derivations).  Other work has proposed the use of infinite objects to represent
%nonconvergent derivations \wrm{cite chomicki}.  Additionally, \wrm{cite}
%presents conservative syntactic conditions to guarantee the non-existence of
%nontrivially periodic derivations in a program for all EDBs.

\subsection{Properties of Asynchronous Derivations}

Having ensured that the finiteness of the ultimate model for any trace of a
finite \lang instance, we now turn to a problem that is specific to the domain
of asynchronous distributed systems: the effects of delay
%,loss
and reordering of messages. We would like to characterize a class of
asynchronous \lang programs that are deterministic in the face of this
non-determinism.
%We clearly cannot rely on equality of models over all time, as models of the
%same program with different {\em async} rule applications will likely differ
%(at least) in the timestamps of tuples.  However, we would like to say that
%these differences do not matter, and define determinism based on the {\em
%eventual} equivalence of models at some timestamp.

%\begin{definition}
%Consider a stable model $M$ resulting from a fixpoint evaluation of a \lang
%program P over a finite input trace.  In other words, we may obtain the
%eventual model of a \lang execution from one of its stable models by {\em
%selecting} tuples corresponding to the ``end of time,'' and projecting out
%time.  If all stable models of a \lang program and EDB trace produce the same
%ultimate model, we say that the program and trace are \emph{trace confluent},
%and if a program is trace confluent for all EDBs we say that the program is
%\emph{confluent}.
%The \emph{eventual model} of a \lang instance is the eventual contents of the
%simply persisted relations (ignoring timestamps).
%
%\end{definition}

%Every safe \lang program has a finite eventual model, and there is a
%well-known decision procedure for calculating the model \wrm{cite chomikci,
%partial fixpoint stuff}. \wrm{In practice, due to magic sets, this isn't
%horribly inefficient}~\cite{magic}
If all traces of a \lang instance produce the same ultimate model, then we
call the instance {\em trace confluent}.  If every instance of a program is
trace confluent, then we say the program is {\em confluent}.  Note that every
non-contradictory Datalog program without \dedalus{choice} is confluent.

\begin{definition}
%
We say that a \lang program is {\em associative} if the
ultimate model is invariant under the operation of exchanging subsets between
adjacent timestamps.
%
\end{definition}

\begin{definition}
%
We say that a \lang program is {\em commutative} if the ultimate model is
invariant under the operation of exchanging two timestamps.
%
\end{definition}

\begin{lemma}
%
A \lang program is confluent if and only if it is associative and commutative.
%
\end{lemma}

\subsubsection{Associativity}

\begin{lemma}
%
A monotonic \lang program is confluent if and only if it is associative.
%
\end{lemma}
%
\begin{proof}
%
It is clear that the confluence of a monotonic \lang program implies its
associativity.

Consider two traces of a \lang program, ordered by timestamp.  We say a prefix
of the two traces {\em agree modulo timestamp} if the prefixes are identical
except for the time suffix.  

\wrm{proof realy messy and bad, but general outline -- committed because i'm about to fall asleep}

Call a fact \dedalus{F}'s {\em support} the set of all sets of facts that are
logically equivalent to \dedalus{F} given a \lang instance.  Note that in a
non-monotonic program, some facts have no support -- i.e., they are derived
based on universal quantification.  Before the point of divergence, program
$P_1$ must have some support that program $P_2$ will never have.  Where did this
support come from?  If it was derived solely from the EDB, then both programs
will have this support.  Thus, it must have been derived at least partly from
the trace.  Given that $P_1$ and $P_2$ \wrm{change to instances} have the same
associativity grouping, everything coincidentally true in $P_1$ will be
coincidentally true in $P_2$ (proof).  Since a support must be coincidentally
true, the eventual models of $P_1$ and $P_2$ are equivalent.
%
\end{proof}

Note that a simple way to ensure associativity of a monotonic program is to
persist all relations that appear in the head of an \dedalus{@async} rule.
This is because this ensures eventual associativity of the program, which is
equivalent to associativity for monotonic programs.

However, eventual associativity is not good enough for non-monotonic programs.

%Since we do not consider the time suffix as part of the ultimate model, how can
%non-determinism in time suffixes affect the ultimate model?  In purely
%monotonic programs, the only issue that can arise is that some derivations may
%not be {\em associative} -- the assignment of the same timestamp to two facts
%may cause a different result than the assignment of a distinct timestamp to
%each fact.  Intuitively, this is because we force unification on time suffixes
%in the body.  Persistence of async relations \nrc{Don't you mean ``rules'', not ``relations''?} solves this by making all
%derivations associative: there will eventually be a timestamp where all async
%derivations ever received are coincidentally true.\nrc{Don't grok this.}

\wrm{provide example here?}  Concretely, consider the committed choice
example in Section~\ref{sec:assignment-and-committed-choice}, which
inserts the first value of \dedalus{condition(A,\_)} into \dedalus{log}.
Careful readers may have noticed that, if multiple conflicting
\dedalus{condition} tuples appear in the same timestamp, multiple
conflicting values will be inserted into the \dedalus{log} relation.
An associativity analysis would automatically detect such bugs.
Furthermore, non-associative rules can always be transformed \rcs{true, right?}
into associative rules by introducing {\em tie breaking} predicates
and aggregates; in the committed choice example, we would simply
replace the left hand side of the second rule
(\dedalus{log(A,B)@next}) with \dedalus{log(A,min<B>)@next}.

Alternatively, we can avoid the need for such rewrites by restricting
the set of permissible executions, just as we did to disallow temporal
paradoxes.  From a pragmatic perspective, we have found that it is
convenient to process each arriving event atomically before processing
the next.  Doing so ensures that our programs are trivially
associative; two events will never arrive simultaneously.  This
greatly simplifies our programs.

From a model theoretic perspective, we apply a compile-time rewrite
that duplicates each \dedalus{@next} rule, leaving one copy intact,
and replacing the head of the other with \dedalus{busy(1)@next}.
Instead of delivering tuples directly, \dedalus{@async}'s choice
construct inserts the tuples into a queue, as in
Section~\ref{sec:priority-queues}.  For each timestamp \dedalus{T}
where \dedalus{busy(1)@T} is not defined, we dequeue and deliver
exactly one tuple.\rcs{bill had some nice things to say about this approach regarding analysability, etc...}

From an operational perspective, this allows \lang programs to
leverage large bodies of existing work in database concurrency control
and compiler optimization techniques that automatically parallelize
code, strengthen loops, and so on.\rcs{cite two textbooks?}  \rcs{Naturally, programs that make use of imperative constructs will complicate such techniques; such issues existed in overlog, and led to semantic ambiguities that complicated earlier research.  We believe that \lang's foundation in logic will largely avoid such issues.  -- (Not sure we want so say this, or that we can back it up)}



%We can immediately identify a subset of the class of confluent \lang programs:
%the class $D$ of monotonic \lang programs, where all predicates have simple
%persistence rules.  Intuitively, any instance of a program in $D$ is trace
%confluent because its ultimate model corresponds exactly to the model of the
%Datalog program obtained by converting all temporal and async rules into
%deductive rules and evaluating a single Datalog fixpoint.  Deductions via
%async rules introduce nondeterminism into the time suffixes of predicates.
%However, the simple persistence of all predicates ensures that for
%some value of the time suffixes, all deductions that can be made will be made.

\subsubsection{Commutativity}

It is easy to see that non-monotonic programs are not necessarily confluent,
even when async predicates are simply persisted.  The intuitive issue now is
{\em commutativity}: if we swap the timestamps of two messages, we might swap
the order of existential and universal quantification, as exhibited by the
instance below, where the 
%quantification over a set occurs before the membership of the set is known,
%and results of this ``incomplete'' quantification may be forever persisted.
nondeterminism implied by the async definitions of \dedalus{p} and \dedalus{q}
leads to two different ultimate models for this instance.  All traces in which
\dedalus{p} is assigned a lower timestamp than \dedalus{q} lead to an ultimate
model including \dedalus{r(1,2)}.  However, if \dedalus{q} precedes
\dedalus{p}, then \dedalus{\(\lnot\) q(A)} is false, and the ultimate model
excludes \dedalus{r(1, 2)}.

\begin{Dedalus}
persist[r,2]
persist[p,2]
persist[q,1]

r1
r(A, B) \(\leftarrow\) 
    p(A, B), \(\lnot\) q(A);

q(A)@async :- e(A);
p(A, B)@async :- f(A, B);

e(1)@10;
f(1, 2)@4;
f(3, 4)@11; 
\end{Dedalus} 

At this stage, the careful reader may be concerned that our ``monotonic''
programs permit async rules, as async rules use the \dedalus{choice} construct,
which makes use of negation.  However, it is easy to see that the universal
quantification in \dedalus{choice} happens over the \dedalus{time} set, which
is always fully defined.  In other words, universal quantification of
\dedalus{time} commutes with any other operation in the program.

%In either case, nonmontonic \wrm{need to define} reasoning (which is easily
%spotted in a logic program) entails applying the \emph{closed world
%assumption} \wrm{need to define} to a predicate which, if it transitively
%depends upon an async derivation, is never really closed \wrm{need to
%clarify}.

\subsection{Identifying Loci of Coordination}
The previous section provided a conservative test for confluence: all monotonic
programs with simply persisted asynchronous predicates are confluent.  In
practice, however, programs that monotonically accumulate information and never
perform some non-monotonic operation on it are rare in the distributed systems
domain.

%Non-monotonic operations in general imply universal quantification, such as
%aggregation (e.g., computing a sum) and negation (determining the
%non-existence of a fact).  Universal quantification allows the non-determinism
%of asynchronous systems to influence the ultimate model, because the universal
%quantification may occur before all messages have been received, and results
%of this ``incomplete'' quantification may be forever persisted.  Reduction in
%general implies universal quantification, whether it involves aggregation
%(e.g., computing a sum), negation (e.g., asserting that %some fact does
%\emph{not} hold because the fact does not exist in our model) or shipping a
%``final'' version of a relation to another location, and in \lang, such
%constructs can only be expressed with syntactically transparent nonmonotonic
%constructs.  

How do we maintain confluence in the presence of non-monotonicity?  We must
introduce {\em coordination} into our program---logic that preserves
commutativity even in the face of non-monotonicity.  One simple type of
coordination involves guarding the universal quantification with a data
dependency that cannot be fulfilled until the entire set quantified over has
been computed.

This notion of ``waiting for a data dependency to be satisfied'' is similar to
the waiting in centralized evaluation of a Datalog program.  First, some
dependency graph of the program or instance, such as a PDG, is analyzed to
identify and collapse {\em monotonic components}---subgraphs that contain no
negated edges.  The Datalog program is evaluated in the topological order of
its collapsed dependency graph.  Later monotonic components ``wait'' for
computation to be completed in earlier monotonic components.  

In \lang, we can leverage this same analysis to identify all {\em loci of
coordination}---all places where the programmer may need to instrument his code
with coordination to ensure confluence.  These are exactly the edges connecting
monotonic components.  Note that this analysis ignores the non-monotonicity in
\dedalus{choice} for the reasons outlined above.  Note also that we still
maintain the requirement that async relations are simply persisted to ensure
associativity. \wrm{talk about relaxing this?}

\wrm{write a paragraph extolling the virtues of logic programming here, and
explaining why this test is good.  that paragraph isn't coming to me now...} 

\paa{need a formal description of the new (or special-cased) stratification algorithm.
regular strat, except any async rule is a stratum boundary \emph{unless} the 
target predicate is naively persisted, with ``p is naively persisted'' conservatively defined
as $\exists$ a rule with $p$ in the head,  p in the body with 'the same bindings', and the rest of the body always satisfied or empty}

%Centralized evaluation of a logic program ensures confluence considering the
%program's PDG, and collapsing {\em monotonic components} -- subgraphs of the
%PDG with no negation -- and evaluating the program in topological order of the
%resulting graph. This ensures that the evaluator defers universal
%quantification over a subset of a relation until all facts in the subset are
%known.  This deferal happens precisely between monotonic components.  Note that
%we can ignore the negation in \dedalus{choice}, for the reasons described
%above.

%The utility of syntactic monotonicity checks lies in the ability to identify
%monotonic \emph{components} of a nonmonotonic program.  Stratification already
%divides a Datalog program into an ordered list of such monotonic components,
%separated by nonmonotonic constructs like negation.  Within each such
%component or \emph{stratum}, rules may be evaluated without ordering or
%coordination, but the strata must be evaluated in order and \emph{sealed}
%before downstream strata that depend nonmonotonically on them may begin
%evaluation.  

%Our use of choice in asynchronous rules uses (ambiguous) negation to represent
%indeterminacy in timestamp assignment by entailing a multiplicity of models,
%and hence on the surface always represents a stratum boundary in the global
%program.  Under this naive interpretation, communication always occurs
%between, and never within, monotonic program components: that is to say,
%communication always occurs at a coordination boundary, and it is not safe to
%process messages ``off the wire'' unless the computation that produced them is
%complete.  

%To avoid this undesirable barrier, and allow pipelining of the results of an
%ongoing computation across network (and hence temporal) boundaries, we propose
%an extension of stratification analysis that identifies when monotonic program
%components cross network boundaries. 

%\paa{the idea is simple: perform a normal stratification analysis, then
%collapse pairs of strata that are separated only b/c of an async rule (and
%it's implied NM choice expansion), when we can show that the async
%consequences are "protected" by a persistence rule.  the result is a new
%strata graph with fewer vertices, and some of whose vertices tolerate fully
%pipelined communication in addition to parallel/unordered evaluation.}

%In \lang, a distributed system is evaluated by many different nodes.  In a
%horizontally-partitoned distributed system, deferring quantification requires
%{\em coordination} between some subset of the nodes, as multiple nodes may provide
%for a subset to be quantified over.  \wrm{connect to coordination in imperative languages.}
%Within a given monotonic component (between universal quantifications), rules may be evaluated without ordering or
%coordination.  One neat upshot is that a Dedalus program can completely identify places in the program that
%might need coordination -- precisely those places in the program (PDG) with negation.  Note that we still need to persist async predicates.


%\paa{below are notes}

%when can we say that the ND choice of timestamp in async rules does not affect the outcome
%of a computation?  in the general case, it almost always does (since 'outcome' implies finality,
%and thus rules out the possibility of choosing a timestamp after the outcome that could affect the outcome.
%\wrm{huh?  the ND choice of timestamp doesn't affect the outcome if it ``doesn't affect the outcome'' (i.e. regardless of the choice value, i still get the same set of facts in the naively persistent relations, for all EDBs)  are you looking for some conservative syntactic condition?}

%we can reason, however, about order independence on a predicate-by-predicate basis, and this
%can significantly simplify how we reason about the replication/distribution of state in the
%global program.  informally, we say that any predicate all of whose possible derivations 
%do not transit across async rules is order-independent \wrm{a queue doesn't involve async, but it's order dependent}, but this is vacuous b/c those derivations
%are fundamentally unordered (well, order is hidden in the interpreter).  more substantively,
%we may say that by default, all event predicates (all predicates to a first approximation) 
%are order-dependent b/c the choice of timestamp determines all of their semantics.  
%can we restore order independence?  yes: persist the predicate.  now any tuple inserted will
%"be there at infinity" -- the only ordering detail of semantic significance is now the 1st 
%timestamp of persistence.  and even this matters only when program logic is nonmonotonic. \wrm{you also need that the inputs are either transitively dependent on only EDB, or are persisted, right?}

%a program with negation (aggregation, shipping, summarization) may decide at some time N
%that a predicate is false, and conclude stuff from it, only to have the predicate become
%true at some M > N \wrm{don't see a problem here}.  this means conclusions exists that are inconsistent with valuations at
%infinity \wrm{don't know what this means}.  we know precisely the circumstances under which this may occur:

%consider a predicate p in a dedalus program and the subgraph of the program's RGG that 
%defines the predicate.  p's semantics are order-insensitive if:

%1. in the global graph from EDB facts, timers etc, there are no stratum boundaries save those encapsulated in the choice expansion.

%2. following each async rule, there is a simple persistence rule.  (we can generalize this some,
%but not a heck of a lot.  a predicate p is 'basically' persistent of there exists a cycle in the RGG 
%with exactly one inductive edge and all other edge (possibly 0) deductive, in which the 
%bindings of p are preserved from p (as premise) to p (as conclusion), and in which either no other predicates participate, or in which we can show that any participating predicate is always true.

%we can see that these conditions imply order-insensitivity wrt ``in-flight'' tuples bound for p.
%any pair of tuples will end up in p and be there ``at some time'' if the network quiesces, at 
%which time we may consider the timestamp irrelevant.  and p is order-insensitive in this respect,
%though operations including join, till the next stratum boundary.

%but can we weaken the conditions?  in general it looks hard.  take (1): if we have indeed crossed a stratum boundary, we have in essence decided ``at some time'' that some predicate is closed to insertion, by universally quantifying over it (to assert its complement, summarize it, ship it, whatever).  recall p() from above.  a rule that negates p() in a subgoal could succeed between 
%two async insertions of p() tuples, which thus clearly do not commute.  the 'committed choice'
%pattern is a rudimentary illustration of 'sealing' a nonmonotonic consequence: log is defined 
%in terms of its own negation.  the 'assignment' pattern is a rudimentary example of... um, I guess a rule that isn't persistent (violates 2) b/c notin condition is clearly not "always true."  ditto the
%mutable state pattern.

%um, idempotence.
\wrm{idempotence}
um, efficient lazy evaluation?  \cite{magic}
