\section{Properties}

\subsection{Temporal Stratification}

In our earlier discussion of Datalog, we discussed the undesirability of a
logical contradiction -- a situation where a fact depends on its own negation.
We avoid this in \lang by introducing a property of programs called {\em
temporal stratification}, defined in terms of a program's {\em predicate
dependency graph} (PDG).  The PDG contains a node $N_i$ for each distinct
predicate $P_i$ used in the program, and an edge $(N_i, N_j)$ if $P_i$ appears
in the head of a rule with $P_j$ in its body.  The edge is annotated with a
$\lnot$ if at least one rule with head $P_i$ has $\lnot P_j$ in its body.  The
edge is annotated with a $+$ if all rules with head $P_i$ and $P_j$ in their
bodies are either inductive or async.  A \lang program is {\em temporally
stratifiable} if all loops in the PDG with an $\lnot$ edge also have a $+$
edge.

The intuitive meaning of a $+$ edge $(N_i, N_j)$ is that $P_j$ derives $P_i$ at
some later time.  The careful reader will note that so far, we have not
restricted asynchronous rules to prevent {\em time travel} -- a condition where
an asynchronous rule gives rise to derivations that precede their antecedents
in time.  While time travel is acceptable for programs where it cannot ever
cause a contradiction (such as programs without negation), it is undesirable in
general, as a fact may derive its own negation (intuitively, a ``temporal
paradox.'')  Thus, an additional condition is required to exclude such
contradictions.
%~\paa{in asynchronous programs}
However, we note that in a practical asynchronous distributed system, nodes are
assumed to have their own clocks, which may run at any arbitrary speed
\wrm{move this defnition of async distr system earlier}.  Nodes assign a
timestamp to an incoming fact that is greater than the timestamp of any
existing fact.  Note that this condition excludes time travel in any possible
execution.

\begin{example}
\label{ex:stratsafe}
A simple temporally stratifiable and temporally safe \slang instance that is
neither syntactically stratifiable nor safe.

\begin{Dedalus}
persist[p, 2]  
  
r1
p(A, B) \(\leftarrow\)
  insert\_p\_req(A, B);

r2  
delete p(A, B) \(\leftarrow\)
  p(A, B),
  del\_p\_req(A);

insert\_p(1, 2)@1;
\end{Dedalus}
\end{example}

In the \slang program in Example~\ref{ex:stratsafe}, \emph{insert\_p} and
\emph{delete\_p} are captured in EDB relations.  This reasonable program is
unstratifiable because $p \succ p\nega \land p\nega \succ p$.  But because the
successor relation is constrained such that $\forall A,B, successor(A, B)
\rightarrow B > A$, any such program is modularly stratified on
\emph{successor}.  Therefore, we have $p_{n} \not\succ^* p\_neg_{n} \not\succ^*
p_{n+1}$; informally, earlier values do not depend on later values.
\paa{shouldn't it be $p_{n} \succ^* p\_neg_{n} \not\succ^* p_{n+1}$ ?}

%Given this discussion, in practice we are interested in three asynchronous
%scenarios: (a) monotonic programs (even with non-monotonicity in time), (b)
%non-monotonic programs whose semantics guarantee monotonicity of time suffixes
%and (c) non-monotonic programs where we have domain knowledge guaranteeing
%monotonicity of time suffixes.  Each represents practical scenarios of
%interest.

%The first category captures the spirit of many simple distributed
%implementations that are built atop unreliable asynchronous substrates.  For
%example, in some Internet publishing applications (weblogs, online fora), it
%is possible due to caching or failure that a ``thread'' of discussion arrives
%out of order, with responses appearing before the comments they reference.  In
%many cases a monotonic ``bag semantics'' for the comment program is considered
%a reasonable interface for readers, and the ability to tolerate temporal
%anomalies simplifies the challenge of scaling a system through distribution.

%The second scenario is achieved in \slang via the use of \dedalus{successor}
%for the time suffix. The asynchronous rules of \lang require additional
%program logic to guarantee monotonic increases in time for predicates with
%dependencies.  In the literature of distributed computing, this is known as a
%{\em causal ordering} and is enforced by distributed clock protocols.  We
%review one classic protocol in the \lang context in Section~\ref{sec:lamport};
%including this protocol into \lang programs ensures temporal monotonicity.

%Finally, certain computational substrates guarantee monotonicity in both
%timestamps and message ordering---for example, some multiprocessor cache
%coherency protocols achieve this.  When temporal monotonicity is given, the
%proofs of temporal stratification and Algorithm~\ref{alg:tsn} both apply.
%\wrm{end paste}

%\dedalus{@async} is clearly more liberal than our operational semantics
%defined above.  One of our goals is formal verification of distributed
%systems.  Thus, for the particular use-case of distributed execution with the
%above operational semantics, we need to discover a logical condition that
%results in a set of models that exhibit the same set of behaviors.  It is
%necessary and sufficient for each fact to carry an entangled timestamp from
%each node.  Timestamps must be propagated through derivations \wrm{explain}.
%When a node receives a fact, it will need to delay processing of the fact
%until its current time is greater than its entangled timestamp.  Note that
%this scheme employs vector clocks.

%Note that the use of Lamport clocks enforces additional constraints by ruling
%out more possible interleavings of events than the operational semantics for
%timestamp assignment. \wrm{check if this is true} \paa{we'll put the lamport
%clock discussion here, then?}

%\subsection{Temporal Safety}

%\wrm{begin paste} In the previous section we demonstrated that \slang can
%capture intuitive notions of persistence and mutability of state via a
%stylized use of Datalog.  However, the alert reader will note that even very
%simple \slang programs make for unusual Datalog.
%%: among other concerns, 
%To begin with, Persistence rules are {\em unsafe}, meaning they produce
%derivations for an infinite number of values of the time suffix.  Traditional
%Datalog interpreters, which work against static databases, would attempt to
%enumerate these values, making this approach impractical.  Equally worrisome
%is the fact that many common patterns for state update via mutable persistence
%entail unstratifiable constructs: predicates that syntactically depend on
%their own (possibly transitive) negation.  

%However, in the context of distributed systems and networks, the need for
%non-terminating ``services'' or ``protocols'' that continually update their
%state is very common.  In this section we show that expressing distributed
%systems properties such as persistence and mutable state in logic does not
%require dispensing with familiar notions of safety and stratification: we take
%traditional notions of acceptable Datalog programs, and extend them in a way
%that admits sensible non-terminating programs.

%\subsection{Temporal Safety} Next we consider the issue of infinite results
%raised in the introduction to this section.  In traditional Datalog, this is a
%well-studied concern.  A Datalog program is considered {\em safe} if it has a
%finite minimal model, and hence has a finite execution.  Safety in Datalog is
%traditionally ensured through the following syntactic constraints:

%\begin{enumerate}
%%
%\item No functions are allowed.
%
%\item Variables are \emph{range restricted}: all attributes of the head goal
%appear in a non-negated body subgoal.
%
%\item The EDB is finite.
%
%\end{enumerate} \wrm{end paste}

\subsection{Temporal Safety}

Next we consider the issue of infinite results.  In Datalog, programs with
infinite results are traditionally ruled out, as Datalog's evaluation
algorithms fail to terminate on such programs.  Similarly, for \lang, we desire
to rule out programs that fail to terminate, given any bounded number of
messages.  We can apply a result of Chomicki et al. \wrm{cite} to prove that
using traditional Datalog-style safety restrictions (no functions, no
entanglement, finite EDB, range-restricted variables), we can always
efficiently compute a {\em partial fixpoint} of a \lang program.  A partial
fixpoint includes all {\em convergent} (forever persisted) derivations, and
excludes all {\em nonconvergent} (nonpersistent, or nontrivially periodic
derivations).  Other work has proposed the use of infinite objects to represent
nonconvergent derivations \wrm{cite chomicki}.  Additionally, \wrm{cite}
presents conservative syntactic conditions to guarantee the non-existence of
nontrivially periodic derivations in a program for all EDBs.

\subsection{Properties of Asynchronous Derivations}

Having ensured that there will always exist finite models of \lang programs,
given finite input, we now turn to a problem that is specific to the domain of
asynchronous distributed systems: the effects of delay, loss and reordering of
messages. We would like to characterize a class of asynchronous \lang programs
that are deterministic in the face of this non-determinism.  We clearly cannot
rely on equality of models over all time, as models of the same program with
different {\em async} rule applications will likely differ (at least) in the
timestamps of tuples.  However, we would like to say that these differences do
not matter, and define determinism based on the {\em eventual} equivalence of
models at some timestamp.

\begin{definition}
%Consider a stable model $M$ resulting from a fixpoint evaluation of a \lang
%program P over a finite input trace.  In other words, we may obtain the
%eventual model of a \lang execution from one of its stable models by {\em
%selecting} tuples corresponding to the ``end of time,'' and projecting out
%time.  If all stable models of a \lang program and EDB trace produce the same
%ultimate model, we say that the program and trace are \emph{trace confluent},
%and if a program is trace confluent for all EDBs we say that the program is
%\emph{confluent}.
The \emph{eventual model} of a \lang instance is the eventual contents of the
simply persisted relations (ignoring timestamps).
%
\end{definition}

Every safe \lang program has a finite eventual model, and there is a well-known
decision procedure for calculating the model \wrm{cite chomikci, partial
fixpoint stuff}. \wrm{In practice, due to magic sets, this isn't horribly
inefficient}~\cite{magic} If all traces \wrm{define trace} of a a \lang
instance produce the same ultimate model, then we call the instance {\em trace
confluent}.  If every instance of a program is trace confluent, then we say the
program is {\em confluent}.  Note that every non-contradictory Datalog program
without \dedalus{choice} is confluent.

\wrm{this para sounds a bit wishy washy.  but if we can say that ``assoc + comm
'' == deterministic, or point to a proof of this, then we can just say
``hey, comm isn't a problem here, so the only problem could be assoc,
and yes here's an exapmle where it's a problem.}

Since we do not consider the time suffix as part of the ultimate model, how can
non-determinism in time suffixes affect the ultimate model?  In purely
monotonic programs, the only issue that can arise is that some derivations may
not be {\em associative} -- the assignment of the same timestamp to two facts
may cause a different result than the assignment of a distinct timestamp to
each fact.  Intuitively, this is because we force unification on time suffixes
in the body.  Persistence of async relations solves this by making all
derivations associative: there will eventually be a timestamp where all async
derivations ever received are coincidentally true.
\wrm{provide example here?}

%We can immediately identify a subset of the class of confluent \lang programs:
%the class $D$ of monotonic \lang programs, where all predicates have simple
%persistence rules.  Intuitively, any instance of a program in $D$ is trace
%confluent because its ultimate model corresponds exactly to the model of the
%Datalog program obtained by converting all temporal and async rules into
%deductive rules and evaluating a single Datalog fixpoint.  Deductions via
%async rules introduce nondeterminism into the time suffixes of predicates.
%However, the simple persistence of all predicates ensures that for
%some value of the time suffixes, all deductions that can be made will be made.

It is easy to see that non-monotonic programs are not necessarily confluent,
even when async predicates are simply persisted.  The intuitive issue now is
{\em commutativity}: if we swap the timestamps of two messages, we might swap
the order of existential and universal quantification, as exhibited by the
instance below, where the 
%quantification over a set occurs before the membership of the set is known,
%and results of this ``incomplete'' quantification may be forever persisted.
nondeterminism implied by the async definitions of \dedalus{p} and \dedalus{q}
leads to two different ultimate models for this instance.  All traces in which
\dedalus{p} is assigned a lower timestamp than \dedalus{q} lead to an ultimate
model including \dedalus{r(1,2)}.  However, if \dedalus{q} precedes
\dedalus{p}, then \dedalus{\(\lnot\) q(A)} is false, and the ultimate model
excludes \dedalus{r(1, 2)}.

\begin{Dedalus}
persist[r,2]
persist[p,2]
persist[q,1]

r1
r(A, B) \(\leftarrow\) 
    p(A, B), \(\lnot\) q(A);

q(A)@async :- e(A);
p(A, B)@async :- f(A, B);

e(1)@10;
f(1, 2)@4;
f(3, 4)@11; 
\end{Dedalus} 

At this stage, the careful reader may be concerned that our ``monotonic''
programs permit async rules, as async rules use the \dedalus{choice} construct,
which makes use of negation.  However, it is easy to see that the universal
quantification in \dedalus{choice} happens over the \dedalus{time} set, which
is always fully defined.  In other words, universal quantification of
\dedalus{time} commutes with any other operation in the program.

%In either case, nonmontonic \wrm{need to define} reasoning (which is easily
%spotted in a logic program) entails applying the \emph{closed world
%assumption} \wrm{need to define} to a predicate which, if it transitively
%depends upon an async derivation, is never really closed \wrm{need to
%clarify}.

\subsubsection{Identifying Loci of Coordination}
The previous section provided a conservative test for confluence: all monotonic
programs with simply persisted asynchronous predicates are confluent.  In
practice, however, programs that monotonically accumulate information and never
perform some non-monotonic operation on it are rare in the distributed systems
domain.

%Non-monotonic operations in general imply universal quantification, such as
%aggregation (e.g., computing a sum) and negation (determining the
%non-existence of a fact).  Universal quantification allows the non-determinism
%of asynchronous systems to influence the ultimate model, because the universal
%quantification may occur before all messages have been received, and results
%of this ``incomplete'' quantification may be forever persisted.  Reduction in
%general implies universal quantification, whether it involves aggregation
%(e.g., computing a sum), negation (e.g., asserting that %some fact does
%\emph{not} hold because the fact does not exist in our model) or shipping a
%``final'' version of a relation to another location, and in \lang, such
%constructs can only be expressed with syntactically transparent nonmonotonic
%constructs.  

How do we maintain confluence in the presence of non-monotonicity?  We must
introduce {\em coordination} into our program -- logic that preserves
commutativity even in the face of non-monotonicity.  One simple type of
coordination involves guarding the universal quantification with a data
dependency that cannot be fulfilled until the entire set quantified over has
been computed.

This notion of ``waiting for a data dependency to be satisfied'' is similar to
the waiting in centralized evaluation of a Datalog program.  First, some
dependency graph of the program or instance, such as a PDG, is analyzed to
identify and collapse {\em monotonic components} -- subgraphs that contain no
negated edges.  The Datalog program is evaluated in the topological order of
its collapsed dependency graph.  Later monotonic components ``wait'' for
computation to be completed in earlier monotonic components.  

In \lang, we can leverage this same analysis to identify all {\em loci of
coordination} -- all places where the programmer may need to instrument his code
with coordination to ensure confluence.  These are exactly the edges connecting
monotonic components.  Note that this analysis ignores the non-monotonicity in
\dedalus{choice} for the reasons outlined above.  Note also that we still
maintain the requirement that async relations are simply persisted to ensure
associativity. \wrm{talk about relaxing this?}

\wrm{write a paragraph extolling the virtues of logic programming here, and
explaining why this test is good.  that paragraph isn't coming to me now...} 

%Centralized evaluation of a logic program ensures confluence considering the
%program's PDG, and collapsing {\em monotonic components} -- subgraphs of the
%PDG with no negation -- and evaluating the program in topological order of the
%resulting graph. This ensures that the evaluator defers universal
%quantification over a subset of a relation until all facts in the subset are
%known.  This deferal happens precisely between monotonic components.  Note that
%we can ignore the negation in \dedalus{choice}, for the reasons described
%above.

%The utility of syntactic monotonicity checks lies in the ability to identify
%monotonic \emph{components} of a nonmonotonic program.  Stratification already
%divides a Datalog program into an ordered list of such monotonic components,
%separated by nonmonotonic constructs like negation.  Within each such
%component or \emph{stratum}, rules may be evaluated without ordering or
%coordination, but the strata must be evaluated in order and \emph{sealed}
%before downstream strata that depend nonmonotonically on them may begin
%evaluation.  

%Our use of choice in asynchronous rules uses (ambiguous) negation to represent
%indeterminacy in timestamp assignment by entailing a multiplicity of models,
%and hence on the surface always represents a stratum boundary in the global
%program.  Under this naive interpretation, communication always occurs
%between, and never within, monotonic program components: that is to say,
%communication always occurs at a coordination boundary, and it is not safe to
%process messages ``off the wire'' unless the computation that produced them is
%complete.  

%To avoid this undesirable barrier, and allow pipelining of the results of an
%ongoing computation across network (and hence temporal) boundaries, we propose
%an extension of stratification analysis that identifies when monotonic program
%components cross network boundaries. 

%\paa{the idea is simple: perform a normal stratification analysis, then
%collapse pairs of strata that are separated only b/c of an async rule (and
%it's implied NM choice expansion), when we can show that the async
%consequences are "protected" by a persistence rule.  the result is a new
%strata graph with fewer vertices, and some of whose vertices tolerate fully
%pipelined communication in addition to parallel/unordered evaluation.}

%In \lang, a distributed system is evaluated by many different nodes.  In a
%horizontally-partitoned distributed system, deferring quantification requires
%{\em coordination} between some subset of the nodes, as multiple nodes may provide
%for a subset to be quantified over.  \wrm{connect to coordination in imperative languages.}
%Within a given monotonic component (between universal quantifications), rules may be evaluated without ordering or
%coordination.  One neat upshot is that a Dedalus program can completely identify places in the program that
%might need coordination -- precisely those places in the program (PDG) with negation.  Note that we still need to persist async predicates.


%\paa{below are notes}

%when can we say that the ND choice of timestamp in async rules does not affect the outcome
%of a computation?  in the general case, it almost always does (since 'outcome' implies finality,
%and thus rules out the possibility of choosing a timestamp after the outcome that could affect the outcome.
%\wrm{huh?  the ND choice of timestamp doesn't affect the outcome if it ``doesn't affect the outcome'' (i.e. regardless of the choice value, i still get the same set of facts in the naively persistent relations, for all EDBs)  are you looking for some conservative syntactic condition?}

%we can reason, however, about order independence on a predicate-by-predicate basis, and this
%can significantly simplify how we reason about the replication/distribution of state in the
%global program.  informally, we say that any predicate all of whose possible derivations 
%do not transit across async rules is order-independent \wrm{a queue doesn't involve async, but it's order dependent}, but this is vacuous b/c those derivations
%are fundamentally unordered (well, order is hidden in the interpreter).  more substantively,
%we may say that by default, all event predicates (all predicates to a first approximation) 
%are order-dependent b/c the choice of timestamp determines all of their semantics.  
%can we restore order independence?  yes: persist the predicate.  now any tuple inserted will
%"be there at infinity" -- the only ordering detail of semantic significance is now the 1st 
%timestamp of persistence.  and even this matters only when program logic is nonmonotonic. \wrm{you also need that the inputs are either transitively dependent on only EDB, or are persisted, right?}

%a program with negation (aggregation, shipping, summarization) may decide at some time N
%that a predicate is false, and conclude stuff from it, only to have the predicate become
%true at some M > N \wrm{don't see a problem here}.  this means conclusions exists that are inconsistent with valuations at
%infinity \wrm{don't know what this means}.  we know precisely the circumstances under which this may occur:

%consider a predicate p in a dedalus program and the subgraph of the program's RGG that 
%defines the predicate.  p's semantics are order-insensitive if:

%1. in the global graph from EDB facts, timers etc, there are no stratum boundaries save those encapsulated in the choice expansion.

%2. following each async rule, there is a simple persistence rule.  (we can generalize this some,
%but not a heck of a lot.  a predicate p is 'basically' persistent of there exists a cycle in the RGG 
%with exactly one inductive edge and all other edge (possibly 0) deductive, in which the 
%bindings of p are preserved from p (as premise) to p (as conclusion), and in which either no other predicates participate, or in which we can show that any participating predicate is always true.

%we can see that these conditions imply order-insensitivity wrt ``in-flight'' tuples bound for p.
%any pair of tuples will end up in p and be there ``at some time'' if the network quiesces, at 
%which time we may consider the timestamp irrelevant.  and p is order-insensitive in this respect,
%though operations including join, till the next stratum boundary.

%but can we weaken the conditions?  in general it looks hard.  take (1): if we have indeed crossed a stratum boundary, we have in essence decided ``at some time'' that some predicate is closed to insertion, by universally quantifying over it (to assert its complement, summarize it, ship it, whatever).  recall p() from above.  a rule that negates p() in a subgoal could succeed between 
%two async insertions of p() tuples, which thus clearly do not commute.  the 'committed choice'
%pattern is a rudimentary illustration of 'sealing' a nonmonotonic consequence: log is defined 
%in terms of its own negation.  the 'assignment' pattern is a rudimentary example of... um, I guess a rule that isn't persistent (violates 2) b/c notin condition is clearly not "always true."  ditto the
%mutable state pattern.

%um, idempotence.
\wrm{idempotence}
%um, efficient lazy evaluation?  \cite{magic}
