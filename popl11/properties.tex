\section{Properties}

blah blah blah

\subsection{Temporal Stratification}

In our earlier discussion of Datalog, we discussed the undesirability of a logical contradiction -- a situation where a fact depends on its own negation.  We avoid this in \lang by introducing a property of programs called {\em temporal stratification}, defined in terms of a program's {\em predicate dependency graph} (PDG).  The PDG contains a node $N_i$ for each distinct predicate $P_i$ used in the program, and an edge $(N_i, N_j)$ if $P_i$ appears in the head of a rule with $P_j$ in its body.  The edge is annotated with a $\lnot$ if at least one rule with head $P_i$ has $\lnot P_j$ in its body.  The edge is annotated with a $+$ if all rules with head $P_i$ and $P_j$ in their bodies are either inductive or async.  A \lang program is {\em temporally stratifiable} if all loops in the PDG with an $\lnot$ edge also have a $+$ edge.

The intuitive meaning of a $+$ edge $(N_i, N_j)$ is that $P_j$ derives $P_i$ at some later time.  The careful reader will note that so far, we have not restricted asynchronous rules to prevent {\em time travel} -- a condition where an asynchronous rule gives rise to derivations that precede their antecedents in time.  While time travel is acceptable for programs where it cannot ever cause a contradiction (such as programs without negation), it is undesirable in general, as a fact may derive its own negation (intuitively, a ``temporal paradox.'')  Thus, an additional condition is required to exclude such contradictions~\paa{in asynchronous programs}.  However, we note that in a practical asynchronous distributed system, nodes are assumed to have their own clocks, which may run at any arbitrary speed.  Nodes assign a timestamp to an incoming fact that is greater than the timestamp of any existing fact.  Note that \paa{in any realistic execution} this condition excludes time travel.

\begin{example}
\label{ex:stratsafe}
A simple temporally stratifiable and temporally safe \slang instance that is neither syntactically stratifiable nor safe.

\begin{Dedalus}
persist[p, 2]  
  
r1
p(A, B) \(\leftarrow\)
  insert\_p\_req(A, B);

r2  
delete p(A, B) \(\leftarrow\)
  p(A, B),
  del\_p\_req(A);

insert\_p(1, 2)@1;
\end{Dedalus}
\end{example}

In the \slang program in Example~\ref{ex:stratsafe}, 
\emph{insert\_p} and \emph{delete\_p} are captured
in EDB relations.  This reasonable program is unstratifiable because $p \succ
p\nega \land p\nega \succ p$.  But because the successor relation is
constrained such that $\forall A,B, successor(A, B) \rightarrow B > A$, any
such program is modularly stratified on \emph{successor}.  Therefore, we have
$p_{n} \not\succ^* p\_neg_{n} \not\succ^* p_{n+1}$; informally, earlier values
do not depend on later values.
\paa{shouldn't it be $p_{n} \succ^* p\_neg_{n} \not\succ^* p_{n+1}$ ?}

%Given this discussion, in practice we are interested in three asynchronous scenarios: (a) monotonic programs (even with non-monotonicity in time), (b) non-monotonic programs whose semantics guarantee monotonicity of time suffixes  and (c) non-monotonic programs where we have domain knowledge guaranteeing monotonicity of time suffixes.  Each represents practical scenarios of interest.

%The first category captures the spirit of many simple distributed implementations that are built atop unreliable asynchronous substrates.  For example, in some Internet publishing applications (weblogs, online fora), it is possible due to caching or failure that a ``thread'' of discussion arrives out of order, with responses appearing before the comments they reference.  In many cases a monotonic ``bag semantics'' for the comment program is considered a reasonable interface for readers, and the ability to tolerate temporal anomalies simplifies the challenge of scaling a system through distribution.

%The second scenario is achieved in \slang via the use of \dedalus{successor} for the time suffix. The asynchronous rules of \lang require additional program logic to guarantee monotonic increases in time for predicates with dependencies.  In the literature of distributed computing, this is known as a {\em causal ordering} and is enforced by distributed clock protocols.  We review one classic protocol in the \lang context in Section~\ref{sec:lamport}; including this protocol into \lang programs ensures temporal monotonicity.

%Finally, certain computational substrates guarantee monotonicity in both timestamps and message ordering---for example, some multiprocessor cache coherency protocols achieve this.  When temporal monotonicity is given, the proofs of temporal stratification and Algorithm~\ref{alg:tsn} both apply.
%\wrm{end paste}

%\dedalus{@async} is clearly more liberal than our operational semantics defined above.  One of our goals is formal verification of distributed systems.  Thus, for the particular use-case of distributed execution with the above operational semantics, we need to discover a logical condition that results in a set of models that exhibit the same set of behaviors.  It is  necessary and sufficient for each fact to carry an entangled timestamp from each node.  Timestamps must be propagated through derivations \wrm{explain}.  When a node receives a fact, it will need to delay processing of the fact until its current time is greater than its entangled timestamp.  Note that this scheme employs vector clocks.

%Note that the use of Lamport clocks enforces additional constraints by ruling out more possible interleavings of events than the operational semantics for timestamp assignment. \wrm{check if this is true}
%\paa{we'll put the lamport clock discussion here, then?}

\subsection{Temporal Safety}

%\wrm{begin paste}
%In the previous section we demonstrated that \slang can capture
%intuitive notions of persistence and mutability of state via a
%stylized use of Datalog.  However, the alert reader will note that
%even very simple \slang programs make for unusual Datalog.
%%: among other concerns, 
%To begin with, Persistence rules are {\em unsafe}, meaning they produce derivations for an infinite number
%of values of the time suffix.  Traditional Datalog interpreters, which
%work against static databases, would attempt to enumerate these
%values, making this approach impractical.
%Equally worrisome is the fact that many common patterns for state update via mutable
%persistence entail unstratifiable constructs: predicates that syntactically depend on their
%own (possibly transitive) negation.  

%However, in the context of distributed systems and networks, the need
%for non-terminating ``services'' or ``protocols'' that continually update their
%state
%is very common.  In this section we show that expressing distributed systems properties
%such as persistence and mutable state in logic does not require
%dispensing with familiar notions of safety and stratification: we take
%traditional notions of acceptable Datalog programs, and extend them in
%a way that admits sensible non-terminating programs.

%\subsection{Temporal Safety}
%Next we consider the issue of infinite results raised in the introduction to this section.
%In traditional Datalog, this is a well-studied concern.
%A Datalog program is considered {\em safe} if it has a finite minimal model, and hence has
%a finite execution.  Safety in Datalog is traditionally ensured
%through the following syntactic constraints:

%\begin{enumerate}
%%
%\item No functions are allowed.
%
%\item Variables are \emph{range restricted}: all attributes of the head goal
%appear in a non-negated body subgoal.
%
%\item The EDB is finite.
%
%\end{enumerate}
%\wrm{end paste}

\subsection{Temporal Safety}

Next we consider the issue of infinite results.  In Datalog, programs with infinite results are traditionally ruled out, as Datalog's evaluation algorithms fail to terminate on such programs.  Similarly, for \lang, we desire to rule out programs that fail to terminate, given any bounded number of messages.  We can apply a result of Chomicki et al. \wrm{cite} to prove that using traditional Datalog-style safety restrictions (no functions, no entanglement, finite EDB, range-restricted variables), we can always efficiently compute a {\em partial fixpoint} of a \lang program.  A partial fixpoint includes all {\em convergent} (forever persisted) derivations, and excludes all {\em nonconvergent} (nonpersistent, or nontrivially periodic derivations).  Other work has proposed the use of infinite objects to represent nonconvergent derivations \wrm{cite chomicki}.  Additionally, we \paa{alvaro et al ;)} presented conservative syntactic conditions in previous work to guarantee the non-existence of nontrivially periodic derivations in a program for all EDBs \wrm{cite tech report}.

\subsection{Properties of Asynchronous Derivations}

Having ensured, via variants on traditional Datalog techniques, that there will always exist 
stable models \wrm{if we use this word, remember to insert def earlier} of \lang programs that are free of contradictions and which will quiesce in the absence of input, we now turn to a problem that is specific to the domain of asynchronous
distributed systems: the effects of delay, loss and reordering of messages. We would like to
characterize a class of asynchronous \lang programs that have the same semantics regardless of
the nondeterminism in {\em async} rules.  We clearly cannot rely on naive equality of stable
models, as models of the same program with different {\em async} rule applications will differ
(at least) in the timestamps of tuples.

\paa{I don't know how to say this 'at infinity' stuff; I am back at max...}
\begin{definition}
Consider the stable model $M$ resulting from a fixpoint evaluation of a \lang program P over a finite input trace.  The \emph{ultimate model} $U$ of $M$ is the model obtained by selecting only the tuples of $M$ with the highest timestamp in $M$, dropping the timestamp attribute,
and removing duplicate tuples.
\end{definition}
\wrm{i think we want to say ``the maximum timestamp is the timestamp at which the program is forever quiescent'' or something}

In other words, we may obtain the ultimate model of a \lang execution from one of its stable
models by {\em selecting} tuples corresponding to the ``end of time,'' and projecting out time.
If all stable models of a \lang program and EDB trace produce the same ultimate model, we say that the program and trace are \emph{trace confluent}, and if a program is trace confluent for all EDBs we say that the program is \emph{confluent}.

We can immediately identify a subset of the class of confluent programs:
the class $D$ of monotonic (negation- and aggregation-free) \lang programs, all of 
whose predicates are defined with basic persistence rules.  Intuitively, any program in $D$
has a single ultimate model for any input trace, and this model corresponds exactly to the
minimal model of the Datalog program obtained by converting all {\em async} rules into deductive rules and evaluating a single Datalog fixpoint.  While deductions via {\em async} 
rules introduce nondeterminism into the time suffixes of predicates in any particular model, 
the inductive persistence of all predicates ensures that for some value of the time suffixes, 
all deductions that can be made will be made, and the program's monotonicity ensures that
no deductions will ever be retracted.   \paa{oh brother}

It is easy to see that nonmonotonic programs are not necessarily confluent even when all
predicates are persisted.  In the program shown below (which excludes for the sake of space 
basic persistence rules for all predicates), the nondeterminism implied by the {\em async}
definitions of {\em p} and {\em q} leads to different ultimate models for the same EDB
(valuations of {\em e} and {\em f}), precisely because {\em p} and {\em q} tuples do not
commute given that {\em q} appears in a negated subgoal.  All the orderings in which {\em p}
precedes {\em q} lead to models in which {\em r(1, 2)} holds, while in the other orderings the subgoal
$\lnot q(A)$ fails and produces models with {\em r(1, 2)} false.

\begin{Dedalus}
r(A, B) \(\leftarrow\) 
    p(A, B), \(\lnot\) q(A);

q(A)@async :- e(A);
p(A, B)@async :- f(A, B);

e(1)@10;
f(1, 2)@4;
f(3, 4)@11; 
\end{Dedalus} 

In either case, nonmontonic reasoning (which is easily spotted in a logic program)
entails applying the \emph{closed world assumption} to a predicate which, if it transitively 
depends upon an {\em async} derivation, is never really closed.

\subsubsection{monotonic components}

\paa{identification of monotonic components that cross network boundaries}

In practice, however, programs that monotonically accumulate information and never \emph{do} 
anything with it are rare in the distributed systems domain.  Summarization of state in 
general will imply universal quantification over that state, whether it involves aggregation (e.g., computing a sum), negation (e.g., asserting that some fact does \emph{not} hold because 
the fact does not exist in our model) or shipping a ``final'' version of a relation to another 
location, and in \lang, such constructs can only be expressed with syntactically transparent
nonmonotonic constructs.  

The utility of syntactic monotonicity checks lies in the ability to identify monotonic 
\emph{components} of a nonmonotonic program.


\paa{below are notes}

when can we say that the ND choice of timestamp in async rules does not affect the outcome
of a computation?  in the general case, it almost always does (since 'outcome' implies finality,
and thus rules out the possibility of choosing a timestamp after the outcome that could affect the outcome.
\wrm{huh?  the ND choice of timestamp doesn't affect the outcome if it ``doesn't affect the outcome'' (i.e. regardless of the choice value, i still get the same set of facts in the naively persistent relations, for all EDBs)  are you looking for some conservative syntactic condition?}

we can reason, however, about order independence on a predicate-by-predicate basis, and this
can significantly simplify how we reason about the replication/distribution of state in the
global program.  informally, we say that any predicate all of whose possible derivations 
do not transit across async rules is order-independent \wrm{a queue doesn't involve async, but it's order dependent}, but this is vacuous b/c those derivations
are fundamentally unordered (well, order is hidden in the interpreter).  more substantively,
we may say that by default, all event predicates (all predicates to a first approximation) 
are order-dependent b/c the choice of timestamp determines all of their semantics.  
can we restore order independence?  yes: persist the predicate.  now any tuple inserted will
"be there at infinity" -- the only ordering detail of semantic significance is now the 1st 
timestamp of persistence.  and even this matters only when program logic is nonmonotonic. \wrm{you also need that the inputs are either transitively dependent on only EDB, or are persisted, right?}

a program with negation (aggregation, shipping, summarization) may decide at some time N
that a predicate is false, and conclude stuff from it, only to have the predicate become
true at some M > N \wrm{don't see a problem here}.  this means conclusions exists that are inconsistent with valuations at
infinity \wrm{don't know what this means}.  we know precisely the circumstances under which this may occur:

consider a predicate p in a dedalus program and the subgraph of the program's RGG that 
defines the predicate.  p's semantics are order-insensitive if:

1. in the global graph from EDB facts, timers etc, there are no stratum boundaries save those encapsulated in the choice expansion.

2. following each async rule, there is a simple persistence rule.  (we can generalize this some,
but not a heck of a lot.  a predicate p is 'basically' persistent of there exists a cycle in the RGG 
with exactly one inductive edge and all other edge (possibly 0) deductive, in which the 
bindings of p are preserved from p (as premise) to p (as conclusion), and in which either no other predicates participate, or in which we can show that any participating predicate is always true.

we can see that these conditions imply order-insensitivity wrt ``in-flight'' tuples bound for p.
any pair of tuples will end up in p and be there ``at some time'' if the network quiesces, at 
which time we may consider the timestamp irrelevant.  and p is order-insensitive in this respect,
though operations including join, till the next stratum boundary.

but can we weaken the conditions?  in general it looks hard.  take (1): if we have indeed crossed a stratum boundary, we have in essence decided ``at some time'' that some predicate is closed to insertion, by universally quantifying over it (to assert its complement, summarize it, ship it, whatever).  recall p() from above.  a rule that negates p() in a subgoal could succeed between 
two async insertions of p() tuples, which thus clearly do not commute.  the 'committed choice'
pattern is a rudimentary illustration of 'sealing' a nonmonotonic consequence: log is defined 
in terms of its own negation.  the 'assignment' pattern is a rudimentary example of... um, I guess a rule that isn't persistent (violates 2) b/c notin condition is clearly not "always true."  ditto the
mutable state pattern.




um, idempotence.

um, efficient lazy evaluation?  \cite{magic}
