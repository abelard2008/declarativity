\section{Consistency and Logical \\ Monotonicity (CALM)}
In this section we present a strong connection between distributed consistency and logical monotonicity.  This discussion informs the language and analysis tools we develop in subsequent sections.

A key problem in distributed programming is reasoning about consistency in the face of {\em
temporal nondeterminism}: the delay and re-ordering of messages and data across
nodes.  Because delays can be unbounded, analysis typically focuses on ``eventual consistency'' after all messages have been delivered~\cite{vogels}.  A sufficient condition for eventual consistency is {\em order independence}: the independence of program execution from temporal
nondeterminism.

% \jmh{We need to pivot to a discussion of monotonicity and logic programming.  Trick: motivate declarative languages via disorderliness?  May be better to be more direct.}
% Programming models based on sets are attractive in this regard, because they keep programmers from making assumptions about the order of data arrival.  \nrc{Next sentence is confusing/vague to me.} But unordered inputs alone are not sufficient: there is still a question of handling delays as set contents stream (or stall) across a network. 
Order independence is a key attribute of declarative languages based on sets, which has led most notably to the success of parallel databases.  But even set-oriented languages can require a degree of ordering in their execution if they are sufficiently expressive.
The theory of relational databases and logic programming provides a framework to reason about these issues. \emph{Monotonic} programs---e.g., programs expressible via selection, projection and join---can be implemented by streaming algorithms that incrementally produce output elements as they receive input elements; the final order or contents of the input will never cause any earlier output to be ``revoked'' once it has been generated.\footnote{Formally, in a monotonic logic program any true statement continues to be true as new axioms---including new facts---are added to the program.}   
\emph{Non-monotonic} programs---e.g., those that contain aggregation or anti-join operations---can only be implemented correctly via blocking algorithms that do not produce any output until they have received all tuples in logical partitions of an input set. 
%%\nrc{The meaning of ``complete logical subset'' in the preceding sentence is unclear.} \jmh{switched to ``logical set partitions'', which is at least technically well-defined, though perhaps makes for vague english.  The example right after hopefully clears this up, no?}  
For example, aggregation queries need to receive entire ``groups'' before producing aggregates, which in general requires receiving the entire input set.

The implications for distributed programming and delayed messaging are clear. Monotonic programs are easy to distribute: they can be implemented via streaming set-based algorithms that produce actionable outputs to consumers while tolerating message reordering and delay from producers.  By contrast, even simple non-monotonic tasks like counting are difficult in distributed systems.  As a mnemonic, we say that \emph{counting requires waiting} in a distributed system: in general, a complete count of distributed data must wait for all its inputs, including stragglers, before producing the correct output.
% ; the alternative is to risk generating outputs that may be prove to be inconsistent with the full input once it arrives.    For example, an antijoin may output a row incorrectly if it does not wait for all the input records; similarly a SUM may be incorrectly reported as below a threshhold if only a subset of the records are summed.  

``Waiting'' is specified in a program via \emph{coordination logic}: code that (a) computes and transmits auxiliary information from producers to enable the recipient to determine when a set has completely arrived across the network, and (b) postpones production of results for consumers until after that determination is made.  Typical coordination mechanisms include sequence numbers, counters, and consensus protocols like Paxos or Two-Phase Commit.
% Our language, \emph{Bud}, omits coordination logic
% from non-monotonic operators by default, and forces the user to specify it.
% This is because badly-designed coordination logic can be a performance penalty,
% and this seems like a hard problem for an optimizer to approximate (an exact
% solution is undecidable).  In any case, this is the hard component of
% distributed systems programming, so a language should focus programmer effort
% on it.  
%Monotonic operators require no coordination logic.

Interestingly, these coordination mechanisms themselves typically involve counting.  For example, Paxos requires message counting to establish that a majority of the members have agreed to a proposal; Two-Phase Commit requires message counting to establish  that all members have agreed.  Hence we also say that \emph{waiting requires counting}, the converse of our earlier mnemonic.  

Our observations about waiting and counting illustrate the crux of what we call the \emph{CALM} principle: the tight relationship between Consistency And Logical Monotonicity.  Monotonic programs \emph{guarantee} eventual consistency under any interleaving of delivery and computation.  By contrast, non-monotonicity---the property that adding an element to an input set may revoke a previously-valid element of an output set---requires coordination schemes that ``wait'' until inputs can be guaranteed to be complete.  

Obviously we wish to minimize the use of coordination, because of well-known
concerns about latency and availability in the face of message delays during
coordination.  We can use the CALM principle to develop checks for distributed
consistency in logic languages, where conservative tests for monotonicity are
well understood. A simple syntactic check is often sufficient: if the program
does not contain any of the symbols in the language that correspond to
non-monotonic operators (e.g., \texttt{NOT IN} or aggregate symbols), then it is
monotonic and can be implemented without coordination, regardless of any
read-write dependencies in the code.  These conservative checks can be refined
further to consider semantics of predicates in the language. For example, the
expression ``\texttt{MIN(x) $< 100$}'' is monotonic despite containing an aggregate, by
virtue of the semantics of \texttt{MIN} and $<$: once a subset $S$ satisfies this
test, any superset of $S$ will also satisfy it.  Further refinements along
these lines exist, increasing the ability of program analyses to verify
monotonicity.
%More advanced whole-program analyses can take into account whether possibly
%non-monotonic conclusions can ever be consistent with other checks in the
%program; while undecidable in general, conservative tests with constraint
%propagation are often possible.  
% 
% It may also be useful to understand which portions of the program's execution
% or output may be affected by network non-determinism, and which are
% deterministic.  In this case, a static or runtime analysis could perform
% \emph{taint tracking} of network nondeterminism.  %Intuitively, a tuple is
% tainted if it is the transitive result of some %non-monotonic operator.

In cases where an analysis cannot guarantee monotonicity of a whole program, it can instead provide a conservative assessment of the points in the program where coordination may be required to ensure consistency.  For example, a shallow syntactic analysis could flag all non-monotonic predicates in a program (e.g., \texttt{NOT IN} tests or predicates with aggregate values as input).
We call the loci suggested by this analysis the program's \emph{points of
order}. A program with non-monotonicity can be made consistent by including coordination logic at its points of order.  

The reader may observe that because ``waiting requires counting,'' adding coordination logic actually increases the number of points of order in a program since it is non-monotonic.  To avoid this problem, the coordination logic must be hand-verified for consistency, after which annotations on the coordination logic can inform the analysis tool to (a) skip the coordination logic in its analysis, and (b) skip the point of order that the coordination logic handles.  

Because analyses based on the CALM principle operate with information about program semantics, they can avoid coordination logic in cases where traditional read/write analysis would require it.  Perhaps more importantly, as we will see in the next sections, logic languages and the analysis of points of order can help programmers redesign code to achieve goals of minimizing coordination.

%%\jmh{Should we say this: in the full paper, we will more directly show how CALM analysis relates to read/write analyses like serializability?}

% 
% In the general case, it is undecidable to verify whether they have done this
% correctly.  In fact, the introduction of coordination logic may involve
% additional non-monotonic operators (for example, two-phase commit has a
% non-monotonic "COUNT users = COUNT messages" statement).  There are a variety
% of possible solutions to this problem.  One option is for expert programmers
% (i.e. us) to encapsulate a suite of coordination protocols in a library, and
% manually verify the correctness of each protocol.  Another option is to develop
% various static analyses in the style of stratification tests for Datalog.
% \wrm{not sure how we can introduce additional detail here without pulling in
% tons of knowledge dependencies} 

% \wrm{addressed this} \jmh{At minimum, we should assert formal proof in one
% direction: purely Monotonic deduction can be done without coordination.  Start
% with explaining syntactic montonicity, perhaps via SQL, Datalog and MapReduce.
% Then enrich this by saying that single-node non-monotonicity is OK (if it's
% acyclic).} \wrm{i don't think it's okay if it's acyclic -- it's only okay if it
% does not depend on any network messages}  \jmh{  We should then enrich further
% by introducing the non-syntactic, ``instance-oriented'''' style monotonicity:
% saying that individual monotonic deductions---i.e. any monotonic tuple
% lineage---can be computed without coordination.  Give intuitive examples that
% are database instance-dependent (a la local stratification), and that are
% program-semantics dependent (a la universal constraint stratification).  The
% latter should do escrow transactions, and/or tee up shopping cart.}

% \wrm{addressed this} \jmh{Now you can talk at least in casual terms about a
% check for consistency: check the program for non-monotonicity.  Can probably do
% intuition of the data dependency here, and hint at how we'll do taint tracking
% later.}

% \wrm{not sure if this is true, also not sure what "universal-constraint-style
% analyses" you are referring to} \jmh{Now that we've said that ``counting
% requires waiting'', point out that ``waiting requires counting'': coordination
% protocols are basically threshhold tests on distributed count aggregates.
% Hence any check for consistency like the one above will flag the coordination
% logic as a problem.  Our solution: either expert programmers (us) verify the
% modules, or we develop universal-constraint-style analyses to validate them.}

% \wrm{i think we decided the other direction of CALM was false.  consider
% committed choice.  that's non-monotonic any way you look at it (except for the
% definition I tried to sell you durig the POPL submission that we ultimately
% rejected, because we believed monotonic should mean "has a monotonic
% representation in FOL")} \jmh{Finally, introduce the Bi-directional CALM
% Conjecture: that non-monotonic deduction (at the individual tuple level)
% requires coordination.  We can wave hands here about this requiring a crisper
% definition of coordination than we needed before.}

% \wrm{not sure what this means} \jmh{Occurs to me there's a disconnect here with
% the intro---in the intro we said that programming without coordination relates
% to ``loose'' consistency.  We should show a case where for the
% universal-constraint scenario, a read-write consistency analysis would flag
% this as bad, and a transaction manager would forbid it.}
