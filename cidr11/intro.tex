\section{Introduction}
Until fairly recently, distributed programming was a rarefied topic handled by expert programmers. But recent technology trends have brought distributed programming to the mainstream of software engineering.  The challenges of distribution---concurrency and asynchrony, performance variability, and partial failure---often translate into tricky data management challenges regarding task coordination and data consistency.
% nrc: Slagging old-timers seems both wrong and unnecessary. Distributed programming was hard, and still is.
%Software engineers dealing with these issues today have a much wider variety of backgrounds and sophistication than in previous decades, but the challenges involved remain largely unchanged.
Hence there is increased need to find ways to simplify the data management challenges inherent in distributed programming.

There are two main bodies of work to guide programmers through these issues.  The first is the ``ACID'' foundation provided by the theory and practice of concurrency control and distributed coordination, exemplified by serializable transactions and consensus protocols like Two-Phase Commit and Paxos.  These concepts are built on careful understanding and control over the ordering of abstract reads and writes of data objects.  On the positive front, mechanisms built from these concepts provide strong consistency guarantees, and are readily available in packaged solutions that shield the programmer from most of the complexities of coordination and consistency.  But there is a widespread belief---even among many seasoned practitioners~\cite{ladis}---that the costs of these mechanisms are too high in many important scenarios.  The interaction of these mechanisms with message delays and node failures often results in data being unavailable. This not only stalls jobs that depend on that data, it also produces transitive delays for other jobs via queueing effects that can be hard to contain.  As a result, there is a great deal of interest in building distributed software that makes minimal (or no) use of these mechanisms.

The second point of reference is a long tradition of research and system development that uses {\em logical} application-level reasoning to tolerate ``loose'' consistency of reads, writes and messages (e.g., \cite{sagas,base,acid20,quicksand}, etc.)  This approach enables machines to operate independently, and hence easily tolerate temporary delays, message reordering, and component failures.  Results include simplified code, improved responsiveness, and a restriction on the effects of delays and failures to those tasks that are directly accessing the unavailable resources.  The challenge with this approach is to ensure that the resulting software truly tolerates the inconsistencies, producing acceptable results in all cases.  Although there is a body of wisdom and best practices that informs this approach, there are few concrete tools to allow programmers to harness that wisdom during software development.  It is hard to know what guarantees are provided by systems built in this style, and hence the resulting code is hard to test and hard to trust.  
%\jmh{The following is useful somewhere, but I chopped it from here as  unnecessary.  It is generally bad software engineering practice to rely on programmer wisdom, which is hard to maintain as code evolves and teams shift over time.}

Merging the best of these traditions, it would be ideal to have a robust theory and practical tools based on higher-level program properties than those used for ACID, allowing developers to produce verifiably trustworthy code in the face of loosely consistent data.  In this paper we demonstrate significant progress in this direction via the use of a declarative language and a static analysis technique for code written in that language.  We begin by introducing the \emph{CALM Principle}, which makes a formal connection between the theory of monotonic logic and the need for distributed coordination to guarantee consistency.  Using an initial version of our {\em Bloom} declarative language, we translate this theory into a practical program analysis technique that detects potential consistency anomalies in distributed programs.  We then show how such anomalies can be handled by a programmer during development: either by introducing coordination mechanisms to ensure consistency, or by program rewrites that identify inconsistency ``taint'' as it propagates through code.  We demonstrate the use of Bloom and our analysis techniques on both ``loose'' and ``transactional'' implementations of a popular distributed systems example: a fault-tolerant replicated shopping cart.  We run our Bloom shopping cart code on Amazon EC2, and demonstrate the performance and consistency effects of the different design styles and the way that our analyses inform code evolution.

As a secondary issue, we submit our Bloom prototype and application examples as a case for the practicality of declarative logic-based programming.  We believe these ideas are ready to graduate from topics of academic interest into practical, general-purpose approaches that we hope can substantially improve the state of the art in distributed programming.