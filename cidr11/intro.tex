\section{Introduction}
Until fairly recently, distributed programming was the domain of a small group of experts. But recent technology trends have brought distributed programming to the mainstream of open source and commercial software.  The challenges of distribution---concurrency and asynchrony, performance variability, and partial failure---often translate into tricky data management challenges regarding task coordination and data consistency.
% nrc: Slagging old-timers seems both wrong and unnecessary. Distributed programming was hard, and still is.
%Software engineers dealing with these issues today have a much wider variety of backgrounds and sophistication than in previous decades, but the challenges involved remain largely unchanged.
Given the growing need to wrestle with these challenges, there is increasing pressure on the data management community to help find solutions to the difficulty of distributed programming.

There are two main bodies of work to guide programmers through these issues.  The first is the ``ACID'' foundation of distributed transactions, grounded in the theory of serializable read/write schedules and consensus protocols like Paxos and Two-Phase Commit.  These techniques provide strong consistency guarantees, and can help shield programmers from much of the complexity of distributed programming. However, there is a widespread belief that the costs of these mechanisms are too high in many important scenarios where availability and/or low-latency response is critical.
%  The interaction of these mechanisms with message delays and node failures often results in data being unavailable. This not only stalls jobs that depend on that data, it also produces transitive delays for other jobs via queueing effects that can be hard to contain.  
As a result, there is a great deal of interest in building distributed software that avoids using these mechanisms.

The second point of reference is a long tradition of research and system development that uses application-specific reasoning to tolerate ``loose'' consistency arising from flexible ordering of reads, writes and messages (e.g., \cite{sagas,beyond,quicksand,base,bayou}). This approach enables machines to continue operating in the face of temporary delays, message reordering, and component failures.  
% Results include simplified code, improved responsiveness, and a restriction on the effects of delays and failures to those tasks that are directly accessing the unavailable resources.  
The challenge with this design style is to ensure that the resulting software tolerates the inconsistencies in a meaningful way, producing acceptable results in all cases.  Although there is a body of wisdom and best practices that informs this approach, there are few concrete software development tools that codify these ideas.  Hence it is typically unclear what guarantees are provided by systems built in this style, and the resulting code is hard to test and hard to trust.  
%\jmh{The following is useful somewhere, but I chopped it from here as  unnecessary.  It is generally bad software engineering practice to rely on programmer wisdom, which is hard to maintain as code evolves and teams shift over time.}

Merging the best of these traditions, it would be ideal to have a robust theory
and practical tools to help programmers reason about and manage high-level
program properties in the face of loosely coordinated consistency.  In this
paper we demonstrate significant progress in this direction.  Our approach is
based on the use of a declarative language and program analysis techniques that
enable both static analyses and runtime annotations of consistency.  We begin by
introducing the \emph{CALM} principle, which connects
the theory of non-monotonic logic to the need for distributed coordination to
achieve consistency.  We present an initial version of our {\em Bloom}
declarative language, and translate concepts of monotonicity into a practical
program analysis technique that detects potential consistency anomalies in
distributed Bloom programs.  We then show how such anomalies can be handled by a
programmer during the development process, either by introducing coordination
mechanisms to ensure consistency or by applying program rewrites that can track
inconsistency ``taint'' as it propagates through code.  To illustrate the Bloom
language and the utility of our analysis, we present two case studies: a
replicated key-value store and a fault-tolerant shopping cart service.

%We run our Bloom shopping cart code on Amazon EC2, and demonstrate the performance and consistency effects of the different design styles and the way that our analyses inform code evolution.

%As a secondary issue, we submit our Bloom prototype and application examples as a case for the practicality of declarative logic-based programming.  We believe these ideas are ready to graduate from topics of academic interest into practical, general-purpose approaches that we hope can substantially improve the state of the art in distributed programming.