\section{Introduction}
Until fairly recently, distributed programming was the domain of a small group of experts. But recent technology trends have brought distributed programming to the mainstream of open source and commercial software.  The challenges of distribution---concurrency and asynchrony, performance variability, and partial failure---often translate into tricky data management challenges regarding task coordination and data consistency.
% nrc: Slagging old-timers seems both wrong and unnecessary. Distributed programming was hard, and still is.
%Software engineers dealing with these issues today have a much wider variety of backgrounds and sophistication than in previous decades, but the challenges involved remain largely unchanged.
Given the growing need to wrestle with these challenges, there is increasing pressure on the data management community to help find solutions to the difficulty of distributed programming

There are two main bodies of work to guide programmers through these issues.  The first is the ``ACID'' foundation of distributed transactions, grounded in the theory of serializable read/write schedules, and distributed consensus protocols like Two-Phase Commit and Paxos.  These concepts provide strong consistency guarantees, and have been reduced to practice in software and programming metaphors that shield the programmer from the complexities of coordination and consistency.  But there is a widespread belief---even among seasoned practitioners~\cite{ladis}---that the costs of these mechanisms are too high in many important scenarios where availability and/or low-latency response is critical.
%  The interaction of these mechanisms with message delays and node failures often results in data being unavailable. This not only stalls jobs that depend on that data, it also produces transitive delays for other jobs via queueing effects that can be hard to contain.  
As a result, there is a great deal of interest in building distributed software that makes minimal (or no) use of these mechanisms.

The second point of reference is a long tradition of research and system development that uses application-specific reasoning to tolerate ``loose'' consistency arising from flexible ordering of reads, writes and messages (e.g., \cite{sagas,base,acid20,quicksand}, etc.)  This approach enables machines to operate independently, and easily tolerate temporary delays, message reordering, and component failures.  
% Results include simplified code, improved responsiveness, and a restriction on the effects of delays and failures to those tasks that are directly accessing the unavailable resources.  
The challenge with this approach is to ensure that the resulting software tolerates the inconsistencies in a meaningful way, producing acceptable results in all cases.  Although there is a body of wisdom and best practices that informs this approach, there are few concrete software development tools that codify these ideas.  Hence it is typically unclear what guarantees are provided by systems built in this style, and the resulting code is hard to test and hard to trust.  
%\jmh{The following is useful somewhere, but I chopped it from here as  unnecessary.  It is generally bad software engineering practice to rely on programmer wisdom, which is hard to maintain as code evolves and teams shift over time.}

Merging the best of these traditions, it would be ideal to have a robust theory and practical tools to  help programmers reason about and manage high-level program properties in the face of loosely-coordinated consistency.  In this paper we demonstrate significant progress in this direction.  Our approach is based on the use of a declarative language, and program analysis techniques that enable both static analyses and runtime annotations of consistency.  We begin by introducing the \emph{CALM Principle}, which makes a formal connection between the theory of monotonic logic and the need for distributed coordination to guarantee consistency.  We present an initial version of our {\em Bloom} declarative language, and translate concepts of monotonicity into a practical program analysis technique that detects potential consistency anomalies in distributed Bloom programs.  We then show how such anomalies can be handled by a programmer during development: either by introducing coordination mechanisms to ensure consistency, or by enabling program rewrites that can track inconsistency ``taint'' as it propagates through code.  To illustrate the Bloom language and the utility of our analysis, we study two implementations of a popular example of distributed consistency: a fault-tolerant replicated shopping cart service.  
%We run our Bloom shopping cart code on Amazon EC2, and demonstrate the performance and consistency effects of the different design styles and the way that our analyses inform code evolution.

%As a secondary issue, we submit our Bloom prototype and application examples as a case for the practicality of declarative logic-based programming.  We believe these ideas are ready to graduate from topics of academic interest into practical, general-purpose approaches that we hope can substantially improve the state of the art in distributed programming.