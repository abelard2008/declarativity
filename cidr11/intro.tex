\section{Introduction}
\jmh{Don't forget: promise a fun live demo.}

Until fairly recently, distributed programming was a rarefied topic handled by systems experts building high-end commercial software. But recent technology trends have brought distributed programming into the mainstream of software engineering.  The inherent challenges of distribution---performance variability, failure management, and availability (among others)---often translate into tricky data management challenges regarding task coordination and data consistency.  Software engineers dealing with these issues today have a much wider variety of backgrounds and sophistication than in previous decades, but the challenges involved remain largely unchanged.  Hence there is increased need to find ways to simplify the data management challenges inherent in distributed programming.

There are two main bodies of work to guide programmers through these issues.  The first is the ``ACID'' foundation provided by the theory and practice of concurrency control and distributed coordination, exemplified by serializable transactions and consensus protocols like Two-Phase Commit and Paxos.  These concepts are built on careful understanding and control over the ordering of {\em physical} I/O: reads, writes, and messages.  On the positive front, mechanisms built from these concepts provide strong guarantees on data consistency, and are readily available in packaged solutions that shield the programmer from most of the complexities of coordination and consistency.  But there is a widespread belief---even among seasoned practitioners~\cite{ladis}---that the costs of these mechanisms are too high in many important scenarios.  The interaction of these mechanisms with message delays and node failures often results in data being unavailable. This not only stalls jobs that depend on that data, it also produces transitive delays for other jobs via queueing effects that can be hard to contain.  As a result, there is a great deal of interest in building distributed software that makes minimal (or no) use of these mechanisms.

The second point of reference is a long tradition of research and system development that uses {\em logical} application-level reasoning to tolerate ``loose'' consistency of reads, writes and messages (e.g., \cite{sagas,base,acid20,quicksand}, etc.)  This approach enables machines to operate independently, and hence easily tolerate temporary delays, message reordering, and component failures.  Results include simplified code, improved responsiveness, and a restriction on the effects of delays and failures to those tasks that are directly accessing the unavailable resources.  The challenge with this approach is to ensure that the resulting software truly tolerates the inconsistencies, producing acceptable results in all cases.  Although there is a body of accreted wisdom that inform this approach, there are few concrete tools to allow programmers to harness that wisdom during software development.  It is hard to know what guarantees are provided by systems built in this style, and hence the resulting code is hard to test and hard to trust.  
\jmh{The following is useful somewhere, but I chopped it from here as  unnecessary.  It is generally bad software engineering practice to rely on programmer wisdom, which is hard to maintain as code evolves and teams shift over time.}

\jmh{I need to inject more WOW into this paragraph.  We should be promising something major.}
What is needed is a theory and practice that addresses higher-level properties of programs than their I/O traces, allowing developers to produce trustworthy code in the face of loosely consistent I/O.  In this paper we demonstrate significant progress in this direction via the use of a declarative language, and whole-program analysis of code written in that languages.  We begin by introducing the CALM Conjecture, which makes a formal connection between the theory of monotonic logic and the need for distributed coordination to guarantee consistency.  Using an initial version of our {\em Bloom} declarative language, we translate this theory into a practical program analysis technique that detects potential consistency anomalies in distributed programs.  We then show how such anomalies can be handled by a programmer during development: either by introducing coordination mechanisms to ensure consistency, or by program rewrites that identify inconsistency ``taint'' as it propagates through code.  We demonstrate the use of Bloom and our analysis techniques on both ``loose'' and ``transactional'' implementations of a canonical distributed systems example: a fault-tolerant replicated shopping cart.  We run our Bloom shopping cart code on Amazon's EC2 cluster, and demonstrate the performance and consistency effects of the different design styles, and the way that our analyses inform code evolution.

As a secondary issue, we believe our Bloom prototype and application examples start to make the case that declarative logic-based programming can graduate from being a topic of academic interest, and be delivered as an attractive, general-purpose approach to substantially improve the state of the art in practical distributed programming.
