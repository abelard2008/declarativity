program bfs_heartbeat;

import bfs.Conf;
import java.lang.System;
import java.util.Set;

timer(clock, physical, 1000, infinity, 0);
timer(long_period, physical, 10000, infinity, 0);

define(sendHeartBeat, {String, String, Integer, Integer, Long, Long});

define(ackHeartBeat, {String, String, Long});
ackHeartBeat(@Dnode, Master, Id) :-
    sendHeartBeat(@Master, Dnode, _, _, _, Id),
    Id != -1L;

// XXX: Deletion from an aggregate table in JOL is currently not well-supported.
// Therefore, workaround the problem by storing the output of the agg in another
// table, and joining against/deleting from the other table. Note that we don't
// clean up state in the aggregate table when a DataNode times out, but that
// should only waste a negligible amount of space.
define(compute_datanode, keys(0, 1), {String, String, Long});
compute_datanode(Master, Addr, max<Tstamp>) :-
    sendHeartBeat(Master, Addr, _, _, Tstamp, _);

define(datanode, keys(0, 1), {String, String, Long});
datanode(Master, Addr, Ts) :-
    compute_datanode(Master, Addr, Ts);

delete
datanode(Master, DataNode, Ts) :-
    datanode(Master, DataNode, Ts),
    long_period#insert(),
    (System.currentTimeMillis() - Ts) > Conf.getHeartbeatRetention();

// Master node, Data node, ChunkId, Actual length of chunk, Timestamp
// of last update from data node
define(chunks, keys(0, 1, 2), {String, String, Integer, Integer, Long});
chunks(Master, DataNode, ChunkId, Length, MaxTstamp) :-
    ChunkId != -1,
    sendHeartBeat(Master, DataNode, ChunkId, Length, _, _),
    datanode(Master, DataNode, MaxTstamp);

/**************************/
// maintenance functions

define(rep_factor, keys(0,1), {String, Integer, Integer});
rep_factor(Master, ChunkId, count<DataNode>) :-
    chunks(Master, DataNode, ChunkId, _, _);

define(representative_datanode, keys(0,1), {String, Integer, String});
//watch(representative_datanode, ae);
representative_datanode(Master, ChunkId, max<DataNode>) :-
    chunks(Master, DataNode, ChunkId, _, _);

define(newReplicaRequest, keys(0,1,3), {String, Integer, Integer, Long});
watch(newReplicaRequest, ae);
newReplicaRequest(Master, ChunkId, Cnt, Time) :-
    rep_factor(Master, ChunkId, Cnt),
    Time := new Long(System.currentTimeMillis()),
    long_period#insert(_, _, _),
    Cnt < Conf.getRepFactor();

delete
newReplicaRequest(Master, ChunkId, Cnt1, Time) :-
    newReplicaRequest(Master, ChunkId, Cnt1, Time),
    clock#insert(),
    rep_factor(Master, ChunkId, Cnt2),
    Cnt2 >= Conf.getRepFactor();

define(newReplica, {String, Integer, String, Integer, Long});
newReplica(Master, ChunkId, Dn, Cnt, Time) :-
    newReplicaRequest(Master, ChunkId, Cnt, Time),
    representative_datanode(Master, ChunkId, Dn),
    clock#insert(),
    (System.currentTimeMillis() - Time) > Conf.getPropagationDelay();

define(catch_migrate, {String, String, Integer, Integer, Set});
watch(catch_migrate,ae);
watch(bfs_global::send_migrate, ae);
public
catch_migrate(@DnWithReplica, Master, ChunkId, CurrRepCnt, Dns) :-
	bfs_global::send_migrate(DnWithReplica, @Master, ChunkId, CurrRepCnt, Dns);
