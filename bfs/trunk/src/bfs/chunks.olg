program bfs_chunks;

import java.lang.System;
import java.util.Set;

define(candidate_datanode, keys(0), {String, Set});
public
candidate_datanode(Master, set<DataNode>) :-
    bfs_heartbeat::dataNodes(Master, DataNode, _);

// reuse the newchunk request dataflow to migrate bfs_heartbeat::chunks
// from one datanode to another
define(chunkMigration, keys(0,1,2), {String, String, Integer, Integer, Set});
watch(chunkMigration, ae);
public
chunkMigration(Master, DnWithReplica, ChunkId, CurrRepCnt, set<DataNode>) :-
    bfs_heartbeat::newReplica(Master, ChunkId, DnWithReplica, CurrRepCnt, _),
    bfs::migration_choices(Master, DataNode, ChunkId),
    bfs_heartbeat::representative_datanode(Master, ChunkId, DnWithReplica);

public
delete
bfs_heartbeat::newReplicaRequest(Master, ChunkId, DnWithReplica, CurrRepCnt, Time) :-
    //bfs_heartbeat::newReplicaRequest(Master, ChunkId, DnWithReplica, CurrRepCnt, Time),
    bfs_heartbeat::newReplica(Master, ChunkId, DnWithReplica, CurrRepCnt, Time),
    chunkMigration(Master, DnWithReplica, ChunkId, CurrRepCnt, _);

watch(send_migrate, ae);
send_migrate(@DnWithReplica, Master, ChunkId, CurrRepCnt, Dns) :-
  chunkMigration(@Master, DnWithReplica, ChunkId, CurrRepCnt, Dns);
