\documentclass{sig-alternate}
\usepackage{color}
\usepackage{times}
\newcommand{\jmh}[1]{{\textcolor{red}{#1 -- jmh}}}
\begin{document}
\conferenceinfo{SOSP'09}{, October 11--14, 2009, Big Sky, Montana, USA}
\CopyrightYear{2009}
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.

\newcommand{\BOOM} {Cumulus}
\newcommand{\JOL} {LogJam}

\title{\BOOM: Exploring Data-Centric Datacenter Development}
\numberofauthors{3}
\author{
\alignauthor
Foo
\alignauthor
Bar
\alignauthor
Baz
\and
UC Berkeley
}
\date{9 March 2009}

\maketitle
\begin{abstract}
\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{Delphi theory}
\keywords{Distributed computing, Datalog}

\section{Introduction}
% Cloud is the next platform
There is broad movement in the computing industry toward consolidation of  services into large datacenters.  This is happening under various banners, including Software as a Service (SaaS), Cloud Computing, and Service Oriented Architectures (SOA).  The terms have subtly different connotations, but the theme is clear: in the coming years, a wide variety of software will be hosted in datacenters.

% Cloud being seeded by legacy 
This transition is being accelerated by the maturation of virtual machine (VM) technologies~\cite{vmsurvey} and datacenter-scale storage systems~\cite{GFS,S3,SSDS}.  Given these building blocks, it has become relatively easy to replicate single-node software ``in the cloud'' on VM instances.  This is reflected in the programming models for emerging cloud computing platforms as well: Amazon's EC2 exposes ``raw'' VMs as their development environment; Google App Engine and Microsoft Azure provide programmers with traditional single-node programming languages, and APIs to distributed storage.

% Will need to go beyond legacy
These programming models are best viewed as transitional: they are ``virtualized legacy'' models, chosen for their maturity and familiarity rather than their unique fit to the datacenter environment.  Typically, new computing platforms lead to new languages and tools that give developers convenient access to the platform's unique features.  In the datacenter, the overriding new feature is distribution and parallelism: the power of multiple coordinated machines.  The programming models listed above offer little help in harnessing this power.  Developers looking to build distributed applications and infrastructure are still required to manage communication channels between pairs of machines, and develop protocols to coordinate computation in an application-specific manner.  This limits the population that can innovate on this new platform, and slows down their progress.

% MapReduce is an early indicator
One notable counter-example to this phenomenon is the MapReduce framework popularized by Google~\cite{mapreduce} and Hadoop~\cite{hadoop}.  MapReduce raises the programming abstraction from a traditional von Neumann model to a functional dataflow model that can be easily auto-parallelized over a shared-storage architecture.   Programmers think about handling sets of data records, rather than fine-grained threads, processes, communication and coordination.  
% Move SQL to related work, it's not necessary for motivation.
%MapReduce is similar in many ways to the shared-disk and shared-nothing architectures~\cite{stonebraker-sharedDB} prevalent in modern database systems, which also expose a high-level data-centric language to programmers.  Clearly neither MapReduce nor relational databases offer a general-purpose programming model for the cloud:  
% MapReduce is targeted specifically at batch processing, and database  transaction processing is too heavyweight for building general-purpose distributed programs.  But they do seem to hold the promise of a more attractive programming model for data centers, if they can be generalized and made more nimble.
MapReduce achieves its simplicity in part by constraining its usage to batch-processing tasks.  Although limited, it seems to hold the promise of a more attractive programming model for data centers, if it can be generalized and made more nimble.
% 
% 
% 
% 
% Datacenters and Cloud Computing are considered by many to be the next major computing platform.  Like mainframes, workstations, and personal computers before them, datacenters are likely to serve a wide variety of applications over time.  As a result, they demand a programming and runtime environment that is suited to the particulars of the platform, yet flexible enough to enable innovative, unforeseen applications.
% 
% To date, many of the programming platforms offered for datacenters have been extensions of familiar environments originally developed to run on individual PC-class machines.  This includes virtualized PCs with a LAN and SAN  (Amazon's EC2), and application-server shells running on multiple independent machines over a shared storage service (Google AppEngine, Microsoft Azure).
% These platforms reduce the management and deployment challenges involved in launching multiple instances of familiar single-node services at a hosting service.  But they do not assist the programmer in handling the fundamental new opportunity and challenge in programming datacenters: harnessing and coordinating large, varying numbers of unpredictable machines.  
% 
% One platform that does address these issues is the Hadoop MapReduce framework and its filesystem, HDFS.  Hadoop presents a programming model that exposes  the parallel power of a datacenter to programmers, while managing its unpredictability in a way that seems to be intuitive to most developers.  Hadoop achieves this goal by narrowly targeting data-parallel batch computing.  As such, it is not a broad, general-purpose programming model.  
% 
% A good litmus test for a language in a new runtime environment is whether one can implement said environment in the language.  C was developed for UNIX; UNIX is written in C.  A number of Java Virtual Machines have been written in Java, significantly accelerating the development of JVM technology. \jmh{Cite the Jikes RVM webpage, which claims dozens to hundreds of papers written.}  By contrast, none of the offerings mentioned above enables a programmer to easily implement a the infrastructure required to run a programmable datacenter.  In that sense these efforts do not answer the need for a new programming framework for datacenter computing.

\subsection{Data-Centric Programming and \BOOM}
Over the last year we have been exploring the use of a more general {\em data-centric} approach to datacenter programming. Reviewing some of the initial datacenter infrastructure efforts in the literature (e.g., \cite{mapreduce,gfs,chubby,dynamo}), it seemed to us that most of their non-trivial logic involves managing various forms of asynchronously-updated state -- sessions, protocols, storage -- rather than intricate, uninterrupted sequences of operations in memory.  We speculated that a high-level data-centric programming language would be well-suited to those tasks, and could significantly simplify the development of this class of infrastructure without introducing computational bottlenecks.  We set out to convince ourselves of this idea in a realistic setting.

% Choose a language
As starting points, we considered the practical lessons of MapReduce and SQL, and the declarative domain-specific languages that have emerged in an increasing variety of research communities in recent years (Section~\ref{sec:relwork}).  
%We were attracted to declarative languages -- i.e. languages based in logic -- because they are amenable not only to parallel dataflow implementations a la MapReduce, but also various guarantees via static code analysis.  
Among the proposals in the literature, we were most attracted to the Overlog language introduced in the P2 system~\cite{p2}.  Although we were not fond of its formal syntax, it had a number of advantages in our setting: it includes example code for network protocol specification, it had been shown to be useful for distributed coordination protocols~\cite{paxonp2}, and offered an elegant metaprogramming framework for developing tests for static program correctness and runtime invariant violations~\cite{evitaraced}.  Rather than counting on the initial P2 implementation, we developed our own Java-based Overlog runtime we call \JOL\ (Section~\ref{sec:jol}).

% Choose a target app
To evaluate the use of a data-centric language for the datacenter, we needed to choose a realistic distributed application to implement.  We chose to develop what we call {\em \BOOM}: an API-compliant, mostly declarative reimplementation of the Hadoop MapReduce engine and its HDFS distributed filesystem.  In writing \BOOM, we preserved the Java API ``skin'' of Hadoop and HDFS, but replaced their complex internals with Overlog.  The Hadoop stack appealed to us for two reasons.   First, unlike a farm of independent web services, the Hadoop and HDFS code entail non-trivial coordination among large numbers of nodes, including scheduling, coordination, data replication, and failover.  Second, Hadoop seemed to be a good litmus test: could we build a data-centric system for the datacenter out of a data-centric language?  This kind of metaprogramming challenge is common in the design of new programming languages (e.g. JVMs implemented in Java~\cite{JikesRVMJalapeno,joeq}). None of the current environments for data-center programming achieves this goal elegantly: you cannot easily implement Hadoop via MapReduce, nor Azure or Google Apps via their interfaces.  You could trivially recompile the Xen VM~\cite{xen} on EC2, but the implementation of a single VM within a single VM does not amount to the kind of distributed offering we desire.

\subsection{Contributions}
The bulk of this paper describes the details of implementing and evolving \BOOM, and running it on a sizeable cluster at a commercial web service.  Our experience implementing \BOOM\ in Overlog was gratifying both in its relative ease, and in the lessons learned along the way: lessons both in how to quickly prototype and evolve distributed infrastructure, and in understanding -- via the limitations of Overlog -- issues that may contribute to a better programming environment for datacenter programming.

This paper presents the evolution of \BOOM\ from a simple prototype to a very full-featured implementation.  We describe how an initial prototype went through a series of major revisions (``revs'') focused on {\em availability, scalability, performance, manageability}, and {\em security}.  In each case, the modifications involved were both simple and relatively isolated from the earlier revisions.  \jmh{Probably we want some mnemonic religious themes here that we carry through the paper. I.e.: ``In each section we reflect on the foo-ness and bar-ness associated with using a data-centric, declarative language.  Candidates include: separation of concerns via separation of data, unification of communication, transient and persistent state, unification of fine-grained monitoring, debugging and assertion checking via metaprogramming.  Also see ``Principles'' at\linebreak https://lincoln.declarativity.net/trac/wiki/LincolnContributions.''}


\subsection{Related Work}
\label{sec:relwork}
Declarative, data-centric languages are not new.  But until recently, they were thought to have limited applicability.  As recently as 2003, Butler Lampson noted -- in response to a challenge in Jim Gray's Turing Lecture -- that declarative languages can radically simplify programming, but have only done so in a few well-known data-centric settings: relational databases and spreadsheets. Still, within a few years of that article, data-centric languages began to appear in a wide variety of application settings in research, and were broadly popularized via the MapReduce paradigm. The renewed interest in declarative and dataflow programming suggests that Lampson's caveat may not be as restrictive as it had seemed at the time: many tasks appear to be data-centric.

\jmh{SQL is more general than mapreduce.}

In recent years, declarative data-centric languages have been proposed in a variety of research settings including overlay networks~\cite{p2}, three-tier web services~\cite{hilda}, natural language processing~\cite{dyna}, modular robotics~\cite{meld}, video games~\cite{cornellgames}, filesystem metadata analysis~\cite{wiscfsck}, and compiler analysis~\cite{bddbddb}.  The languages used in all these settings are data-centric in the sense that the programmer primarily focuses on batches of data: tables or streams of records, which in some of the cases described above are partitioned across machines in a network.  

Many of the languages described above are {\em declarative}, based on logic programming that specifies outcomes (``what''), not implementations (``how'').  Some -- notably MapReduce, but also SGL~\cite{cornellgames} -- are {\em algebraic} ({\em dataflow}) languages, focusing the programmer's attention on the composition and extension of a small set of operators that produce and consume bulk data types (sets or streams).  \jmh{The following is long and parenthetical, but probably scratches a typical OS person's itch regarding SQL vs. MapReduce, etc.}  Typically there is an equivalence between algebraic and declarative languages; Codd's proof of this equivalence in the relational context formed the foundation of relational databases~\cite{Codd70}. Not surprisingly then, it has been observed that dataflow languages can be optimized without requiring the programmer to think declaratively~\cite{volcano,cornellgames}, and there have been recent commercial integrations of MapReduce and SQL in relational databases~\cite{greenplum,aster}.  So declarative and algebraic dataflow languages are much more similar than different. By contrast, most traditional languages ask programmers to focus mostly on imperative threads of control specifying sequences of instructions, which typically run on a single machine and communicate with other threads via shared channels.

\jmh{LINQ and DryadLINQ.}

\jmh{Erlang.}

\jmh{Lamport's TLS and other largely theoretical logic-programming ideas.}

\jmh{Mention LBTrust w.r.t. metaprogramming and security.}

\section{Background: Overlog and \JOL}
The Overlog language is sketched in a variety of papers, most recently by Condie, et al.~\cite{evitaraced}  Originally an event-driven language with a logic syntax, it has evolved a more pure declarative core based in Datalog, the standard deductive query language from database theory~\cite{ullmanbook}.  We review it here with an eye toward explaining its integration with Java in \JOL.  

Basic Datalog operates over relational tables. A Datalog progra, consists of {\em rules} that can be roughly thought of as (potentially recursive) SQL {\em views}: i.e. named queries.  A simple Datalog rule has the form:
\[
	r_{\mbox{\em head}}(<\mbox{\em col-list}>) :- r_1(<\mbox{\em col-list}>), \ldots, r_n(<\mbox{\em col-list}>)
\]
Each term $r_i$ represents a relation -- i.e., a database table or a view.  The columns of the relation are listed a comma-separated list of variable names and/or scalar expressions; by convention, variables begin with capital letters.  The symbol ``$:-$'' connotes deduction; material to the right of the deduction symbol is called the {\em body} (corresponding to the {\tt FROM} and {\tt WHERE} clauses in SQL), the relation to the left is called the {\em head} (corresponding to the {\tt SELECT} clause in SQL).  Tables in the body are joined together, based on repeated variables in the col-lists of multiple relations: a tuple is deduced in the output only if it has matching values in the appropriate positions in the input.

\begin{figure*}[ht]
\begin{minipage}{0.5\linewidth}
\begin{small}
\begin{verbatim}
path(Start, End, Neighbor, Cost1+Cost2)
        :- link(Start, Neighbor, Cost1),
           path(Neighbor, End, Hop, Cost2).
\end{verbatim}
\end{small}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{small}
\begin{verbatim}
	WITH path(Start, NextHop, End, Cost) AS
	( 	 SELECT link.Start, path.End, 
	           link.Neighbor, link.Cost+path.Cost
	      FROM link, path
	     WHERE link.Neighbor = path.NextHop
	)
\end{verbatim}
\end{small}
\end{minipage}
\caption{Simple Datalog and SQL for computing paths as transitive closure of links.}
\label{fig:datalogsql}
\end{figure*}
For example, the canonical Datalog program for recursively computing paths from links is shown in Figure~\ref{fig:datalogsql}, along with corresponding SQL.  The SQL {\tt WITH} clause is used to allow {\tt path} to be defined recursively in terms of itself.  Note how the {\tt WHERE} clause corresponds to the repeated use of the variable {\tt Neighbor} in the Datalog.

Overlog is an extension of Datalog.
The Overlog data model consists of relational tables that are ``horizontally'' partitioned by rows across a set of machines based on some column called the {\em location specifier}.  By default, the location specifier column's data type is a network address type.  In \JOL\ we support an extensible set of network address types corresponding to different protocols, though for this paper we implemented TCP and UDP, and hence location specifiers are on IP address:port pairs.  \JOL\ also supports columns with data types corresponding to Java object classes, much like the abstract data types of Postgres~\cite{postgres}.

We find the semantics of the Overlog language easiest to understand ``inside out'', viewed from the perspective of a single node.  Overlog execution occurs in logical timesteps. Within a timestep, each node works on only the tuples stored locally; logically, communication occurs between timesteps. Each timestep consists of five phases, which correspond to an atomic execution of a Datalog program on any local database changes in that timestep.  In Phase 1, an {\em insertion queue} is inspected: each well-formed tuple in that queue specifies the name of a known table,  conforms to that table's schema, and has the local node's address in the location specifier field.  Tuples satisfying these constraints are inserted into the corresponding table\footnote{Non-conforming tuples are inserted into an ``exception'' table which can be configured either to store or drop tuples.}.  In Phase 2, the local database is considered immutable, and the Overlog programs installed in the system are treated as sets of purely declarative Datalog queries, which are run recursively on the database to a fixpoint (when the rules imply no further results)\footnote{For efficiency, \JOL\ uses a standard delta-computation approach, to compute only results that could have changed since the last timestep~\cite{ross-matviews}.}.  In Phase 3, the outputs of the queries in Phase 2 that have a local value in the location specifier field are stored in the local database (as so-called {\em Materialized Views}); outputs with remote location specifiers are batched up to be sent to their appropriate destination.  The exception to this processing are the results of Overlog queries prefaced by the {\tt delete} keyword; local {\tt delete} results are enqueued for Phase 4, whereas remote {\tt delete} results are marked as {\tt deletion} tuples before they are enqueued on the network.  In Phase 4, the local results of the {\tt delete} queries are processed to determine any derived tuples (previously computed in Phases 2 and 3 at any timestep) on which they depend recursively; then the tuples to be deleted {\em and all their local consequents} are deleted from the database; remote deletion consequents are also added to the network queue.  Finally, in Phase 5 the outbound network queues are flushed using the appropriate network protocols.   \jmh{Actually, Phase 1 has to deal with deletion tuples too, right?  Should they run first?}



The global semantics that follow depend on the program. Loo proved that programs without deletion, negation 


\jmh{Steal a quick intro from a previous paper.  Mention that it's evolved since SOSP 05 to be more truly declarative.  Brag briefly about metaprogramming of compiler (borrowing Evita Raced) and the actual scheduler and runtime.  Say we don't have space to describe further, but it speaks to the litmus test of the intro: \JOL\ is way smaller than P2.}
\section{A Naive Analytics Stack}
Simple implementations of a Map/Reduce workalike, running on top of a
GFS workalike~\cite{gfs-sosp}.

\section{Reliability}
\begin{itemize}
\item
  RCS Local durability primitive: Force persistent tables at end of
  timestamp.  (paxos 'just works' (?)) Need datalog rewrite for 2PC's
  prepare (ie: transactions in datalog rewrites).  2PC would be a
  forward ref to performance section if it is here.  Splitting 2PC
  into perf section might confuse durability story.

\item
  Replicated HDFS master nodes ($\to$ crash recovery; assuming we have
  enough elements in the log?)

\item
  Replicated Hadoop master

\end{itemize}

\section{Scalability}
\begin{itemize}
\item
  Partitioned HDFS master node
\end{itemize}

\section{Performance}
\begin{itemize}
\item
  Scheduler
\item
  Location-aware
\item
  RCS Reliable master vs master w/ hot failover vs 2PC vs Paxos.  (Could be in perf or reliability section, but choice depends on failure rate, workload, etc...)
\item
  Benchmark generation / monitoring
\end{itemize}

\section{Management / Debugging}
\begin{itemize}
\item
  Constraint / invariant enforcement
\item
  Monitoring
\item
  Logging
\item
  Auto-tuning based on management information (``autonomic'')
\end{itemize}

\section{Multi-user / Security}
\begin{itemize}
\item
  Access control and delegation
  \begin{itemize}
  \item
    Relation to info flow OS?
  \item
    How to handle external Java code? Maybe this isn't so hard/significant
  \end{itemize}
\item
  Audit trails
\item
  Data privacy?
\item
  Resource
\end{itemize}
\section{Conclusions}

\bibliographystyle{abbrv}
\bibliography{sosp}
\end{document}
