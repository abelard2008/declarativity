\section{Implementation}
\label{sec:impl}

In this section, we describe how \lang programs can be evaluated. First, we
detail a variant of semi-naive evaluation that supports lattices. We validate
that our implementation of semi-naive evaluation results in significant
performance gains and is competitive with a traditional set-oriented version of
semi-naive evaluation. We also detail the engineering effort required to extend
Bud to support \lang. % XXX: last sentence is weak

\subsection{Evaluation strategy}
\label{sec:lattice-eval-strat}
\emph{Naive} evaluation is a simple but inefficient approach to evaluating
recursive Datalog programs. Evaluation proceeds in ``rounds.'' In each round, all
the rules in the program are evaluated over the entire database (including all
derivations made in previous rounds). This process stops when a round makes no
new derivations. Naive evaluation is inefficient because it makes many redundant
derivations: once a fact has been derived in round $i$, it is rederived in every
subsequent round.

\nrc{NOT DONE!}
\emph{Semi-naive} evaluation improves upon naive evaluation by making fewer
redundant derivations~\cite{Balbin1987}. Let $\Delta_0$ represent the initial
database state. In the first round, all the rules are evaluated over $\Delta_0$;
let $\Delta_1$ represent the new facts derived in this round. In the second
round, we only need to compute derivations that are dependent on
$\Delta_1$---everything that can be derived purely from $\Delta_0$ has already
been computed.

A similar evaluation strategy works for \lang statements that invoke lattice
morphisms. For each lattice identifier $i$, we record two lattice elements:
$v_i$ and $\Delta^r_i$. $v_i$ is the least upper bound of all the lattice
elements ever produced by any statement with $i$ on the lhs---that is, it is the
current lattice element associated with $i$. $\Delta^r_i$ represents the new
derivations for $i$ that have been made in evaluation round $r$. During round
one, the program's statements are evaluated and $i$ is mapped to $v_i$; this
computes $\Delta^1_i$. In round two, $i$ is now mapped to $\Delta^1_i$ and
evaluating the program's statements yields $\Delta^2_i$. This process continues
until $\Delta^j_i = \Delta^{j+1}_i$ for all identifiers $i$.

This optimization can be used for morphisms, but not ordinary monotone
functions. This is because seminaive evaluation requires the ability to invoke a
method on two different lattice values $\Delta^j_i$ and $\Delta^k_i$

 relies on the ability to apply a
method to $\Delta^r_i$, and then later merge $\Delta^r_i$ with $v_i$ to obtain
the final value for $i$. This is safe for morphisms:

 During the first round, the program's statements are evaluated and
lattice identifiers are mapped to lattice elements in the normal way. At the end
of the round, each identifier also has a ``delta'' value that represents new
derivations made for statements with that identifier on the lhs. In round 2, the
lattice identifier is mapped to the ``delta'' value from round 1, rather than
the lattice identifiers are mapped to the delta element of the respective la

 over
the current value associated with each lattice element. Rather than merging rhs
values directly into the lhs lattice, 

A similar idea can be applied to \lang programs that apply morphisms to
lattices. However, semi-naive evaluation \emph{cannot} be used for \lang
statements that invoke monotone functions. This is because seminaive evaluation
requires the ability to split the input into pieces (``deltas''), and apply the
function to each piece, then merge together the output of the function over the
pieces. Morphisms allow this, but monotone functions do not. For example,
consider how we might compute the \texttt{size} monotone function over an
\texttt{lset} lattice.

% The basic insight is that if a fact is derived for the first time in round $i$,
% it must somehow depend on a fact that was derived for the first time in round
% $i-1$; otherwise, $f$ would have been derived earlier. Hence, we can rewrite the
% program to evaluate

\subsection{Performance validation}
\label{sec:lattice-perf}
\begin{figure}[t]
\includegraphics[width=\linewidth]{fig/sn_perf}
\caption{Performance comparison of three different methods for computing the
  transitive closure of a graph.}
\label{fig:tc-perf-graph}
\end{figure}

To validate the effectiveness of semi-naive evaluation for \lang programs, we
wrote two versions of a program to compute the transitive closure of a directed
acyclic graph. One version was written in Bloom and used traditional
set-oriented collections. The other version was written in \lang using morphisms
over the \texttt{lset} lattice. For the \lang version, we ran the program both
with and without semi-naive evaluation enabled. As input, we used synthetic
graphs of various sizes---in a graph with $n$ nodes, each node had $O(\log_2 n)$
outgoing edges. We ran the experiment on a late 2010 MacBook Air with a 2.13 Ghz
Intel Core 2 Duo processor and 4GB of RAM, running Mac OS X 10.7.3 and Ruby
1.8.7-p352. We ran each program variant five times on each graph and report the
mean elapsed wall-clock time.

Figure~\ref{fig:tc-perf-graph} shows how the runtime of each program varied with
the size of the graph. Note that we only report results for the naive \lang
strategy on small input sizes because this variant ran very slowly as the graph
size increased. The poor performance of naive evaluation is not surprising:
after deriving all paths of length $n$, naive evaluation will then rederive all
those paths at every subsequent ``step'' of the fixpoint computation. In
contrast, after computing length $n$ paths, a semi-naive strategy will only
generate length $n+1$ paths in the next step. Bloom and semi-naive \lang achieve
similar results. We instrumented Bud to count the number of derivations made by
the Bloom and semi-naive lattice variants---as expected, both programs made about
the same number of derivations. These results suggest that our implementation of
semi-naive evaluation for \lang is effective and performs comparably with a
traditional Datalog system.

For large inputs, Bloom began to outperform the semi-naive lattice variant. We
suspect this is because our current implementation of lattices requires more
data copies. Lattices are immutable, so the \texttt{lset} merge function
allocates a new object to hold the result of the merge. In contrast, Bloom
collections are modified in-place. We plan to address this by allowing the
lattice runtime to avoid copies when it can determine that in-place updates are
safe.

\subsection{Modifying Bud}
We were able to extend Bud to support \lang with relatively minor changes. Bud
initially had about 7200 lines of Ruby source code (LOC). The core lattice
features (the \texttt{Bud::Lattice} base class and the mapping from identifiers
to lattice elements) required about 300 LOC. Modifying Bud's fixpoint logic to
include lattices required only 10 LOC, while the program rewriting required to
enable semi-naive evaluation required 100 LOC. Modifying Bud's collection
classes to support merging of embedded lattice values required modifying about
125 LOC. The builtin lattice classes constituted an additional 300 LOC. In
total, adding support for \lang required less than 900 lines of added or
modified code, and took about two man-months of engineering time.%  This
% experience suggests that support for lattices can be added to an existing
% Datalog engine in a relatively straightforward manner.
