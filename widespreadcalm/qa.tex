%!TEX root = proposal.tex
\jmh{This reads fine, but it seems like it was all done already in the cited workshop paper.  Call out what remains to be done, and how hard/deep that work is.  More importantly, maybe we should expand scope here to also talk about debugging disorderly programs in addition to testing?  E.g. describe a suite of metaphors/facets (logical dataflow, physical communication a la Lamport diagrams, execution tracing/data provenance) and raise questions about how CALM connects to each, and how they fit together.}


Writing distributed software is difficult because developers must ensure that
their program behaves correctly in the face of network non-determinism---e.g.,
node failures, message reordering and duplication. Two techniques are commonly
used to address this difficulty in practical systems: \emph{verification} and
\emph{testing}.

Verification usually involves writing a formal specification that models the program's
behavior, and then systematically exploring the state space that can be reached
in any execution. Verification is a powerful approach that can find
difficult-to-reproduce errors, but it requires a high level of mathematical
sophistication, both to write the specifications and to interpret tool feedback.
Hence formal verification has seen limited practical adoption to date. In
contrast, \emph{testing} is widely used. Whereas formal verification typically
requires that the entire system be specified before providing feedback, testing
allows ``pay as you go'' quality assurance. A developer can write a small set of
tests to ``bootstrap'' a new piece of functionality, then incrementally add
tests to better cover the space of expected program inputs. Unfortunately,
testing is a poor fit for distributed software, because few testing tools allow
the developer to account for network non-determinism. To ensure that a module
produces the expected results for every possible network behavior, developers
typically need to build significant infrastructure to ``force'' the system to
follow a particular non-deterministic trace for a particular test case.

% \nrc{I don't understand the point of this paragraph or why it was located here.}
% CALM provides a solid foundation for program analysis---monotonic programs
% produce the same results in all executions---freeing programmers from reasoning
% about non-determinism in program executions.  Unfortunately, while languages like
% Bloom make it easy to construct (and ``bless'') purely monotonic
% \emph{components}, completely monotonic \emph{systems} are rare in practice.
% Non-monotonic distributed programs---as the CALM Theorem asserts---may exhibit
% inconsistencies (e.g., in the form of divergent replica state) if they are not
% properly \emph{coordinated} (e.g., by using a total order broadcast protocol to
% eliminate non-determinism in message delivery order).  While Bloom (inspired by
% the CALM Theorem) has static analysis capabilities that can pinpoint program
% locations where coordination should be interposed to ensure consistent outcomes,
% it offers little help confirming if a supplied coordination protocol is actually
% correct.

To help developers write reliable distributed software in Bloom, we are building
a testing framework called \emph{BloomUnit}~\cite{Alvaro2012}. By leveraging the
unique characteristics of Bloom, BloomUnit draws on ideas from both traditional
testing and formal verification. Some clear advantages to this approach follow:

\begin{itemize}
\item
  At its heart, Bloom is a query language.  If we view the execution of a module
  over some input as a database containing the trace of inputs and outputs to
  that module, we can recast the difficult problem of correctness specifications
  into the easier problem of querying a database. A BloomUnit assertion is
  simply a query that compares module inputs, outputs, and timing information.

\item
  Database systems use \emph{constraints} to characterize admissible database
  instances.  Inspired by recent work on lightweight formal
  methods~\cite{Jackson2012}, we are exploring using relational constraints to
  generate valid test inputs automatically.

\item
  Most importantly, the CALM Theorem implies that we can significantly reduce
  the enormous space of executions (corresponding to different message
  orderings) that must be searched to ensure that a distributed system satisfies
  its correctness requirements.  For monotonic modules, all orderings produce
  the same result, so we need only explore a single execution.  For hybrid
  modules, we need only explore message reorderings for racing messages that are
  bound for non-monotonic processing at the receiver.
\end{itemize}

When a distributed program produces an unexpected result---either in testing or in
deployment---tracking down the cause of the error can be very challenging.  Because
the execution was nondeterministic, it may be difficult to reproduce the error in a subsequent
run.  Moreover, because the program is distributed, it is likely that although we observed the 
error at one site, its cause lies at one or more other sites.  Surely the cause of the error
occurred \emph{before} the error itself, but even identifying the set of events that preceeded the 
error~\cite{timeclocks} is nontrivial!  Had we the foresight to log execution details at each node,
it could have simplified this search for causes.  Unfortunately, logging trades off between 
runtime overhead and detail, and it is difficult to know the appropriate ``level'' at which to
log events until bugs actually surface.  If we recorded highly detailed logs, it is likely
that the debugging information we seek is a needle in a haystack.

We believe that the CALM Theorem has important implications for distributed debugging.
Just as monotonicity analysis allowed us to prune the space of executions that needed to be
explored during quality assurance, similar analyses should dramatically reduce the number of events
that must be logged by each site.  A purely monotonic program is a deterministic function of its inputs;
to replay an execution of such a program we need only to have recorded its inputs.  To replay a nonmonotonic 
program, we must record its inputs and those nondeterministic events (i.e., message delivery orderings) 
that could have produced different outputs under different orderings.  

This event reduction has implications not just for logging efficiency but for log interpretation during
the debugging process.  Consider an interaction with a distributed version of the KVS previously presented.
At some point in the execution, the value associated with a particular key has become corrupted and we wish
to track down the cause of this stray PUT.  Unfortunately, the store has a read-dominant workload, and concurrent 
with the PUT in question are thousands of in-flight read requests.  It is infeasible to visualize this execution,
and difficult to reason about which message orderings actually matter with respect to the observed anomaly.
CALM analysis tell us that PUTs do not commute (due to the nonmonotonic operations they trigger), so each PUT
should be represented individually on the timeline.  On the other hand, GETs trigger monotonic processing, so
their ordering is immaterial as long as the log accurately represents which PUTs they follow.
Therefore for the key in question we may partition the set of GETs into two classes: those that precede the ``bad''
PUT and those that follow it.  From thousands of concrete events that occurred in the execution, we may log (and
display) only three.



\subsection{Summary of Tasks and Goals}

We are currently developing the BloomUnit system, a quality assurance tool
for programs written in Bloom, and a collection of debugging utilities.

\begin{itemize}
\item \textbf{Declarative assertions:}
BloomUnit users will write declarative test specifications that describe the 
intended input-output behavior of the program. These specifications will be 
written 
as Bloom queries over the (distributed) execution trace of the program; 
using Bloom avoids the need for users to learn another language. 
BloomUnit will allow ``pay as you go'' testing: a simple test specification 
is equivalent to a single test case, while a more complex test specification 
can encapsulate many different test cases.


\item \textbf{Constraint-guided input generation:}
Rather than supplying concrete inputs for test specifications, users will 
instead 
specify the constraints that the input must satisfy; BloomUnit will then 
automatically generate inputs that are consistent with those constraints.

\item \textbf{Semantics-aware state space exploration:}
  BloomUnit will systematically explore the space of possible network behaviors
  in an intelligent manner. To reduce the size of this space, we may again
  leverage the CALM Theorem.  If we can recognize that certain code fragments
  are monotonic, it follows that they are insensitive to message delivery order.
  Hence the evaluator may explore only those message delivery order permutations
  that could have produced different outputs.

\item \textbf{Dataflow debugging:}  Bloom already provides a graphical debugging tool that allows programmers
to visualize data as it moves through the (local) dataflow implied by a Bloom program.  To aid in debugging
executions that span multiple sites,  we will augment the logical dataflow ``view'' with a space / time visualization
(in the style of Lamport diagrams), in which sites are represented as timelines and messages connect timelines.
The debugging process might begin in the logical dataflow at the site where the unexpected output of behavior was observed.
As the search for causes widens, the programmer may ``zoom out'' to the space/time view and track the cause of the observed
event backwards in time to other sites.  To ensure that the programmer only visualizes (and hence reasons about) message orders
that could possibly have contributed to the observed result, we will use CALM analysis to collapse groups of messages that
could have been received in any order.  

\item \textbf{Evaluation:}  We plan to evaluate this work using a variety of quantitative approaches.  We can quantify directly the savings in state-space exploration and event reduction against techniques that do not take advantage of CALM analysis; the metrics here are the number of states or events needed to be explored in either case.  We are also considering performing user studies to assess the benefits of the tools we are building, as we have done in recent work on query specification~\cite{dataplay} and data cleaning~\cite{2011-wrangler}.
\end{itemize}

