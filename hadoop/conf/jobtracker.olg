program jobtracker;

import org.apache.hadoop.mapred.JobID;
import org.apache.hadoop.mapred.TaskID;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobPriority;
import org.apache.hadoop.mapred.JobConf;

import org.apache.hadoop.mapred.declarative.Constants;
import org.apache.hadoop.mapred.declarative.util.JobState;
import org.apache.hadoop.mapred.declarative.util.TaskState;
import org.apache.hadoop.mapred.declarative.util.Function;
import org.apache.hadoop.mapred.declarative.util.FileInput;
import org.apache.hadoop.mapred.TaskTrackerAction;

import jol.types.basic.ValueList;
import java.lang.Integer;
import java.lang.Long;
import java.lang.Enum;

/*************** Task Tracker Monitor *********************/

timer(checkTracker, physical, 60000, infinity, 60000);

taskTracker(TrackerName, Host, HttpPort, Constants.TaskTrackerState.FAILED, 
            Timestamp, Failures, MapCount, ReduceCount, MaxMap, MaxReduce) :-
	checkTracker(Period, TTL, Delay),
	taskTracker(TrackerName, Host, HttpPort, TrackerState, Timestamp, Failures, 
	            MapCount, ReduceCount, MaxMap, MaxReduce),
	TrackerState == Constants.TaskTrackerState.RUNNING,
	java.lang.System.currentTimeMillis() - Timestamp > Period;


/*************** TASK INIT *********************/

define(initJob,  {JobID, String, String, JobConf, String, String, JobPriority, Long, JobState});
define(initTask, {JobID, JobConf, String, TaskID, Constants.TaskType, Integer, FileInput, Integer, TaskState});
define(taskFile, {JobID, TaskID, ValueList});
define(taskFileLocation, keys(0,1), {JobID, TaskID, String});

job(JobID, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, Status) :-
	initJob(JobID, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, Status);

initTask(JobID, JobConf, JobFile, null, null, null, null, null, null) :-
	initJob(JobID, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, Status);
	
/* Break out of the current fixpoint thread (using 'async') since taskCreate table function
   makes blocking calls to the DFS. */
async initTask
task(JobID, TaskID, Type, Partition, Input, MapCount, Status) :-
	taskCreate(initTask(JobID, JobConf, JobFile, TaskID, Type, Partition, Input, MapCount, Status));
	
taskFile(JobId, TaskId, Locations) :-
	task(JobId, TaskId, Type, _, Input, _, Status),
	Status.state() == Constants.TaskState.UNASSIGNED,
	Type == Constants.TaskType.MAP
	{ Split := Input.split(); Locations := Function.getLocations(Split); };
	
taskFileLocation(JobId, TaskId, Location) :-
	flatten(taskFile(JobId, TaskId, Locations)),
	Location := (String) Locations;
	
	
/*************** Network Statistics  *********************/

define(distance, keys(0,1), {String, String, Integer});
define(networkDistance, keys(0,1), {String, String, Integer});

distance(Host, Host, 0) :-
	networkTopology(Host, Location, Parent, Level);
	
distance(Host, Location, 1) :-
	networkTopology(Host, Location, Parent, Level);
	
networkDistance(Location1, Location2, min<Distance>) :-
	distance(Location1, Location, Distance1),
	distance(Location2, Location, Distance2),
	Location1 != Location2,
	Distance := Distance1 + Distance2;
	

/*************** Job Status Maintenance *********************/
import jol.types.basic.ComparableSet;
	
define(mapTasks,    keys(0), {JobID, ComparableSet});
define(reduceTasks, keys(0), {JobID, ComparableSet});
define(updateJob, {JobID});
define(killjob,   {JobID, String});


/* Update the job state. */
updateJobState
job(JobId, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, Status) :-
	updateJob(JobId), mapTasks(JobId, Maps), reduceTasks(JobId, Reduces),
	job(JobId, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, _),
	Status := new JobState(JobId, Maps, Reduces);
	
/* Update the job state. */
updateJobStateNoReduce
job(JobId, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, Status) :-
	updateJob(JobId), mapTasks(JobId, Maps), notin reduceTasks(JobId, Reduces),
	job(JobId, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, _),
	Status := new JobState(JobId, Maps);
	
updateJob(JobId) :- mapTasks(JobId);
updateJob(JobId) :- reduceTasks(JobId);
	
mapTaskStatus
mapTasks(JobId, set<Status>) :-
	task(JobId, TaskId, Type, _, _, _, Status),
	Type == Constants.TaskType.MAP;
	
reduceTasks(JobId, set<Status>) :-
	task(JobId, TaskId, Type, _, _, _, Status),
	Type == Constants.TaskType.REDUCE;
	
public
killjob(JobId, TrackerName) :-
	job(JobId, _, _, _, _, _, _, _, JobStatus),
	taskAttempt(JobId, TaskID, _, _, TaskState, _, _, TrackerName, _, _, _),
	JobStatus.state()  == Constants.JobState.FAILED,
	TaskState == Constants.TaskState.RUNNING;
	
actionKillJob
taskTrackerAction(TrackerName, TaskTrackerAction.ActionType.KILL_JOB, Action) :-	
	killjob(JobId, TrackerName),
	Action := Function.killJob(JobId);
	
    
/*************** Task Status Maintenance *********************/

define(taskState, {JobID, TaskID, TaskState});

taskAttempt(JobID, TaskID, AttemptID, Progress, Constants.TaskState.FAILED, 
            Phase, Diag, TrackerName, FileLoc, Start, Finish) :-
	taskTracker(TrackerName, _, _, TrackerState, _, _, _, _, _, _),
	taskAttempt(JobID, TaskID, AttemptID, Progress, TaskState, Phase, Diag, TrackerName, FileLoc, Start, Finish),
	TrackerState != Constants.TaskTrackerState.RUNNING,
	TaskState == Constants.TaskState.RUNNING;
	
public
taskState(JobID, TaskID, generic<NewStatus.attempt(AttemptID, Progress, State, Phase, Start, Finish)>) :-
	taskAttempt(JobID, TaskID, AttemptID, Progress, State, Phase, _, _, _, Start, Finish),
	task(JobID, TaskID, Type, _, _, _, TaskStatus),
	TaskStatus.state() == Constants.TaskState.UNASSIGNED || 
	TaskStatus.state() == Constants.TaskState.FAILED || 
	TaskStatus.state() == Constants.TaskState.RUNNING,
	NewStatus := (TaskState) TaskStatus.clone();
	
task(JobID, TaskID, Type, Partition, Input, MapCount, TaskStatus) :-
	taskState(JobID, TaskID, TaskStatus),
	task(JobID, TaskID, Type, Partition, Input, MapCount, _);
	
	