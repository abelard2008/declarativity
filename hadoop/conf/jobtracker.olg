program hadoop;

import org.apache.hadoop.mapred.JobID;
import org.apache.hadoop.mapred.TaskID;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobPriority;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.TaskTracker;
import org.apache.hadoop.mapred.TaskAttemptID;

import org.apache.hadoop.mapred.declarative.Constants;
import org.apache.hadoop.mapred.declarative.util.JobState;
import org.apache.hadoop.mapred.declarative.util.TaskState;
import org.apache.hadoop.mapred.declarative.util.Function;
import org.apache.hadoop.mapred.TaskTrackerAction;
import org.apache.hadoop.mapred.KillTaskAction;

import java.lang.System;
import java.lang.Integer;
import java.lang.Long;
import java.lang.Enum;
import java.util.List;

watch(job, a);
watch(task, a);
watch(taskAttempt, a);

/*************** TaskTracker Utility ***********/
define(trackerUtility, keys(0), {String, String, float, float});

/*************** TaskTracker Handlers ***********/

public forwardAction
taskTrackerAction(@TaskTracker, JobTracker, ActionType, Action) :-
	taskTrackerAction(@JobTracker, TaskTracker, ActionType, Action);
	
define(jobTracker, {String, String, Constants.TrackerState});
jobTracker(@TaskTracker, JobTracker, Constants.TrackerState.RUNNING) :-
	taskTracker(@JobTracker, TaskTracker, _, _, Constants.TrackerState.INITIAL);

/*************** JobTracker State *********************/
define(activeJobs, keys(1), {Integer, String});
define(activeTasks, keys(1), {Integer, String});
define(failedTaskLocation, keys(0,1), {JobID, TaskID, String});
define(mapsWaitingForSchedule, keys(1), {Integer, String});
define(reducesWaitingForSchedule, keys(1), {Integer, String});

reducesWaitingForSchedule(count<TaskId>, "reduce count") :-
    task(JobId, TaskId, Constants.TaskType.REDUCE, _, _, _, TaskStatus),
    TaskStatus.state() == Constants.TaskState.UNASSIGNED ||
    TaskStatus.state() == Constants.TaskState.FAILED;

mapsWaitingForSchedule(count<TaskId>, "map count") :-
    task(JobId, TaskId, Constants.TaskType.MAP, _, _, _, TaskStatus),
    taskFileLocation(JobId, TaskId, Loc),
    TaskStatus.state() == Constants.TaskState.UNASSIGNED ||
    TaskStatus.state() == Constants.TaskState.FAILED;

activeJobs(count<JobId>, "active jobs") :-
	job(JobId, _, _, _, _, _, _, _, FinishTime, _),
	FinishTime == 0L;
	
activeTasks(count<TaskId>, "active tasks") :-
	task(JobId, TaskId, Type, Partition, Input, MapCount, Status),
	Status.state() == Constants.TaskState.RUNNING;

failedTaskLocation(JobId, TaskId, TaskTracker) :-
	hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, _, Constants.TaskState.FAILED)
	{
		JobId  := AttemptId.getJobID();
		TaskId := AttemptId.getTaskID();
	};

/*************** Clean up *********************/

define(cleanup, {JobID, JobState});

cleanup(JobId, JStatus.state()) :-
	job(JobId, _, _, _, _, _, _, _, _, JStatus),
	JStatus.state() == Constants.JobState.SUCCEEDED || 
	JStatus.state() == Constants.JobState.KILLED    || 
	JStatus.state() == Constants.JobState.FAILED;
	
delete
task(JobId, TaskId, Type, Partition, Input, MapCount, Status) :-
	cleanup(JobId),
	task(JobId, TaskId, Type, Partition, Input, MapCount, Status);

delete
hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, Progress, State, Phase, TaskFileLoc, Start, Finish) :-
	cleanup(JobId),
	hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, Progress, State, Phase, TaskFileLoc, Start, Finish),
	AttemptId.getJobID() == JobId;
    
delete
taskFileLocation(JobId, TaskId, Loc) :-
	cleanup(JobId), taskFileLocation(JobId, TaskId, Loc);
	

/*** Inform trackers that job is done ***/
define(forwardTracker, {String, String, JobID, JobState});
forwardTracker(JobTracker, TaskTracker, JobId, State) :-
	cleanup(JobId, State),
    taskAttempt(JobTracker, TaskTracker, AttemptId, _, _, _, _, _, _),
    AttemptId.getJobID() == JobId;
    
jobCompletion(@TaskTracker, JobId, State) :-
	forwardTracker(@JobTracker, TaskTracker, JobId, State);

/*************** TASK INIT *********************/

define(taskFile, {JobID, TaskID, List});
define(taskFileLocation, keys(0,1), {JobID, TaskID, String});

taskFile(JobId, TaskId, Locations) :-
	task(JobId, TaskId, Type, _, Input, _, Status),
	Status.state() == Constants.TaskState.UNASSIGNED,
	Type == Constants.TaskType.MAP
	{ Locations := Function.getLocations(Input); };
	
	
taskFileLocation(JobId, TaskId, Location) :-
	flatten(taskFile(JobId, TaskId, Locations)),
	Location := (String) Locations;
	
/*************** Job Status Maintenance *********************/
import java.util.Set;
	
/* Update the job state. */
public updateJobState
job(JobId, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, FinishTime, JobStatus) :-
	taskUpdate(JobId, TaskId, Type, TaskStatus),
	job(JobId, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, _, JobStatus),
	JobStatus.task(Type, TaskStatus)
	{
		FinishTime := JobStatus.state() == Constants.JobState.SUCCEEDED ? 
		              java.lang.System.currentTimeMillis() : 0L;
	};
	
define(killjob, {JobID});
public killJob
job(JobId, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, FinishTime, JobStatus) :-
	killjob(JobId), job(JobId, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, _, JobStatus)
	{
		FinishTime := java.lang.System.currentTimeMillis();
		JobStatus.killjob();
	};
	

public
hadoop::taskTrackerAction(JobTracker, TaskTracker, TaskTrackerAction.ActionType.KILL_TASK, Action) :-	
	job(JobId, _, _, _, _, _, _, _, _, JobStatus), 
	JobStatus.state() == Constants.JobState.KILLED || 
	JobStatus.state() == Constants.JobState.FAILED,
	hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, _, Constants.TaskState.RUNNING, _, _, _, _)
    {
        Action := new KillTaskAction(AttemptId);
    };
	
    
/*************** Task Status Maintenance *********************/

define(taskUpdate, {JobID, TaskID, Constants.TaskType, TaskState});

public
task(JobId, TaskId, Type, Partition, Input, MapCount, Status) :-
    taskUpdate(JobId, TaskId, _, Status),
    task(JobId, TaskId, Type, Partition, Input, MapCount, _);
	
/*************** CLEAN UP TASK STATE ******/
delete
hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, Progress, State, Phase, TaskFileLoc, Start, Finish) :-
    cleanup(JobId),
    hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, Progress, State, Phase, TaskFileLoc, Start, Finish),
    AttemptId.getJobID() == JobId;

delete 
mapCompletion(JT, MapId, TT) :-
    cleanup(JobId),
    mapCompletion(JT, MapId, TT),
    MapId.getJobID() == JobId;

delete
sendCompletion(JobTracker, TaskTracker, MapId, TaskFileLoc) :-
    cleanup(JobId),
    sendCompletion(JobTracker, TaskTracker, MapId, TaskFileLoc),
    MapId.getJobID() == JobId;

/************** SEND REDUCE SCHEDULE EVENTS TO MAPPERS ****************/
watch(reduceSchedule, a);

public
reduceSchedule(@TaskTracker, ReduceId, Partition, Address) :-
    hadoop::taskAttempt#insert(@JobTracker, TaskTracker, AttemptId, _, State, Phase, _, _, _),
    reduceSchedule(@JobTracker, ReduceId, Partition, Address),
    AttemptId.isMap(), Phase == Constants.TaskPhase.STARTING;
    
public
reduceSchedule(@TaskTracker, ReduceId, Partition, Address) :-
    hadoop::taskAttempt(@JobTracker, TaskTracker, AttemptId, _, State, Phase, _, _, _),
    reduceSchedule#insert(@JobTracker, ReduceId, Partition, Address),
    Phase == Constants.TaskPhase.MAP;


/************** SEND MAP TASK ATTEMPT STATUS TO REDUCER TASKTRACKERS ****************/
/* timer(completionCheck, physical, 5000, infinity, 0); */
define(mapCompletion, keys(1), {String, TaskAttemptID, String});
define(sendCompletion, keys(1,2), {String, String, TaskAttemptID, String});
define(feedReduce, keys(1,2), {String, String, TaskAttemptID});

feedReduce(JobTracker, TaskTracker, ReduceId) :-
   hadoop::taskAttempt#insert(JobTracker, TaskTracker, ReduceId);

public
mapCompletion(JobTracker, MapId, TaskFileLoc) :-
    hadoop::taskAttempt#insert(JobTracker, _, MapId, _, State, Phase, TaskFileLoc, _, _),
    State == Constants.TaskState.SUCCEEDED, Phase == Constants.TaskPhase.MAP;

public
sendCompletion(JobTracker, TaskTracker, MapId, TaskFileLoc) :-
   mapCompletion(JobTracker, MapId, TaskFileLoc),
   feedReduce(JobTracker, TaskTracker, ReduceId),
   !ReduceId.isMap(), MapId.getJobID() == ReduceId.getJobID();

/* Tell reducers where the succeeded maps reside */
hadoop::taskAttempt(@TaskTracker, JobTracker, MapId, 1.0f, Constants.TaskState.SUCCEEDED, Constants.TaskPhase.MAP, TaskFileLoc, 0, 0) :-
        sendCompletion#insert(@JobTracker, TaskTracker, MapId, TaskFileLoc);

/************** GENERATE TASK UPDATES BASED ON UPDATES TO TASK ATTEMPT STATE ********/

public
taskUpdate(JobId, TaskId, Type, StatusUpdate) :-
	hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, Progress, State, Phase, TaskFileLoc, Start, Finish),
	task(JobId, TaskId, Type, _, _, _, TaskStatus),
	TaskId == AttemptId.getTaskID(), TaskStatus.state() != Constants.TaskState.SUCCEEDED 
	{
		StatusUpdate := TaskStatus.attempt(AttemptId, Progress, State, Phase, Start, Finish);
	};
	
	
