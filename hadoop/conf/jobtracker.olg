program hadoop;

import org.apache.hadoop.mapred.JobID;
import org.apache.hadoop.mapred.TaskID;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobPriority;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.TaskTracker;

import org.apache.hadoop.mapred.declarative.Constants;
import org.apache.hadoop.mapred.declarative.util.JobState;
import org.apache.hadoop.mapred.declarative.util.TaskState;
import org.apache.hadoop.mapred.declarative.util.Function;
import org.apache.hadoop.mapred.declarative.util.FileInput;
import org.apache.hadoop.mapred.TaskTrackerAction;
import org.apache.hadoop.mapred.KillTaskAction;

import java.lang.System;
import java.lang.Integer;
import java.lang.Long;
import java.lang.Enum;
import java.util.List;

watch(job, a);
watch(task, a);
watch(taskAttempt, a);
watch(taskTrackerAction, s);

/*************** TaskTracker Handlers ***********/

public forwardAction
taskTrackerAction(@TaskTracker, JobTracker, ActionType, Action) :-
	taskTrackerAction(@JobTracker, TaskTracker, ActionType, Action);
	
watch(jobTracker, s);
define(jobTracker, {String, String, Constants.TrackerState});
jobTracker(@TaskTracker, JobTracker, Constants.TrackerState.RUNNING) :-
	taskTracker(@JobTracker, TaskTracker, _, _, Constants.TrackerState.INITIAL);

/*************** JobTracker State *********************/
define(activeJobs, keys(), {Integer});
define(activeTasks, keys(), {Integer});
define(failedTaskLocation, keys(0,1), {JobID, TaskID, String});
define(tasksWaitingForSchedule, keys(0), {Integer});

watch(failedTaskLocation, a);
watch(tasksWaitingForSchedule, a);

tasksWaitingForSchedule(count<TaskId>) :-
    task(JobId, TaskId, _, _, _, _, TaskStatus),
    TaskStatus.state() == Constants.TaskState.UNASSIGNED ||
    TaskStatus.state() == Constants.TaskState.FAILED;

activeJobs(count<JobId>) :-
	job(JobId, _, _, _, _, _, _, _, FinishTime, _),
	FinishTime == 0L;
	
activeTasks(count<TaskId>) :-
	task(JobId, TaskId, Type, Partition, Input, MapCount, Status),
	Status.state() == Constants.TaskState.RUNNING;

failedTaskLocation(JobId, TaskId, TaskTracker) :-
	hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, _, Constants.TaskState.FAILED)
	{
		JobId  := AttemptId.getJobID();
		TaskId := AttemptId.getTaskID();
	};

/*************** Clean up *********************/

define(cleanup, {JobID, JobState});

cleanup(JobId, JStatus.state()) :-
	job(JobId, _, _, _, _, _, _, _, _, JStatus),
	JStatus.state() == Constants.JobState.SUCCEEDED || 
	JStatus.state() == Constants.JobState.FAILED;
	
delete
task(JobId, TaskId, Type, Partition, Input, MapCount, Status) :-
	cleanup(JobId),
	task(JobId, TaskId, Type, Partition, Input, MapCount, Status);

delete
hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, Progress, State, Phase, TaskFileLoc, Start, Finish) :-
	cleanup(JobId),
	hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, Progress, State, Phase, TaskFileLoc, Start, Finish),
	AttemptId.getJobID() == JobId;
    
delete
taskFileLocation(JobId, TaskId, Loc) :-
	cleanup(JobId), taskFileLocation(JobId, TaskId, Loc);

/*** Inform trackers that job is done ***/
define(forwardTracker, {String, String, JobID, JobState});
forwardTracker(JobTracker, TaskTracker, JobId, State) :-
	cleanup(JobId, State),
    taskAttempt(JobTracker, TaskTracker, AttemptId, _, _, _, _, _, _),
    AttemptId.getJobID() == JobId;
    
jobCompletion(@TaskTracker, JobId, State) :-
	forwardTracker(@JobTracker, TaskTracker, JobId, State);

/*************** TASK INIT *********************/

define(initTask, {JobID, JobConf, String, TaskID, Constants.TaskType, Integer, FileInput, Integer, TaskState});
define(taskFile, {JobID, TaskID, List});
define(taskFileLocation, keys(0,1), {JobID, TaskID, String});

initTask(JobID, JobConf, JobFile, null, null, null, null, null, null) :-
	job(JobID, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, FinishTime, JobStatus),
	JobStatus.state() == Constants.JobState.PREP
	{
		JobStatus.state(Constants.JobState.RUNNING); // Update state to running
	};
	
/* Break out of the current fixpoint thread (using 'async') since taskCreate table function
   makes blocking calls to the DFS. */
async initTask
task(JobID, TaskID, Type, Partition, Input, MapCount, Status) :-
	taskCreate(initTask(JobID, JobConf, JobFile, TaskID, Type, Partition, Input, MapCount, Status));
	
taskFile(JobId, TaskId, Locations) :-
	task(JobId, TaskId, Type, _, Input, _, Status),
	Status.state() == Constants.TaskState.UNASSIGNED,
	Type == Constants.TaskType.MAP
	{ Split := Input.split(); Locations := Function.getLocations(Split); };
	
	
taskFileLocation(JobId, TaskId, Location) :-
	flatten(taskFile(JobId, TaskId, Locations)),
	Location := (String) Locations;
	
/*************** Job Status Maintenance *********************/
import java.util.Set;
	
/* Update the job state. */
public updateJobState
job(JobId, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, FinishTime, JobStatus) :-
	taskUpdate(JobId, TaskId, Type, TaskStatus),
	job(JobId, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, _, JobStatus),
	JobStatus.task(Type, TaskStatus)
	{
		FinishTime := JobStatus.state() == Constants.JobState.SUCCEEDED ? 
		              java.lang.System.currentTimeMillis() : 0L;
	};
	
define(killjob, {JobID});
public killJob
job(JobId, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, FinishTime, JobStatus) :-
	killjob(JobId), job(JobId, JobName, JobFile, JobConf, User, URL, Priority, SubmitTime, _, JobStatus)
	{
		FinishTime := java.lang.System.currentTimeMillis();
		JobStatus.killjob();
	};
	

public
hadoop::taskTrackerAction(JobTracker, TaskTracker, TaskTrackerAction.ActionType.KILL_TASK, Action) :-	
	job(JobId, _, _, _, _, _, _, _, _, JobStatus), 
	JobStatus.state() == Constants.JobState.KILLED || 
	JobStatus.state() == Constants.JobState.FAILED,
	hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, _, Constants.TaskState.RUNNING, _, _, _, _)
    {
        Action := new KillTaskAction(AttemptId);
    };
	
    
/*************** Task Status Maintenance *********************/

define(taskUpdate, {JobID, TaskID, Constants.TaskType, TaskState});

public
task(JobId, TaskId, Type, Partition, Input, MapCount, Status) :-
    taskUpdate(JobId, TaskId, _, Status),
	task(JobId, TaskId, Type, Partition, Input, MapCount, _);
	
/*************** CLEAN UP TASK ATTEMPT STATE ******/
delete
hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, Progress, State, Phase, TaskFileLoc, Start, Finish) :-
    taskUpdate(JobId, TaskId, _, Status), Status.state() == Constants.TaskState.SUCCEEDED,
	hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, Progress, State, Phase, TaskFileLoc, Start, Finish),
	AttemptId.getTaskID() == TaskId;

/************** SEND MAP TASK ATTEMPT STATUS TO REDUCER TASKTRACKERS ****************/

/* Tell reducers where the succeeded maps reside */
hadoop::taskAttempt(@TaskTracker, JobTracker, Id1, Progress, State, Phase, TaskFileLoc, Start, Finish) :-
	hadoop::taskAttempt(@JobTracker, _, Id1, Progress, State, Phase, TaskFileLoc, Start, Finish),
	State == Constants.TaskState.SUCCEEDED, Phase == Constants.TaskPhase.MAP,
	hadoop::taskAttempt(@JobTracker, TaskTracker, Id2, _, _, _, _, _, _),
	!Id2.isMap(), Id1.getJobID() == Id2.getJobID();

/************** GENERATE TASK UPDATES BASED ON UPDATES TO TASK ATTEMPT STATE ********/

public
taskUpdate(JobId, TaskId, Type, StatusUpdate) :-
	hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, Progress, State, Phase, TaskFileLoc, Start, Finish),
	task(JobId, TaskId, Type, _, _, _, TaskStatus),
	TaskId == AttemptId.getTaskID(), TaskStatus.state() != Constants.TaskState.SUCCEEDED 
	{
		StatusUpdate := TaskStatus.attempt(AttemptId, Progress, State, Phase, Start, Finish);
	};
