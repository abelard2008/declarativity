program scheduler;

import java.lang.String;
import java.lang.Integer;
import java.lang.Float;
import java.lang.System;
import jol.types.basic.ValueList;
import org.apache.hadoop.mapred.declarative.Constants;
import org.apache.hadoop.mapred.TaskTrackerAction;
import org.apache.hadoop.mapred.declarative.util.Function;
import org.apache.hadoop.mapred.declarative.util.JobState;
import org.apache.hadoop.mapred.JobPriority;
import org.apache.hadoop.mapred.JobID;
import org.apache.hadoop.mapred.TaskID;
import org.apache.hadoop.mapred.TaskAttemptID;


/*************** Task Attempt Scheduler *********************/

define(trackerCount, keys(0),   {Integer});
define(taskAttempts, keys(0,1), {JobID, TaskID, Integer});
define(schedule,        {JobID, TaskID, String});

watch(hadoop::taskAttempt, a);
watch(trackerMapWorkload, a);
watch(trackerReduceWorkload, a);

public
trackerCount(count<TaskTracker>) :-
	hadoop::taskTracker(JobTracker, TaskTracker, _, _, Constants.TrackerState.RUNNING);
	
public
taskAttempts(JobId, TaskId, count<AttemptId>) :-
	hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId)
	{
		JobId  := AttemptId.getJobID();
		TaskId := AttemptId.getTaskID();
	};
	
public
hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, 0f, State, Phase, FileLocation, System.currentTimeMillis(), 0L) :-
	schedule(JobId, TaskId, TaskTracker), notin taskAttempts(JobId, TaskId, _),
	hadoop::taskTracker(JobTracker, TaskTracker, Host, HttpPort, Constants.TrackerState.RUNNING, _, _, _, _, _)
	{
		State        := Constants.TaskState.RUNNING;
		Phase        := Constants.TaskPhase.STARTING;
		AttemptId    := new TaskAttemptID(TaskId, 0);
		FileLocation := "http://" + Host + ":" + HttpPort.toString();
	};

public
hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, 0f, State, Phase, FileLocation, System.currentTimeMillis(), 0L) :-
	schedule(JobId, TaskId, TaskTracker), taskAttempts(JobId, TaskId, Attempts),
	hadoop::taskTracker(JobTracker, TaskTracker, Host, HttpPort,  Constants.TrackerState.RUNNING, _, _, _, _, _)
	{
		State        := Constants.TaskState.RUNNING;
		Phase        := Constants.TaskPhase.STARTING;
		AttemptId    := new TaskAttemptID(TaskId, Attempts);
		FileLocation := "http://" + Host + ":" + HttpPort.toString();
	};
	
actionSchedule
hadoop::taskTrackerAction(JobTracker, TaskTracker, TaskTrackerAction.ActionType.LAUNCH_TASK, Action) :-	
    hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, _, Constants.TaskState.RUNNING, Constants.TaskPhase.STARTING),
    JobId := AttemptId.getJobID(), TaskId := AttemptId.getTaskID(),
	hadoop::job(JobId, JobName, JobFile),
	hadoop::task(JobId, TaskId, Type, Partition, FileInput, MapCount, _)
	{
		Action := Type == Constants.TaskType.MAP ? 
		          Function.launchMap(FileInput, JobFile, AttemptId, Partition, true) :
	 	          Function.launchReduce(JobFile, AttemptId, Partition, MapCount, true);
	};

/************** Keep tracker of task tracker workload status ********/
define(trackerMapWorkload,     keys(0), {String, Integer});
define(trackerReduceWorkload,  keys(0), {String, Integer});
define(trackerMapWork,         keys(0), {String, Integer});
define(trackerReduceWork,      keys(0), {String, Integer});

/* Initialize tracker workload. But don't erase when tracker goes to running state,
hence the #insert (basically turn off view maintenance). */
public
trackerMapWorkload(TaskTracker, 0) :-
	hadoop::taskTracker#insert(JobTracker, TaskTracker, _, _, Constants.TrackerState.INITIAL);
	
public
trackerReduceWorkload(TaskTracker, 0) :-
	hadoop::taskTracker#insert(JobTracker, TaskTracker, _, _, Constants.TrackerState.INITIAL);
	
trackerMapWorkload(TaskTracker, Maps) :-
    trackerMapWork(TaskTracker, Maps);

trackerReduceWorkload(TaskTracker, Reduces) :-
    trackerReduceWork(TaskTracker, Reduces);

public
trackerMapWork(TaskTracker, count<AttemptId>) :-
    hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, _, Constants.TaskState.RUNNING),
    AttemptId.isMap();
    
public
trackerReduceWork(TaskTracker, count<AttemptId>) :-
    hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, _, Constants.TaskState.RUNNING),
    !AttemptId.isMap();
    
/******************** Signal task tracker slots available *******************/
define(trackerSlots, {String, String, Integer, Integer});
define(slotSignal, {String, String, Integer, Integer});

public
slotSignal(TaskTracker, Host, MaxMap, MaxReduce) :-
    hadoop::task(JobId, TaskId, _, _, _, _, TaskStatus),
    TaskStatus.state() != Constants.TaskState.RUNNING,
	hadoop::taskTracker(JobTracker, TaskTracker, Host, _, Constants.TrackerState.RUNNING, _, _, _, MaxMap, MaxReduce);
	
slotSignal(TaskTracker, Host, MaxMap, MaxReduce) :-
    trackerMapWorkload#insert(TaskTracker),
	hadoop::taskTracker(JobTracker, TaskTracker, Host, _, Constants.TrackerState.RUNNING, _, _, _, MaxMap, MaxReduce);
	
slotSignal(TaskTracker, Host, MaxMap, MaxReduce) :-
    trackerReduceWorkload#insert(TaskTracker),
	hadoop::taskTracker(JobTracker, TaskTracker, Host, _, Constants.TrackerState.RUNNING, _, _, _, MaxMap, MaxReduce);

trackerSlots(TaskTracker, Host, Maps, 0) :-
    slotSignal(TaskTracker, Host, MaxMap, MaxReduce),
    hadoop::mapsWaitingForSchedule(Count), Count > 0,
    trackerMapWorkload(TaskTracker, MapCount),
    MapCount < MaxMap
    {
      Maps := MapCount < MaxMap ? MaxMap - MapCount : 0;
    };

trackerSlots(TaskTracker, Host, 0, Reduces) :-
    slotSignal(TaskTracker, Host, MaxMap, MaxReduce),
    hadoop::reducesWaitingForSchedule(Count), Count > 0,
    trackerReduceWorkload(TaskTracker, ReduceCount),
    ReduceCount < MaxReduce
    {
      Reduces := ReduceCount < MaxReduce ? MaxReduce - ReduceCount : 0;
    };


