program scheduler;

import java.lang.String;
import java.lang.Integer;
import java.lang.Float;
import java.lang.System;
import jol.types.basic.ValueList;
import org.apache.hadoop.mapred.declarative.Constants;
import org.apache.hadoop.mapred.TaskTrackerAction;
import org.apache.hadoop.mapred.declarative.util.Function;
import org.apache.hadoop.mapred.declarative.util.JobState;
import org.apache.hadoop.mapred.JobPriority;
import org.apache.hadoop.mapred.JobID;
import org.apache.hadoop.mapred.TaskID;
import org.apache.hadoop.mapred.TaskAttemptID;


/*************** Task Attempt Scheduler *********************/

define(trackerCount, keys(0),   {Integer});
define(taskAttempts, keys(0,1), {JobID, TaskID, Integer});
define(schedule,        {JobID, TaskID, String});

public
trackerCount(count<TaskTracker>) :-
	hadoop::taskTracker(JobTracker, TaskTracker, _, _, Constants.TrackerState.RUNNING);
	
public
taskAttempts(JobId, TaskId, count<AttemptId>) :-
	hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId)
	{
		JobId  := AttemptId.getJobID();
		TaskId := AttemptId.getTaskID();
	};
	
public
hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, 0f, State, Phase, FileLocation, System.currentTimeMillis(), 0L) :-
	schedule(JobId, TaskId, TaskTracker), notin taskAttempts(JobId, TaskId, _),
	hadoop::taskTracker(JobTracker, TaskTracker, Host, HttpPort, Constants.TrackerState.RUNNING, _, _, _, _, _)
	{
		State        := Constants.TaskState.RUNNING;
		Phase        := Constants.TaskPhase.STARTING;
		AttemptId    := new TaskAttemptID(TaskId, 0);
		FileLocation := "http://" + Host + ":" + HttpPort.toString();
	};

public
hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, 0f, State, Phase, FileLocation, System.currentTimeMillis(), 0L) :-
	schedule(JobId, TaskId, TaskTracker), taskAttempts(JobId, TaskId, Attempts),
	hadoop::taskTracker(JobTracker, TaskTracker, Host, HttpPort,  Constants.TrackerState.RUNNING, _, _, _, _, _)
	{
		State        := Constants.TaskState.RUNNING;
		Phase        := Constants.TaskPhase.STARTING;
		AttemptId    := new TaskAttemptID(TaskId, Attempts);
		FileLocation := "http://" + Host + ":" + HttpPort.toString();
	};
	
watch(hadoop::taskAttempt, a);
watch(hadoop::taskTrackerAction, sa);
actionSchedule
hadoop::taskTrackerAction(JobTracker, TaskTracker, TaskTrackerAction.ActionType.LAUNCH_TASK, Action) :-	
    hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, _, Constants.TaskState.RUNNING, Constants.TaskPhase.STARTING),
    JobId := AttemptId.getJobID(), TaskId := AttemptId.getTaskID(),
	hadoop::job(JobId, JobName, JobFile),
	hadoop::task(JobId, TaskId, Type, Partition, FileInput, MapCount, _)
	{
		Action := Type == Constants.TaskType.MAP ? 
		          Function.launchMap(FileInput, JobFile, AttemptId, Partition) :
	 	          Function.launchReduce(JobFile, AttemptId, Partition, MapCount);
	};

/******************** Signal task tracker slots available *******************/

watch(trackerSlots, a);
define(trackerSlots, {String, String, Integer, Integer});
public
trackerSlots(TaskTracker, Host, Maps, Reduces) :-
	hadoop::tasksWaitingForSchedule(Count), Count > 0,
	hadoop::trackerWorkload(TaskTracker, MapCount, ReduceCount),
	hadoop::taskTracker(_, TaskTracker, Host, _, State, _, _, _, MaxMap, MaxReduce, _),
	MapCount < MaxMap || ReduceCount < MaxReduce
    {
      Maps := MapCount < MaxMap ? MaxMap - MapCount : 0;
      Reduces := ReduceCount < MaxReduce ? MaxReduce - ReduceCount : 0;
    };
	