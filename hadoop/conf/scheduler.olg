program scheduler;

import java.lang.String;
import java.lang.Integer;
import java.lang.Float;
import java.lang.System;
import jol.types.basic.ValueList;
import org.apache.hadoop.mapred.declarative.Constants;
import org.apache.hadoop.mapred.TaskTrackerAction;
import org.apache.hadoop.mapred.declarative.util.Function;
import org.apache.hadoop.mapred.declarative.util.JobState;
import org.apache.hadoop.mapred.JobPriority;
import org.apache.hadoop.mapred.JobID;
import org.apache.hadoop.mapred.TaskID;
import org.apache.hadoop.mapred.TaskAttemptID;


/*************** Task Attempt Scheduler *********************/

define(trackerCount, keys(0),   {Integer});
define(taskAttempts, keys(0,1), {JobID, TaskID, Integer});
define(schedule,        {JobID, TaskID, String});

public
trackerCount(count<TaskTracker>) :-
	hadoop::taskTracker(JobTracker, TaskTracker, _, _, Constants.TrackerState.RUNNING);
	
public
taskAttempts(JobId, TaskId, count<AttemptId>) :-
	hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId)
	{
		JobId  := AttemptId.getJobID();
		TaskId := AttemptId.getTaskID();
	};
	
public
hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, 0f, State, Phase, FileLocation, System.currentTimeMillis(), 0L) :-
	schedule(JobId, TaskId, TaskTracker), notin taskAttempts(JobId, TaskId, _),
	hadoop::taskTracker(JobTracker, TaskTracker, Host, HttpPort, Constants.TrackerState.RUNNING, _, _, _, _, _)
	{
		State        := Constants.TaskState.RUNNING;
		Phase        := Constants.TaskPhase.STARTING;
		AttemptId    := new TaskAttemptID(TaskId, 0);
		FileLocation := "http://" + Host + ":" + HttpPort.toString();
	};

public
hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, 0f, State, Phase, FileLocation, System.currentTimeMillis(), 0L) :-
	schedule(JobId, TaskId, TaskTracker), taskAttempts(JobId, TaskId, Attempts),
	hadoop::taskTracker(JobTracker, TaskTracker, Host, HttpPort,  Constants.TrackerState.RUNNING, _, _, _, _, _)
	{
		State        := Constants.TaskState.RUNNING;
		Phase        := Constants.TaskPhase.STARTING;
		AttemptId    := new TaskAttemptID(TaskId, Attempts);
		FileLocation := "http://" + Host + ":" + HttpPort.toString();
	};
	
actionSchedule
hadoop::taskTrackerAction(JobTracker, TaskTracker, TaskTrackerAction.ActionType.LAUNCH_TASK, Action) :-	
    hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, _, Constants.TaskState.RUNNING, Constants.TaskPhase.STARTING),
    JobId := AttemptId.getJobID(), TaskId := AttemptId.getTaskID(),
	hadoop::job(JobId, JobName, JobFile),
	hadoop::task(JobId, TaskId, Type, Partition, FileInput, MapCount, _)
	{
		Action := Type == Constants.TaskType.MAP ? 
		          Function.launchMap(FileInput, JobFile, AttemptId, Partition) :
	 	          Function.launchReduce(JobFile, AttemptId, Partition, MapCount);
	};

/************** Keep tracker of task tracker workload status ********/
define(trackerWorkload,   keys(0), {String, Integer, Integer});
define(trackerMapWork,    keys(0), {String, Integer});
define(trackerReduceWork, keys(0), {String, Integer});

/* Initialize tracker workload. But don't erase when tracker goes to running state,
hence the #insert (basically turn off view maintenance). */
public
trackerWorkload(TaskTracker, 0, 0) :-
	hadoop::taskTracker#insert(JobTracker, TaskTracker, _, _, Constants.TrackerState.INITIAL);
	
trackerWorkload(TaskTracker, Maps, Reduces) :-
	trackerMapWork(TaskTracker, Maps), trackerReduceWork(TaskTracker, Reduces);
	
trackerWorkload(TaskTracker, Maps, 0) :-
	trackerMapWork(TaskTracker, Maps), notin trackerReduceWork(TaskTracker, Reduces);
	
trackerWorkload(TaskTracker, 0, Reduces) :-
	trackerReduceWork(TaskTracker, Reduces), notin trackerMapWork(TaskTracker, Maps); 

public
trackerMapWork(TaskTracker, count<AttemptId>) :-
    hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, _, Constants.TaskState.RUNNING, Phase),
    Phase != Constants.TaskPhase.STARTING, AttemptId.isMap();
    
public
trackerReduceWork(TaskTracker, count<AttemptId>) :-
    hadoop::taskAttempt(JobTracker, TaskTracker, AttemptId, _, Constants.TaskState.RUNNING, Phase),
    Phase != Constants.TaskPhase.STARTING, !AttemptId.isMap();

/******************** Signal task tracker slots available *******************/

define(slotSignal, {String, String, Integer, Integer});

public
slotSignal(TaskTracker, Host, MaxMap, MaxReduce) :-
	trackerWorkload(TaskTracker, MapCount, ReduceCount),
	hadoop::taskTracker#insert(_, TaskTracker, Host, _, State, _, _, _, MaxMap, MaxReduce, _);

define(trackerSlots, {String, String, Integer, Integer});
trackerSlots(TaskTracker, Host, Maps, 0) :-
    slotSignal(TaskTracker, Host, MaxMap, MaxReduce),
    hadoop::mapsWaitingForSchedule(Count), Count > 0,
    trackerWorkload(TaskTracker, MapCount, ReduceCount),
    MapCount < MaxMap
    {
      Maps := (MapCount + ReduceCount) < MaxMap ? MaxMap - (MapCount + ReduceCount) : 0;
    };

trackerSlots(TaskTracker, Host, 0, Reduces) :-
    slotSignal(TaskTracker, Host, MaxMap, MaxReduce),
    hadoop::reducesWaitingForSchedule(Count), Count > 0,
    trackerWorkload(TaskTracker, MapCount, ReduceCount),
    ReduceCount < MaxReduce
    {
      Reduces := (MapCount + ReduceCount) < MaxReduce ? MaxReduce - (ReduceCount + MapCount) : 0;
    };


