\documentclass{article}
\usepackage{fullpage}
\usepackage{epsfig}
\usepackage{url}
\title{BoomFS: A Declarative Approach To Building\\Distributed Filesystems}
\author{Peter Alvaro, Neil Conway}
\date{December 19, 2008}
\twocolumn
\begin{document}
\maketitle
\begin{abstract}
  While architectures for distributed computing are changing rapidly,
  techniques for building distributed systems have remained
  stagnant. As distributed computation becomes the common case,
  traditional techniques for building distributed systems will become
  untenable, because they force programmers to deal with the mundane
  boilerplate details of constructing reliable distributed
  systems. This yields programs that are difficult to construct,
  understand, modify, and adapt to new environments. We propose BOOM,
  an ongoing project to develop concise declarative specifications for
  a broad class of scalable distributed systems. In this paper, we
  describe the first application in the BOOM stack: BoomFS, a
  distributed filesystem that is implemented using a combination of
  Java and declarative logic. We show that BoomFS is easy to
  understand and modify, and achieves competitive performance in a
  preliminary performance study.
\end{abstract}
\section{Introduction}
\label{introduction}
With the widespread adoption of cloud computing, pervasive mobile
clients, and many-core processors, computing architectures are
undergoing a period of disruptive change. In the near future, nearly
every non-trivial software system will be physically distributed:
distributed programming will be the common case.

Traditional techniques for building distributed systems are ill-suited
to this new environment. Despite considerable research, developing
fault-tolerant distributed systems remains enormously difficult and
expensive, and is typically only attempted by experienced programmers
with extensive training in the field. As more journeyman developers
must deal with the challenges of distributed computing, this situation
will become increasingly untenable. Better techniques for constructing
distributed systems are urgently needed.

We believe that a new approach to building distributed systems is
necessary to address this need. Inspired by prior work on declarative
networking~\cite{dn-sigmod, network-data-indep}, we propose a new
architectural style for distributed programs in which policy and
protocol are specified in a declarative logic language, while the
operational mechanisms of the system are written in a traditional
imperative language. We are engaged in the Berkeley Orders of
Magnitude (BOOM) project, which aims to use this style to build common
cloud computing infrastructure components using orders of magnitude
less code, and operate at orders of magnitude more scale. In Section
\ref{boom-vision}, we discuss the problems with traditional tools for
building distributed systems in more detail, and outline the BOOM
vision in contrast.

The first system we have built using this architectural style is
BoomFS, a distributed filesystem similar to the Google File
System~\cite{gfs}. Our goal in building this system was not novelty of
design: although BoomFS supports multiple master nodes, its
architecture is otherwise very similar to GFS. Instead, our aim was to
show that a GFS-like filesystem can be easily and concisely
implemented in the BOOM style. The resulting system should achieve
competitive performance, and be easy to understand and adapt to new
environments and policies. In Section \ref{system-arch}, we describe
the basic architecture of BoomFS. In Section \ref{system-realize}, we
detail how this architecture was realized using a mixture of
declarative logic and Java. In Section \ref{perf-eval}, we report on a
preliminary performance study comparing BoomFS and HDFS, an open
source implementation of the GFS design. In Section \ref{future-work},
we explore extensions to the file system that are facilitated by our
use of a declarative logic language. In Section \ref{related-work}, we
discuss related work, and we conclude in Section \ref{conclusion}.

\section{The BOOM Vision}
\label{boom-vision}
The fundamental problem with traditional techniques for building
distributed systems is that they provide the wrong abstractions to the
programmer. Distributed systems are typically implemented with tools
developed for single-machine programs and only superficially adapted
to the challenges of a distributed setting. Programmers are forced to
deal with the mundane details of communication, synchronization, and
consensus. As a result, the essence of the distributed computation is
obscured by a forest of boilerplate details. Evidence for this can be
seen in the fact that distributed algorithms such as Paxos can be
stated in a page of pseudocode, but typically require many thousands
of lines of code to implement using standard
tools~\cite{paxos-made-live}.

Traditional tools operate at a low level of abstraction because they
force programmers to intermingle the specification of \emph{what} a
distributed program should do with \emph{how} it can be done. That is,
mechanism and policy are specified together, and implemented in the
same language. This has two primary problems: it yields fragile
programs that cannot easily adapt to change, and it results in
programs that are difficult to understand.

For example, consider a client program that wants to compute a
function over data stored in a compute cloud. Should the function's
code be sent to the server, should the data be sent to the client, or
should both code and data be sent to an intermediary? These
alternatives can be viewed as ``query plans'' that accomplish the same
objective, but have different performance characteristics. The optimal
plan depends on factors including the relative costs of network
bandwidth, server-side computation and client-side computation, how
much data is required, how expensive the function is, and the
frequency with which the function is invoked or the input data is
modified. All of these parameters are likely to fluctuate, both within
a single environment over time,\footnote{Internal heterogeneity and
  performance variability in virtualized environments such as Amazon's
  Elastic Compute Cloud has been well-documented in recent
  work~\cite{late-sched}.} and when the distributed program is
deployed to a new environment. Traditional approaches to constructing
distributed systems require hardcoding assumptions about these
parameters. This yields fragile programs that are expensive to modify,
and difficult to adapt to new environments.

% XXX: rework following paragraph
In addition to inflexibility, the failure to separate mechanism from
policy makes programs harder to understand and reason about. The
policies and protocols of a program (the ``what'') are typically the
most interesting parts of it (and often the hardest to get right). The
mechanisms that achieve those policies are often straightforward but
verbose. By implementing policy and protocol together with mechanism,
the mundane details of the latter obscure the essential nature of the
former, harming comprehensibility. Furthermore, a language that is
appropriate for implementing mechanism is unlikely to be ideal for
specifying policy, and vice versa. There are many examples of
algorithms that can be concisely expressed in an appropriate
declarative language, but are much harder to write in an imperative
language (e.g.\ \cite{chord-overlog}).

Inspired by the data independence provided by the relational model, we
aim to provide \emph{network scale independence} for distributed
systems by separating the programmer's intent from its concrete
realization. In BOOM, a distributed system is composed of two types of
components:
\begin{itemize}
\item
  \emph{imperative} components implement the basic units of
  functionality of a distributed system. Imperative components are
  typically used to perform tasks like I/O and numerical
  computation. These tasks are usually best stated in an imperative
  language like Java or C++, particularly because these components
  often involve interaction with the operating system or native
  libraries. In BoomFS, imperative components implemented in Java are
  used to transfer data chunks between hosts, and to read and write
  data from the native file system.

\item
  \emph{declarative} components specify the bulk of the logic of the
  distributed system. Declarative components are written as a
  collection of logical rules that describe the coordination and
  composition of the imperative components. Essentially, the
  declarative logic is responsible for deciding ``what'' a member of
  the distributed system should do; the imperative components are
  responsible for realizing those actions. A declarative component is
  essentially a join between a stream of events and a
  database. Evaluating this query over an event stream results in
  producing more events (either at the local node or a remote node),
  inserting new database tuples, or invoking imperative
  components.

  Declarative components are implemented in a high-level declarative
  logic language, such as Overlog~\cite{dn-sigmod}. This requires the
  state of the distributed system to be represented in tables.

  In BoomFS, declarative components implement the protocols between
  clients and master nodes, between different masters, and between
  masters and data nodes. That is, declarative rules implement the
  ``control path'', which contains the vast majority of the complexity
  in the BoomFS design. The data path (from clients to data nodes and
  between different data nodes, is implemented as an imperative
  component, because it is straightforward and primarily involves the
  transfer of bytes.
\end{itemize}

\subsection{Declarative Specification of Distributed Filesystems}
Why choose GFS to implement in a declarative language?

Before directly addressing why we chose this GFS re-envisioning as the the first vehicle of the BOOM philosophy, we observe that it is common for well-designed systems to structurally separate control and data paths.  Often this is a result of practical concerns that have little to do with the abstract notion of separating policy and mechanism: first, it is undesirable for data path congestion to delay control messages, and second, a certain amount of control overhead associated with better functionality is acceptable, so long as it is off the critical path of data transfer, which will contribute the most to overall latency as transfer sizes increase .  Data transfer mechanisms such as DOT, and even early Internet applications like FTP, send control messages on a separate channel from data transfer.

GFS was a good fit for our approach for several reasons.  First, the separation of data and control planes described above is central to its design.  Their design choice of a single master introduced the possibility of bottlenecks, and a master-independent data path alleviates this potential hotspot, making client-master communication necessary only for atomic filesystem metadata operations.   The overhead associated with the necessary consistency guarantees over these operations is divorced from the data path in the GFS design.

Second, we observe that in GFS and other distributed filesystems, the bulk of system complexity lies not in mechanism but in policy.  This is not to say that operational components (zero-copy I/O, fast checksumming, careful use of caches at various levels) are not critical to system performance, but merely that the bulk of the design and implementation deal with enforcing and maintaining the various invariants that make the system behave correctly, including replica placement decisions, failure handling, load balancing and consensus among agents in shared computations.

Combining these observations, we see that the GFS style of distributed filesystem is a perfect vehicle for our philosophy.  Our task in developing a declaratively specified prototype is to represent the distributed state of the file system (the union of the various states in the finite automata describing the master and datanode servers) as a single relational database, with relations partitioned across nodes.  Both policy (which expresses constraints over these relations, and conditions under which the contents of the relations may change) and protocol (which expresses how data may move from one partition, and hence node,  to another) are then expressed in a network-aware recursive query language that operates over the relations.

We feel that our approach is a natural way to express systems of this kind, and results in a codebase that is easy to understand, validate against the original specification, and extend.  As we built the prototype, we concentrated first on the correctness of the specification and on exploiting the economy of mechanism exposed by flattening system state.  For the mechanism components including data transfer and source-routing for the data path, we implemented simple, best-effort modules as place holders while we composed the collection of logical rules comprising the declarative specification of the system.  Once this was correct, it was simple to return to the mechanism modules and tune them to improve performance, without worrying about violating the correctness guaranteed by the policy layer.

\section{System Architecture}
\label{system-arch}
We now turn to discussing the architecture, implementation, and
performance characteristics of BoomFS. The architecture of the system
is directly inspired by the design of the Google File System. The
design goals and system architecture of GFS are well-documented
elsewhere~\cite{gfs, hdfs-arch}, so we include only a short discussion
of them here.

Like GFS, BoomFS does not attempt to be a general-purpose distributed
filesystem. Instead, it is designed to perform well for a particular
class of workloads, and to operate on a particular cluster
architecture. We focus on achieving good performance for large
sequential reads and writes. Files are assumed to be very large;
therefore, we choose a large chunk size of XXX. We focus on delivering
high sequential read and write throughput and efficient network
utilization, rather than achieving low latency or efficient usage of
stable storage. Members of a BoomFS system are assumed to be
unreliable commodity machines, provisioned with relatively modest
resources.  Reliability is achieved by storing multiple copies of each
chunk; scalability is achieved by spreading file system contents over
a large cluster of machines (typically hundreds or thousands).

\begin{figure}
% XXX
\centering
\epsfig{file=figures/big_read_throughput.eps, width=1\columnwidth}
\caption{The components of the BoomFS architecture}
\label{fig:system-arch}
\end{figure}

\subsection{System Overview}
The major components of the BoomFS design are depicted in Figure
\ref{fig:system-arch}. There are three types of nodes in BoomFS:
\emph{master nodes}, \emph{data nodes}, and \emph{clients}. Master
nodes contain the canonical description of the structure of the
filesystem. The filesystem is described by a mapping from file names
to file identifiers, and a mapping from file identifiers to the
sequence of chunks that contain the file's content.\footnote{The
  current prototype does not support directories, but this is a
  relatively inconsequential implementation shortcut that we plan to
  address in the near future.} To avoid a single point of failure, our
design allows for multiple master nodes, which are kept in synchrony
using a consensus protocol (currently Paxos~\cite{paxos-made-simple}).

Data nodes are responsible for storing chunks. They have no knowledge
of the structure of the filesystem as a whole, nor even which files
the chunks they store belong to. Each data node periodically sends a
\emph{heartbeat} to the master nodes. This notifies the masters that
the data node is still alive, and contains a list of the chunks
currently stored at the data node. The masters use this information to
update a mapping from chunk identifiers to the set of data nodes that
might be holding that chunk. Note that the canonical description of
the content of a data node resides at the data node itself, not at the
master nodes; the master's copy of this information is updated lazily.

Finally, client nodes represent application programs that wish to read
and write files. We expect that applications will interact with the
system through a client library that provides a convenient stream-like
abstraction for files stored in BoomFS. In the future, we plan to
implement the HDFS API, to allow BoomFS to easily replace HDFS in
existing Hadoop installations. % mention POSIX?

\subsection{Filesystem Operations}
\label{fs-ops}
To append to a file, a client node begins by contacting one of the
master nodes, and requesting that a new chunk be added to a particular
file.\footnote{In the current prototype, random writes are not
  supported, and each append operation creates a new chunk --- we
  expect that each append will be large enough to justify occupying a
  single chunk. We discuss our plans for relaxing these constraints in
  Section \ref{future-work}.} The master first generates a new chunk
identifier. To ensure that the state of the file system is consistent
across all the masters, the master uses a consensus protocol to ensure
that all masters have agreed to add the new chunk identifier at the
same position in the chunk list for the appropriate file. Once
consensus has been reached, the master replies to the client with the
new chunk identifier, and a list of data nodes that might be
appropriate locations for the new chunk.

The client is then responsible for pushing data for the chunk to a
sufficient number of data nodes. There are various policies a client
could use to do this. For example, the client could directly connect
to all the data nodes and send the chunk content itself, or it might
only send the data to a single data node, and instruct that node to
propagate the data onward. Once the chunk content has been completely
received by a data node and written to disk, this fact will be
reflected in the next heartbeat that the data node sends to the
masters. In turn, this will make the data node available for
subsequent operations on the chunk.

To read from a file, a client once again begins by contacting one of
the master nodes to fetch the list of chunks that make up the target
file. For each chunk identifier in the list, the client consults the
master to determine the set of data nodes that have copies of that
chunk. The client then reads the chunk by connecting to one of the
data nodes directly.

To delete a file, a client contacts a master node. The master passes
the request through the consensus protocol. When consensus has been
reached, all the masters remove the file from their internal
metadata. There is no need to eagerly remove the chunks belonging to
the file from data nodes: the next time the data node sends a
heartbeat to a master, the master replies with a list of the chunks on
the data node that is has no knowledge of. These orphaned chunks can
be garbage collected at the data node's leisure.

Other filesystem operations are straightforward. Clients interact with
master nodes to obtain directory listings, and create and remove
directories.\footnote{Our prototype implementation of BoomFS does not
  currently support directories, but we expect that this will be easy
  to fix.}

\section{System Realization}
\label{system-realize}
The filesystem design described in Section \ref{system-arch} is
abstract, and it might reasonably be implemented using a variety of
techniques. In this section, we describe how we implemented the BoomFS
design using a combination of Java, for the imperative components, and
Overlog, for the declarative components. We validate this
implementation decision in two ways: by demonstrating how this
implementation allows for the easy modification of filesystem policies
in Section XXX, and by conducting a performance study in Section
\ref{perf-eval}.

\subsection{Software Architecture}
\begin{figure}
% XXX
\centering
\epsfig{file=figures/big_read_throughput.eps, width=1\columnwidth}
\caption{The architecture of a program using JOL}
\label{fig:jol-arch}
\end{figure}
In BoomFS, masters, data nodes, and clients are all implemented using
a combination of Java and Overlog. We used JOL~\cite{jol}, a
Java-based Overlog implementation that allows an Overlog evaluator to
be embedded inside a Java program, and allows Overlog programs to
contain Java objects and invoke Java methods. Java programs can insert
and delete tuples from Overlog relations, and register ``queries''
that invoke a callback when a particular state is reached by the
Overlog instance. Figure \ref{fig:jol-arch} depicts this design.

Each node type in BoomFS is structured in a similar way. Upon startup,
the node runs Java code that bootstraps a JOL instance, and installs
the Overlog files for BoomFS.

This architecture is quite similar to the familiar structure of an
application that uses a DBMS: the database is responsible for managing
data, and the application uses queries to retrieve and modify the
state of the database. However, there are some crucial differences
between this architecture and the design of BoomFS. First, Overlog
relations are partitioned over multiple nodes.

\subsection{State Representation}
Any filesystem contains two kinds of state: data and metadata. In
BoomFS, data is stored as a collection of chunks distributed over the
data nodes. Each data node stores chunks as normal files on its
filesystem.

All the metadata in BoomFS is represented as relations; some of these
relations are partitioned over multiple nodes.

\subsection{Control Protocol}

\subsection{Data Protocol}
The data protocol is a straightforward mechanism for transferring
chunk content between two hosts (either between a client and a data
node, or between two data nodes). The server side of the protocol is
implemented as a simple imperative component that runs on each data
node. It listens on a dedicated TCP port, and handles each client
connection using a separate thread. Data transfer is done using the
\texttt{FileChannel.transferTo()} mechanism provided by Java NIO,
which is a Java mechanism that allows an underlying zero copy API to
do the bulk data transfer (such as \texttt{sendfile(2)} on Linux). The
data protocol consists of only two operations: writing a chunk from
the client to the server, and reading a chunk from the server to the
client.

While the data protocol is unicast, a client node typically wants to
write a new chunk to multiple data nodes. As noted in Section
\ref{fs-ops}, there are several ways in which this could be
accomplished. The current BoomFS prototype does ``source routing'':
after obtaining a new chunk identifier from a master node, the client
decides on the sequence of data nodes that it wants to write a new
chunk to, and then transfers the chunk to the first node in the
sequence. That node then transfers the chunk to the second node in the
sequence, and so on. While this approach is effective at utilizing
network bandwidth, it is subject to partial failures, and the current
implementation is not pipelined. More importantly, routing decisions
of this nature should be specified in Overlog, not hardcoded in
Java. We plan to rectify this shortcoming in the near future.

\subsection{Fault Tolerance}
In the target environment of GFS deployments, namely large clusters of
commodity machines, ``component failures are the norm rather than the
exception''~\cite{gfs}.  In this section we explore several of the
failure modes of the system, and discuss how our integration of a
declaratively-specified Paxos implementation addresses shortcomings in
the original GFS and HDFS design.

\subsubsection{Data Node Failure}
As long as the replication factor of a file is greater than one, the
loss of a data node will be transparent to applications using the
system.  The last heartbeats of the lost node will quickly expire from
the soft-state tables on the masters, and clients will no longer be
directed to this server for read or write requests.  The updated soft
state will cause aggregates in the BFS logic to be recomputed, which
in turn cause new migration events to be fired, selecting another
replica as a target for the chunks whose replication factor is now too
low.  GFS and HDFS both use this kind of replication strategy, though
Ghemawat et al. discuss the possible use of other forms of redundancy,
such as parity.

\subsubsection{Data Node Disk Failure}
The effect of this failure is more or less the same as for data node
failure, assuming that all the filesystem data is stored on a single
disk.  The data node will continue to send heartbeats to the master,
but one of these will include a delta record describing the loss of
the files on the disk.  The same chain of inferences described in the
lost data node scenario described above will fire for these chunks.
It should be noted that our current prototype will throw an exception
after trying to read from the lost directory, and the data node
service will stop, resulting in \emph{identical} behavior to the
failed data node scenario.

\subsubsection{Data Corruption}
GFS and HDFS implement data integrity checking at different
granularities: in HDFS, chunk (called ``blocks'' in HDFS) checksums
are calculated over the entire 64M chunk, while in GFS, a chunk is
subdivided into 64K blocks, each of which has an associated 32bit
checksum.  Our prototype did not implement any integrity checks, but
we plan to add them soon.

\subsubsection{Master Failure}
GFS keeps all filesystem metadata on a single master server to which
all client requests are directed: the mapping is maintained through
the DNS.  Several secondary masters are run in a log replay mode
similar to the log-shipping approach employed by database systems: a
metadata change is not considered committed until the log tail has
been flushed not only to stable local storage, but to all secondary
masters.  This implies the use of a two-phase commit protocol, but the
implementation is not discussed in detail.  If the primary master
fails, the failure must be detected by an external application, which
then remaps the DNS entry to one of the secondaries.
 
HDFS supports the concept of a ``Secondary NameNode'' or backup master
that asynchronously applies log entries to a checkpoint image.  The
primary master supports writing log entries to multiple directories,
one of which can be a remote filesystem such as NFS: a secondary
master can then read the log from this filesystem.  Presumably, this
adds considerable overhead to the performance of the master.
 
In our prototype, a configurable number of masters operate in
lock-step using the Paxos consensus protocol, also implemented in the
Overlog language.

\section{Performance Evaluation}
\label{perf-eval}
We validate the applicability of our approach by prototyping a
distributed filesystem based on Google's GFS, and evaluating its
performance and robustness to failure.  Our goal is to achieve
competitive performance on the large append and read workload
characteristic of the GFS environment in spite of the increased
control overhead of our Overlog runtime, and to demonstrate fault
tolerance even when master nodes are lost.  In this section, we
present an evaluation of how successful we were in achieving these
goals.

In the first section, we compare the cost of metadata operations
between our prototype and HDFS, the Hadoop open source implementation
of GFS.  Following this, we simulate a MapReduce sort benchmark,
comparing read and write performance between BFS and HDFS as we scale
the number of concurrent clients and data nodes together.  Finally, we
discuss the various failure modes of the architecture and the impact
of our multi-master extension on these scenarios.

\subsection{Test Environment}
GFS is a proprietary filesystem: we can only speculate about its
implementation based on the documentation that Google has published,
and we cannot evaluate its performance against our own simulated
workloads.  Instead, we used Hadoop, Yahoo's open source
implementation of MapReduce, which includes a distributed filesystem
(HDFS) based on GFS.

We performed our experiments on the Amazon Elastic Compute Cloud, a
virtualized ``utility computing'' environment~\cite{amazon-ec2}.  EC2
provides a convenient and inexpensive testing platform, but this
convenience can come at the cost of high variability in network and
disk performance (LATE paper, EC2).  Hadoop is already well-integrated
with EC2, and instrumenting BFS to handle dynamic address assignment
was trivial.

\subsection{Metadata Operations}
\begin{figure}
\centering
\epsfig{file=figures/metadata_throughput.eps, width=1\columnwidth}
\caption{Performance for metadata operations}
\label{fig:metadata-perf}
\end{figure}

In our first experiment, we compare the latency of metadata operations
affecting the control path.  A directory listing uses the client
protocol and requires a round trip between the client and master and a
lookup on the master, as does touching a file on the filesystem.
Copying a zero-byte file requires two round trips to the master: one
to get a new chunk id for the append operation, and another to request
a link of data nodes who can accept the new chunkid.  (Does hadoop
combine these?  probably, and it would be easy for us to do so).

The results of running 100 of each of these metadata operations are
shown in Figure \ref{fig:metadata-perf}.  As we anticipated, the
overhead of these operations is significantly higher in our
implementation that in HDFS.  This is due in part to client-side
caching, enabled in HDFS and not yet implemented in BFS, although it
would be simple to locally materialize lookup results into relations
that can be queried before visiting the master. (actually, I don't
think we can claim this.  you can't cache lookup results, new file
create requests or new chunk requests!).

\subsection{Sort Benchmark}
Our next experiment evaluates read and write performance under a
typical MapReduce workload.  The sort benchmark of Dean and
Ghemawat~\cite{mapreduce} is a simple and practical test that puts
equal stress on the read and write components of the distributed
filesystem.

The MapReduce specification of sorting is trivial, largely because
sorting (over \emph{some} key) is a side-effect of the framework.
Thus, the map function simply returns the sort key and the entire line
as the key's value, and the reduce function is simply the identity
function.  In an execution of the sort, a single input file is read in
parallel in disjoint sections by all of the mappers, which apply the
hash function and write the bucketed results to their local disks.
The reducers then read these files via RPC calls, apply the built-in
merge sort, and write as many files back to the GFS as there are
reducers.

For the purposes of our experiments, we are interested only in the
costs associated with the parallel reads at the start of the workflow
described above, and the parallel writes described at the end.  Hence,
in our simulation we dispense with the actual sorting, hashing and
crossbarring, and focus merely on the filesystem performance as the
number of concurrent clients and data nodes are scaled up together.
For a given concurrency level D, we provision an EC2 cluster with a
master node and D data nodes, and prime the filesystem by creating D
100MB files with a replication factor of 2.  We then spawn D client
processes (one on each data node), which read one of the files from the
filesystem, write it to local disk, then create a new distributed
filesystem file and append the contents of the local file to it.
Thus, each client reads (writes) 100MB to (from) the filesystem
concurrently in each sort benchmark, as we vary D.

\begin{figure}
\centering
\epsfig{file=figures/big_read_throughput.eps, width=1\columnwidth}
\caption{Sequential read performance for the sort benchmark}
\label{fig:big-read-perf}
\end{figure}
\begin{figure}
\centering
\epsfig{file=figures/big_write_throughput.eps, width=1\columnwidth}
\caption{Sequential write performance for the sort benchmark}
\label{fig:big-write-perf}
\end{figure}
Our results are summarized in Figures \ref{fig:big-read-perf} and
\ref{fig:big-write-perf}.  At lower concurrency levels, BFS is
comparable in read performance and handily outperforms HDFS in write
performance.  This result supports our intuition that the relatively
high cost of metadata operations is quickly amortized by the highly
efficient datapath under the large transfers characteristic of
MapReduce and Hadoop workloads.

In HDFS, read latency increases gradually as load increases, but write
latency reacts very quickly to concurrency, increasing by almost two
times as we increase the number of clients from 1 to 4.  Beyond that
point, HDFS gracefully handles increasing load, with low variance.
BFS shows similar read results, but significantly lower write latency
(less than half that of HDFS, though the variance is higher).  At 16
concurrent clients, the BFS implementation began suffering timeouts.
At this time is is difficult for us to say whether this is due to
issues with queueing in the Overlog runtime implementation, race
conditions or other bugs in the Overlog specification of GFS, or a
problem with the custom data transfer protocol that we implemented.
The variance for x=16 in the graph in Figure 4 reflects the fact that
out of 48 observations, 4 timeouts occurred, resulting in write times
greater than 60 seconds (because our timeout was 60 seconds).  If we
omit those four observations from our results, the mean write response
time is 10.12 seconds.

\section{Future Work}
\label{future-work}
\begin{itemize}
\item
  Challenges/difficulties

\item
  Some of the Lincoln vision?

\item
  Cross-layer optimizations

\item
  Continuous query optimization

\item
  Hadoop integration
\end{itemize}

\section{Related Work}
\label{related-work}

\section{Conclusion}
\label{conclusion}
We have implemented BoomFS, a distributed filesystem that is similar
to GFS and HDFS. BoomFS achieves competitive performance with HDFS
while avoiding a single point of failure through its support for
multiple master nodes.

More importantly, BoomFS is implemented using the BOOM architectural
style: the complicated protocols and policy of the control path are
implemented in declarative logic, while the relatively simple,
high-performance data path is implemented in Java. BoomFS was
relatively easy to implement; because the implementation is concise
and comprehensible, BoomFS can be extended and adapted to new
environments. BoomFS is the first member of a class of data center
infrastructure projects we plan to undertake under the mantle of the
BOOM project.

\bibliographystyle{abbrv}
\bibliography{paper}
\appendix
\section{Paxos in Overlog}
\end{document}
