\documentclass{article}
\usepackage{fullpage}
\title{BoomFS: A Declarative Distributed Filesystem}
\author{Peter Alvaro, Neil Conway}
\date{December 20, 2008}
\begin{document}
\maketitle
\begin{abstract}
  While architectures for distributed computing are changing rapidly,
  techniques for building distributed systems have remained
  stagnant. As distributed computation becomes the common case,
  traditional techniques for building distributed systems will become
  untenable, because they force programs to deal with the mundane
  boilerplate details of constructing reliable distributed
  systems. This yields programs that are difficult to construct,
  understand, modify, and port to new environments. We propose BOOM, a
  new paradigm for concisely specifying a broad class of distributed
  systems. In this paper, we describe the first application in the
  BOOM stack: BoomFS, a distributed filesystem similar to the Google
  File System\cite{gfs} that is implemented in a mixture of Java and
  declarative logic.
\end{abstract}

\section{Introduction}
With the widespread adoption of cloud computing, pervasive mobile
clients, and many-core processors, computing architectures are
undergoing a period of disruptive change. In the near future, nearly
every non-trivial software system will be physically distributed:
distributed programming will be the common case.

Traditional techniques for building distributed systems are ill-suited
to this new environment. Despite considerable research, developing
fault-tolerant distributed systems remains enormously difficult and
expensive, and is typically only attempted by experienced programmers
with extensive training in the field. As more journeyman developers
must deal with the challenges of distributed computing as a matter of
course, this situation will become untenable. Better techniques for
constructing distributed systems are urgently needed.

We believe a new approach to building distributed systems is necessary
to address this need. Inspired by prior work on declarative
networking, \ldots

In Section 2, we discuss the failings of traditional tools for
building distributed systems, and outline the BOOM vision in
contrast. We also motivate our decision to begin the BOOM project by
implementing a declarative file system. In Section 3, we describe the
design goals and basic architecture of BoomFS. In Section 4, we detail
how this architecture was realized in the BOOM style. In Section 5, we
report on a performance study comparing BoomFS and HDFS\cite{}. In
Section 6, we explore extensions to the file system that are
facilitated by our use of a declarative coordination language. In
Section 7, we discuss related work, and we conclude in Section 8.

\section{The BOOM Vision}
The fundamental problem with traditional techniques for building
distributed systems is that they provide the wrong abstractions to the
programmer. Distributed systems are typically implemented with tools
developed for single-machine programs, and only superficially adapted
to the challenges of a distributed setting. Programmers are forced to
deal with the mundane details of communication, synchronization, and
consensus. As a result, the essence of the distributed computation is
obscured by a forest of boilerplate details. Evidence for this can be
seen in the fact that distributed algorithms such as Paxos can be
stated in a page of pseudocode, but typically require many thousands
of lines of code to implement using standard
tools\cite{paxosmadelive}.

Traditional tools operate at a low level of abstraction because they
force programmers to intermingle the specification of \emph{what} a
distributed program should do with \emph{how} it can be done. This
results in fragile programs that fail to adapt to changes in their
environments. For example, consider a client program that wants to
compute a function over data stored in a compute cloud. Should the
function code be sent to the server, or should the data be sent to the
client? The optimal policy depends on factors including the relative
costs of network bandwidth, server-side computation and client-side
computation, as well as how much data is required, how expensive the
function is, and the frequency with which the function is invoked or
the input data is modified. All of these variables are likely to
fluctuate in a distributed system, so encoding a particular decision
into the application program results in a fragile design. This
fragility results in programs that cannot easily be modified, or
ported to distributed systems with varying performance
characteristics.

Inspired by the data independence provided by the relational model, we
aim to provide \emph{network scale independence} for distributed
systems by separating the programmer's intent from its concrete
realization. In BOOM, a distributed system is composed of two types of
components:
\begin{itemize}
\item
  \emph{imperative} components implement the basic units of
  functionality of a distributed system. Imperative components are
  typically used to perform tasks like I/O and numerical
  computation. These tasks are usually best stated in an imperative
  language like Java or C++, particularly because these components
  often involve interaction with the operating system or native
  libraries. In BoomFS, imperative components implemented in Java are
  used to transfer data chunks between hosts, and to read and write
  data from the native file system.

\item
  \emph{declarative} components specify the bulk of the logic of the
  distributed system. Declarative components are written as a
  collection of logical rules that describe the coordination and
  composition of the imperative components. Essentially, the
  declarative logic is responsible for deciding ``what'' a member of
  the distributed system should do; the imperative components are
  responsible for realizing those actions. A declarative component is
  essentially a join between a stream of events and a
  database. Evaluating this query over an event stream results in
  producing more events (either at the local node or a remote node),
  inserting new database tuples, or invoking imperative
  components.

  Declarative components are implemented in a high-level declarative
  logic language, such as Overlog[cite]. This requires the state of
  the distributed system to be represented in tables.

  In BoomFS, declarative components implement the protocols between
  clients and master nodes, between different masters, and between
  masters and data nodes. That is, declarative rules implement the
  ``control path'', which represents the vast majority of the
  complexity in the BoomFS design. The data path (from clients to data
  nodes and between different data nodes, is implemented as an
  imperative component, because it is straightforward and primarily
  involves copying bytes.
\end{itemize}

\subsection{Declarative Specification of Distributed Filesystems}
Why choose GFS to implement in a declarative language?
\begin{itemize}
\item
  Separation of control and data paths in the design.

\item
  Data centric, ``systemsy''

\item
  Existing implementations intermingle policy and mechanism
\end{itemize}

\section{System Architecture}

\section{System Realization}
\begin{itemize}
\item
  Language integration: what belongs in declarative land, and what in
  imperative land?

\item
  Representation of data

\item
  Use of Paxos as a component
\end{itemize}

\section{Performance Evaluation}

We  validate the applicability of our approach by prototyping a distributed filesystem based on Google's GFS, and evaluating its performance and robustness to failure.  Our goal is to achieve competitive performance on the large append and read workload characteristic of the GFS environment in spite of the increased control overhead of our Overlog runtime, and to demonstrate fault tolerance even when master nodes are lost.  In this section, we present an evaluation of how successful we were in achieving these goals.

In the first section, we compare the cost of metadata operations between our prototype and HDFS, the Hadoop open source implementation of GFS.  Following this, we simulate a MapReduce sort benchmark, comparing read and write performance between BFS and HDFS as we 
scale the number of concurrent clients and datanodes together.  Finally, we discuss the various failure modes of the architecture and the impact of our multi-master extension on these scenarios.

\subsection{Test Environment}

GFS is a proprietary filesystem: we can only speculate about its implementation based on the documentation that Google has published, and we cannot evaluate its performance against our own simulated workloads.  Instead, we used Hadoop, Yahoo's open source implemenation of MapReduce, which includes a distributed filesystem (HDFS) based on GFS.

We performed our experiments on the Amazon Elastic Compute Cloud, a virtualized "utility computing" environment.  EC2 provides a convenient and inexpensive testing platform, but this convenience can come at the cost of high variability in network and disk performance (LATE paper, EC2).  Hadoop is already well-integrated with EC2, and instrumenting BFS to handle dynamic address assignment was trivial.

\subsection{Metadata Operations}

In our first experiment, we compare the latency of metadata operations affecting the control path.  A directory listing uses the client protocol and requires a round trip between the client and master and a lookup on the master, as does touching a file on the filesystem.  Copying a zero-byte file requires two round trips to the master: one to get a new chunk id for the append operation, and another to request a link of datanodes who can accept the new chunkid.  (Does hadoop combine these?  probably, and it would be easy for us to do so).

The results of running 100 of each of these metadata operations are shown in Figure 1.  As we anticipated, the overhead of these operations is significantly higher in our implementation that in HDFS.  This is due in part to client-side caching, enabled in HDFS and not yet implemented in BFS, although it would be simple to locally materialize lookup results into relations that can be queried before visiting the master. (actually, I don't think we can claim this.  you can't cache lookup results, new file create requests or new chunk requests!).

\subsection{Sort Benchmark}

Our next experiment evaluates read and write performance under a typical MapReduce workload.  The sort benchmark used by Dean et al [] is a simple and practical test that puts equal stress on the read and write components of the distributed filesystem.  

The MapReduce specification of sorting is trivial, largely because sorting (over \emph{some} key) is a side-effect of the framework.  Thus, the map function simply returns the sort key and the entire line as the key's value, and the reduce function is simply the identity function.  In an execution of the sort, a single input file is read in parallel in disjoint sections by all of the mappers, which apply the hash function and write the bucketed results to their local disks.  The reducers then read these files via RPC calls, apply the built-in merge sort, and write as many files back to the GFS as there are reducers.

For the purposes of our experiments, we are interested only in the costs associated with the parallel reads at the start of the workflow described above, and the parallel writes described at the end.  Hence, in our simulation we dispense with the actual sorting, hashing and crossbarring, and focus merely on the filesystem performance as  the number of concurrent clients and datanodes are scaled up together.  For a given concurrency level D, we provision an EC2 cluster with a master node and D datanodes, and prime the filesystem by creating D 100MB files with a replication factor of 2.  We then spawn D client processes (one on each datanode), which read one of the files from the filesystem, write it to local disk, then create a new distributed filesystem file and append the contents of the local file to it.  Thus, each client reads (writes) 100MB to (from) the filesystem concurrently in each sort benchmark, as we vary D.

Our results are summarized in Figures 2 and 3.  At lower concurrency levels, BFS is comparable in read performance and handily outperforms HSFS in write performance.  This result supports our intuition that the relatively high cost of metadata operations is quickly amortized by the highly efficient datapath under the large transfers characteristic of MapReduce and Hadoop workloads.

In HDFS, read latency increases gradually as load increases, but write latency reacts very quickly to concurrency, increasing by almost two times as we increase the number of clients from 1 to 4.  Beyond that point, HDFS gracefully handles increasing load, with low variance.  BFS shows similar read results, but significantly lower write latency (less than half that of HDFS, though the variance is higher).  At 16 concurrent clients, the BFS implementation began suffering timeouts.  At this time is is difficult for us to say whether this is due to issues with queueing in the Overlog runtime implementation, race conditions or other bugs in the Overlog  specification of GFS, or a problem with the custom data transfer protocol that we implemented.  The variance for x=16 in the graph in Figure 4 reflects the fact that out of 48 observations, 4 timeouts occurred, resulting in write times greater than 60 seconds (because our timeout was 60 seconds).  If we omit those four observations from our results, the mean write response time is 10.12 seconds.

\subsection{Fault Tolerance}



\section{Future Work}
\begin{itemize}
\item
  Challenges/difficulties

\item
  Some of the Lincoln vision?

\item
  Cross-layer optimizations

\item
  Continuous query optimization

\item
  Hadoop integration
\end{itemize}

\section{Related Work}

\section{Conclusions}

\bibliographystyle{abbrv}
\bibliography{paper}
\appendix
\section{Paxos in Overlog}
\end{document}
