program gfs;

import java.util.Random;
import jol.types.basic.ValueList;
import gfs.Conf;
import gfs.Master;
import gfs.Shell;

// The address of the local node
define(self, keys(0), {String});
self(Conf.getSelfAddress());

gfs_global::file(Self, "Foo", Chunks) :-
    self(Self),
    Chunks := new ValueList({1,2,3});

// Node, Master node
watch(master_for_node, ae);
define(master_for_node, keys(0, 1), {String, String});

// Node address, Request ID, Request type, Arg
define(start_request, keys(0, 1), {String, Integer, String, String});

// Master node, Request ID, Source node, Request type, Arg
define(request, keys(0, 1), {String, Integer, String, String, String});

watch(request, ae);

request(@Master, ReqId, Source, ReqType, Args) :-
    self(@Source),
    master_for_node(@Source, Master),
    // XXX PA
    start_request(@Source, ReqId, ReqType, Args);

// Client node, Request ID, Master node, Success, Results
define(response, keys(0, 1), {String, Integer, String, Boolean, ValueList});
watch(response, ae);

// if we get a "NewChunk" request for a file, create a new chunk id

define(chunkSeq, keys(0), {String, Integer});
watch(chunkSeq, ae);
chunkSeq(Master, Top+1) :-   
    self(Master),
    notin chunkSeq(Master,SomeId),
    chunkHWM(Master,Top);

chunkSeq(Master, I+1) :-
    chunkSeq(Master, I),
    request#insert(Master, Id, Source, "NewChunk", _);


response(@Source, Id, Master, true, Stuff) :-
    //request(@Master, Id, Source, "NewChunk", FName),
    // this needs to be tied into a paxos decree to enforce the ordering.
    newListHolder(@Master, Source, Id, Stuff, I, FName),
    Stuff.insert(I.toString());

define(newListHolder,keys(0,2),{String,String,Integer,ValueList,Integer,String});
watch(newListHolder, ae);

newListHolder(Master,Source,Id,List,I,FName) :- 
    candidate_datanode(Master, Id, Source, FName, I, Stuff),
    request(Master, Id, Source, NewChunk, FName),
    List := new ValueList(Stuff.toArray());

define(chunk_cnt, keys(0,1), {String,String,Integer});
//watch(chunk_cnt, ae);
chunk_cnt(Master, DataNode, count<ChunkId>) :-
    //long_period(_,_,_),
    chunks(Master, DataNode, ChunkId,_);

/*
define(lowest_loaded_nodes,keys(0,2), {String, Integer, String});
watch(lowest_loaded_nodes, ae);
lowest_loaded_nodes(Master, Cnt, DataNode) :-
    flatk(Master, Id),
    possible_choices(Master, Cnt, DataNode, Id);

delete
possible_choices(Master, Cnt, DataNode, Id) :-
    possible_choices(Master, Cnt, DataNode, Id),
    lowest_loaded_nodes(Master,Cnt,DataNode);

define(flatk,keys(0,1),{String, Integer});
//watch(flatk, ae);
flatk(Master, IdFlat) :-
    flatten(kpile(Master, Id)),
    IdFlat := (Integer)Id;

define(kpile,keys(0,1), {String, ValueList});
watch(kpile, ae);
kpile(Master, topk<Id,3>) :-
    possible_choices(Master, Cnt, DataNode, Id);

*/

define(possible_choices,keys(0,1,2,3), {String, Integer, String,Integer});
//define(possible_choices, {String, Integer, String,Integer});
//watch(possible_choices, ae);
possible_choices(Master, Cnt, DataNode, Id) :-
    flat_cnt(Master, Cnt),
    // SLOW!!
    //Id := (new Random()).nextInt(1000),
    Id := -100,
    chunk_cnt(Master, DataNode, Cnt);


define(flat_cnt, keys(0,1), {String, Integer});
flat_cnt(Master, Cnt) :-
    flatten(lowest_loaded_cnt(Master, BK)),
    Cnt := (Integer)BK;

define(lowest_loaded_cnt, keys(0), {String,ValueList});
//watch(lowest_loaded_cnt, ae);
lowest_loaded_cnt(Master, bottomk<Cnt, 10>) :-
    chunk_cnt(Master, _, Cnt); 

/*
delete
lowest_loaded_cnt(Master, Cnts) :-
    lowest_loaded_cnt(Master, Cnts),
    lowest_loaded_nodes#insert(Master, Cnt, DataNode); 
*/    

watch(candidate_datanode, ae);

define(candidate_datanode, keys(0, 2), {String, Integer, String, String, Integer, ValueList});
candidate_datanode(Master, Id, Source, FName, I, generic<(new ValueList()).insert(DataNode)>) :-
    request(Master, Id, Source, "NewChunk", FName),
    chunkSeq(Master,I),
    //lowest_loaded_nodes(Master, Cnt, DataNode),
    possible_choices(Master, Cnt, DataNode, Id2),
    gfs_global::file(Master, FName, _),
    notin candidate_datanode(Master, Id, Source, FName, _, _);

delete 
request(Master, Id, Source, "NewChunk", FName) :-
    newListHolder(Master, Source, Id, List, I, FName);


// a "NewChunk" request for a nonexistent file causes an error.
response(@Source, Id, Master, false, null) :-
    request(@Master, Id, Source, "NewChunk", FName),
    notin gfs_global::file(@Master, FName, _);


// If we get a "ChunkList" request for an extant file, send back its
// chunks
// X-
response(@Source, Id, Master, true, Chunks) :-
    request(@Master, Id, Source, "ChunkList", FName),
    gfs_global::file(@Master, FName, Chunks);

response(@Source, Id, Master, false, null) :-
    request(@Master, Id, Source, "ChunkList", FName),
    notin gfs_global::file(@Master, FName, _);


// this is an alias of request, for "ChunkLocation" request types, with ChunkId cast as an Integer 
// I was having problems with overlog math on Longs....
define(clRequest, {String, Integer, String, String, Integer});
clRequest(Master, ReqId, Source, "ChunkLocations", CId) :-
    request(Master, ReqId, Source, "ChunkLocations", ChunkId),
    CId := Integer.valueOf(ChunkId);

// we need to have a ground-truth HWM for 
define(chunkHWM, keys(0, 1), {String,Integer});
chunkHWM(Master,max<ChunkId>) :-
    chunks(Master,_,ChunkId,_);

//watch(chunkHWM,ae);

// If we get a "ChunkLocations" request for a chunk that we have knowledge
// of, return a list of the nodes holding that chunk. Otherwise, return
// an error.
define(compute_chunk_locs, keys(0, 1, 2), {String, Integer, String, ValueList});

compute_chunk_locs(@Master, ReqId, Source, generic<(new ValueList()).insert(NodeAddr)>) :-
    //request(@Master, ReqId, Source, "ChunkLocations", ChunkId),
    clRequest(@Master, ReqId, Source, "ChunkLocations", ChunkId),
    chunks(@Master, NodeAddr, ChunkId, _);

response(@Source, Id, Master, true, NodeList) :-
    request(@Master, Id, Source, "ChunkLocations", _),
    compute_chunk_locs(@Master, Id, Source, NodeList);

response(@Source, ReqId, Master, false, null) :-
    //request(@Master, Id, Source, "ChunkLocations", ChunkId),
    clRequest(@Master, ReqId, Source, "ChunkLocations", ChunkId),
    notin chunks(@Master, _, ChunkId, _);



// If we get a "cat" request for a file that exists, send back the
// contents of the file
// X-?
response(@Source, Id, Master, Success, Chunks) :-
    request(@Master, Id, Source, "Cat", FName),
    gfs_global::file(@Master, FName, Chunks),
    Success := true;

// If the file doesn't exist, return an error message
response(@Source, Id, Master, Success, ErrMessage) :-
    request(@Master, Id, Source, "Cat", FName),
    notin gfs_global::file(@Master, FName, _),
    Success := false,
    ErrMessage := new ValueList();//"File does not exist";

define(create_request_pending, keys(0, 1), {String, Integer, String, Boolean});
watch(create_request_pending, ae);

// If we got a "create" request for a file that didn't exist,
// we need to wait until we get consensus from Paxos; then we
// can send back a positive ack.
// XXX: the use of "#insert" here is a hack
create_request_pending(@Master, Id, Source, true) :-
    request#insert(@Master, Id, Source, "Create", FName),
    notin gfs_global::file(@Master, FName, _);

// If we get a "create" request for a file that exists, we
// need to wait until we get consensus from Paxos; then we
// can send back a negative ack
create_request_pending(@Master, Id, Source, false) :-
    request#insert(@Master, Id, Source, "Create", FName),
    gfs_global::file(@Master, FName, _);

public
response(@Source, Id, Master, Success, null) :-
    gfs_global::create_request_done(@Master, Id, Source, Success);

// If we get an "ls" request, send back the names of all the files in
// the file system. If the file system is empty, send back an empty
// ValueList. Note that we need to do the aggregation in a sub-rule.
define(compute_ls, keys(0, 1), {String, Integer, String, ValueList});
watch(compute_ls, ae);
compute_ls(@Master, Id, Source, generic<(new ValueList()).insert(FName)>) :-
    request(@Master, Id, Source, "Ls", _),
    gfs_global::file(@Master, FName, _);

response(@Source, Id, Master, true, FileList) :-
    request(@Master, Id, Source, "Ls", _),
    compute_ls(@Master, Id, Source, FileList);

response(@Source, Id, Master, true, FileList) :-
    request(@Master, Id, Source, "Ls", _),
    notin gfs_global::file(@Master, _, _),
    FileList := new ValueList();

define(rm_request_pending, keys(0, 1), {String, Integer, String, Boolean});

// If we got an "rm" request and the file exists, remove it and send
// back a positive ack.
public
rm_request_pending(@Master, Id, Source, true) :-
    request(@Master, Id, Source, "Rm", FName),
    gfs_global::file(@Master, FName, _);

public
response(@Source, Id, Master, Success, null) :-
    gfs_global::rm_request_done(@Master, Id, Source, Success);

// If we got an "rm" request and the file doesn't exist, return a
// negative ack. NB: unlike in the "create" case, we don't need the
// "#insert" hack, since deletions are postponed the end of the
// fixpoint.
// XXX: we should probably synchronize with Paxos here
response(@Source, Id, Master, false, null) :-
    request(@Master, Id, Source, "Rm", FName),
    notin gfs_global::file(@Master, FName, _);

/*****************************/
define(sendHeartBeat, keys(0, 1, 2), {String, String,Integer, Long, Long});
define(dataNodes, keys(0, 1), {String, String, Long});
//watch(dataNodes, ae);
//watch(sendHeartBeat, ae);
timer(clock, physical, 5000, 5000, 1000);

define(ackHeartBeat, keys(0, 1, 2), {String, String, Long});

ackHeartBeat(@Dnode, Master, Id) :-
    sendHeartBeat(@Master, Dnode, _, _, Id),
    Id != -1L;

dataNodes(Master, Host, max<Tstamp>) :-
    sendHeartBeat(Master, Host, _, Tstamp, _);

delete
sendHeartBeat(Master, Host, ChunkId, Tstamp, Id) :-
    sendHeartBeat(Master, Host, ChunkId, Tstamp, Id),
    dataNodes(Master, Host, HighTstamp),
    HighTstamp > Tstamp;
    

// Master node, Data node, BlockId, Timestamp of last
// update from data node
define(chunks, keys(0, 1, 2),{String, String, Integer, Long});
chunks(Master, DataNode, ChunkId, MaxTstamp) :-
    sendHeartBeat(Master, DataNode, ChunkId, Tstamp, Id),
    dataNodes(Master, DataNode, MaxTstamp),
    (java.lang.System.currentTimeMillis() - MaxTstamp) < 10000L;

delete
chunks(Master, DataNode, ChunkId, Ts) :-
    chunks(Master, DataNode, ChunkId, Ts),
    clock(_, _, _),
    (java.lang.System.currentTimeMillis() - Ts) > 10000L;


/**************************/
// maintenance functions

define(rep_factor, keys(0,1), {String,Integer,Integer});
watch(rep_factor, ae);
rep_factor(Master, ChunkId, count<DataNode>) :-
    long_period(_,_,_),
    chunks(Master, DataNode, ChunkId, Ts);

define(back_of_list, keys(0,1), {String,Integer,String});
//watch(back_of_list, ae);
back_of_list(Master, ChunkId, min<DataNode>) :-
    long_period(_,_,_),
    chunks(Master, DataNode, ChunkId, Ts);

timer(long_period, physical, 10000, 100000, 0);

define(to_delete, keys(0,2), {String,String,Integer});
//watch(to_delete, ae);
to_delete(Master, DataNode, ChunkId) :-
    rep_factor(Master, ChunkId, Cnt),
    back_of_list(Master, ChunkId, DataNode),
    ChunkId > 1,
    long_period(_, _, _),
    Cnt > Conf.getRepFactor();

define(newReplica, {String, Integer});
watch(newReplica, ae);
newReplica(Master, ChunkId) :-
    rep_factor(Master, ChunkId, Cnt),
    ChunkId > 1,
    long_period(_, _, _),
    Cnt < Conf.getRepFactor();

// hacky constraint: one message per tick
define(send_delete, keys(0), {String, Integer, String});
send_delete(@DataNode,ChunkId,Master) :-
    to_delete(@Master,DataNode,ChunkId);

// not correct.
/*
delete 
chunks(Master, DataNode, ChunkId, MTS) :-
    chunks(Master, DataNode, ChunkId, MTS),
    to_delete(Master,DataNode,ChunkId);
*/
