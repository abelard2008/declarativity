program gfs;

import jol.types.basic.ValueList;
import gfs.Conf;
import gfs.Master;
import gfs.Shell;

// The address of the local node
define(self, keys(0), {String});
self(Conf.getSelfAddress());

gfs_global::file(Self, "Foo", Chunks) :-
    self(Self),
    Chunks := new ValueList({1,2,3});

// Node, Master node
watch(master_for_node, ae);
define(master_for_node, keys(0, 1), {String, String});

// Node address, Request ID, Request type, Arg
define(start_request, keys(0, 1), {String, Integer, String, String});

// Master node, Request ID, Source node, Request type, Arg
define(request, keys(0, 1), {String, Integer, String, String, String});

watch(request, ae);

request(@Master, ReqId, Source, ReqType, Args) :-
    self(@Source),
    master_for_node(@Source, Master),
    start_request(@Source, ReqId, ReqType, Args);

// Client node, Request ID, Master node, Success, Results
define(response, keys(0, 1), {String, Integer, String, Boolean, ValueList});
watch(response, ae);

// if we get a "NewChunk" request for a file, create a new chunk id

define(chunkSeq, keys(0), {String, Integer});
watch(chunkSeq, ae);
chunkSeq(Master, Top+1) :-   
    self(Master),
    notin chunkSeq(Master,SomeId),
    chunkHWM(Master,Top);

chunkSeq(Master, I+1) :-
    chunkSeq(Master, I),
    request#insert(Master, Id, Source, "NewChunk", _);



response(@Source, Id, Master, true, Stuff) :-
    request(@Master, Id, Source, "NewChunk", FName),
    // this needs to be tied into a paxos decree to enforce the ordering.
    candidate_datanode#insert(@Master, Id, Source, FName, I, Stuff),
    Stuff.insert(I.toString());

watch(candidate_datanode, ae);

define(candidate_datanode, keys(0, 2), {String, Integer, String, String, Integer, ValueList});
candidate_datanode(Master, Id, Source, FName, I, generic<(new ValueList()).insert(DataNode)>) :-
    request#insert(Master, Id, Source, "NewChunk", FName),
    chunkSeq(Master,I),
    chunks(Master,DataNode,ChunkId,_),
    gfs_global::file(Master, FName, _);
    // this is where it gets interesting.
    // some distance function over the datanodes...


// add chunks to the file list.
watch(gfs_global::file, ae);
public
gfs_global::file(Master,FName,Chunks) :-
    gfs_global::file(Master,FName,Chunks),
    candidate_datanode#insert(Master, Id, Source, FName, I, Stuff),
    Chunks.insert(I);



// a "NewChunk" request for a nonexistent file causes an error.
response(@Source, Id, Master, false, null) :-
    request(@Master, Id, Source, "NewChunk", FName),
    notin gfs_global::file(@Master, FName, _);


// If we get a "ChunkList" request for an extant file, send back its
// chunks
response(@Source, Id, Master, true, Chunks) :-
    request(@Master, Id, Source, "ChunkList", FName),
    gfs_global::file(@Master, FName, Chunks);

response(@Source, Id, Master, false, null) :-
    request(@Master, Id, Source, "ChunkList", FName),
    notin gfs_global::file(@Master, FName, _);


// this is an alias of request, for "ChunkLocation" request types, with ChunkId cast as an Integer 
// I was having problems with overlog math on Longs....
define(clRequest, {String, Integer, String, String, Integer});
clRequest(Master, ReqId, Source, "ChunkLocations", CId) :-
    request(Master, ReqId, Source, "ChunkLocations", ChunkId),
    CId := Integer.valueOf(ChunkId);

// we need to have a ground-truth HWM for 
define(chunkHWM, keys(0, 1), {String,Integer});
chunkHWM(Master,max<ChunkId>) :-
    chunks(Master,_,ChunkId,_);

watch(chunkHWM,ae);

// If we get a "ChunkLocations" request for a chunk that we have knowledge
// of, return a list of the nodes holding that chunk. Otherwise, return
// an error.
define(compute_chunk_locs, keys(0, 1, 2), {String, Integer, String, ValueList});

compute_chunk_locs(@Master, ReqId, Source, generic<(new ValueList()).insert(NodeAddr)>) :-
    //request(@Master, ReqId, Source, "ChunkLocations", ChunkId),
    clRequest(@Master, ReqId, Source, "ChunkLocations", ChunkId),
    chunks(@Master, NodeAddr, ChunkId, _);

response(@Source, Id, Master, true, NodeList) :-
    request(@Master, Id, Source, "ChunkLocations", _),
    compute_chunk_locs(@Master, Id, Source, NodeList);

response(@Source, ReqId, Master, false, null) :-
    //request(@Master, Id, Source, "ChunkLocations", ChunkId),
    clRequest(@Master, ReqId, Source, "ChunkLocations", ChunkId),
    notin chunks(@Master, _, ChunkId, _);



// If we get a "cat" request for a file that exists, send back the
// contents of the file
response(@Source, Id, Master, Success, Chunks) :-
    request(@Master, Id, Source, "Cat", FName),
    gfs_global::file(@Master, FName, Chunks),
    Success := true;

// If the file doesn't exist, return an error message
response(@Source, Id, Master, Success, ErrMessage) :-
    request(@Master, Id, Source, "Cat", FName),
    notin gfs_global::file(@Master, FName, _),
    Success := false,
    ErrMessage := new ValueList();//"File does not exist";

define(create_request_pending, keys(0, 1), {String, Integer, String, Boolean});
watch(create_request_pending, ae);

// If we got a "create" request for a file that didn't exist,
// we need to wait until we get consensus from Paxos; then we
// can send back a positive ack.
// XXX: the use of "#insert" here is a hack
create_request_pending(@Master, Id, Source, true) :-
    request#insert(@Master, Id, Source, "Create", FName),
    notin gfs_global::file(@Master, FName, _);

// If we get a "create" request for a file that exists, we
// need to wait until we get consensus from Paxos; then we
// can send back a negative ack
create_request_pending(@Master, Id, Source, false) :-
    request#insert(@Master, Id, Source, "Create", FName),
    gfs_global::file(@Master, FName, _);

public
response(@Source, Id, Master, Success, null) :-
    gfs_global::create_request_done(@Master, Id, Source, Success);

// If we get an "ls" request, send back the names of all the files in
// the file system. If the file system is empty, send back an empty
// ValueList. Note that we need to do the aggregation in a sub-rule.
define(compute_ls, keys(0, 1), {String, Integer, String, ValueList});

compute_ls(@Master, Id, Source, generic<(new ValueList()).insert(FName)>) :-
    request(@Master, Id, Source, "Ls", _),
    gfs_global::file(@Master, FName, _);

response(@Source, Id, Master, true, FileList) :-
    request(@Master, Id, Source, "Ls", _),
    compute_ls(@Master, Id, Source, FileList);

response(@Source, Id, Master, true, FileList) :-
    request(@Master, Id, Source, "Ls", _),
    notin gfs_global::file(@Master, _, _),
    FileList := new ValueList();

define(rm_request_pending, keys(0, 1), {String, Integer, String, Boolean});

// If we got an "rm" request and the file exists, remove it and send
// back a positive ack.
public
rm_request_pending(@Master, Id, Source, true) :-
    request(@Master, Id, Source, "Rm", FName),
    gfs_global::file(@Master, FName, _);

public
response(@Source, Id, Master, Success, null) :-
    gfs_global::rm_request_done(@Master, Id, Source, Success);

// If we got an "rm" request and the file doesn't exist, return a
// negative ack. NB: unlike in the "create" case, we don't need the
// "#insert" hack, since deletions are postponed the end of the
// fixpoint.
// XXX: we should probably synchronize with Paxos here
response(@Source, Id, Master, false, null) :-
    request(@Master, Id, Source, "Rm", FName),
    notin gfs_global::file(@Master, FName, _);

/*****************************/
define(sendHeartBeat, keys(0, 1, 2), {String, String,Integer, Long, Long});
define(dataNodes, keys(0, 1), {String, String, Long});
//watch(dataNodes, ae);
//watch(sendHeartBeat, ae);
timer(clock, physical, 5000, 5000, 1000);

define(ackHeartBeat, keys(0, 1, 2), {String, String, Long});

ackHeartBeat(@Dnode, Master, Id) :-
    sendHeartBeat(@Master, Dnode, _, _, Id),
    Id != -1L;

dataNodes(Master, Host, max<Tstamp>) :-
    sendHeartBeat(Master, Host, _, Tstamp, _);

// Master node, Data node, BlockId, Timestamp of last
// update from data node
define(chunks, keys(0, 1, 2),{String, String, Integer, Long});
//watch(chunks, ae);
chunks(Master, DataNode, ChunkId, MaxTstamp) :-
    sendHeartBeat(Master, DataNode, ChunkId, Tstamp, Id),
    dataNodes(Master, DataNode, MaxTstamp),
    (java.lang.System.currentTimeMillis() - MaxTstamp) < 10000L;

delete
chunks(Master, DataNode, ChunkId, Ts) :-
    chunks(Master, DataNode, ChunkId, Ts),
    clock(_, _, _),
    (java.lang.System.currentTimeMillis() - Ts) > 10000L;
