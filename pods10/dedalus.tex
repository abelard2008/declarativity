\section{Dedalus}

By reifying time as data, we are able to reason about time in our logic.  some useful things fall out of this right away: persistence is now programatic rather than a separate type, ditto key constraints.  event creation vs. effect ambiguities are resolved.

Perhaps more importantly, the infinite sequence of abstract time gives us a way to reason about ordering, which is particularly difficult in a set-oriented language like Datalog.  The ordering over any program inputs (e.g. message queues) can be represented as a mapping between the ordering domain of the input and the time relation.

\subsection{Syntax}

A Dedalus program is a Datalog program in which every predicate is annotated with a time suffix. Time suffixes take one of the forms:

\begin{enumerate}
\item $N$
\item $N + 1$
\item $N + r(A_{1}, A_{2}, [...], A_{n})$
\item an integer
\end{enumerate}

A Dedalus predicate therefore has the following form:

$p(A_{1}, A_{2}, [...], A_{n})@S$

The predicate p() is a truth-valued function over its arguments $A_{1} - A_{n}$, which may be of any type, and S, which is an integer expression 
referring to the logical clock time at which the predicate holds, taking one of the three suffix forms above. Facts and rules in Dedalus are 
defined just as in Datalog, with the additional restrictions:

\begin{itemize}
\item Every body predicate may only have the suffix $N$.
\item A head predicate may have any suffix except a constant integer.
\item A fact must be posited with a constant integer for S, or the special function now().
\end{itemize}
Rules with the head suffix $N$ are called \emph{deductive} or atemporal rules, and describe all the logical consequences of facts in a given 
timestep. Deductive rules may be interpreted as pure Datalog rules by dropping the suffixes, treating all facts that are true in the current 
timestep as Datalog EDB, and running the rules to fixpoint.

Rules with the head suffix $N + 1$ are called \emph{inductive} temporal rules, and describe the relationship between facts in the current timestep 
and their consequences in the immediate next timestep. Inductive rules allow us to atomically capture change in time, and to model persistent state.

Rules with the head suffix $N+r(_)$ are also temporal rules, but unlike inductive rules, they carry no guarantee as to in which timestep, if any, 
their consequences will become visible. Such rules, called message rules, allow us to model network messages between nodes: the nodes 
are likely to have different clock values, and messages may be lost or delayed arbitrarily.

\subsubsection{Events}

An event in Dedalus corresponds to a Datalog fact.  It is a bodyless head clause with all constant terms in the form


$p(C_{1},C_{2},[...],C_{n})@I;$


where the elements of C are constants of any type and I is an integer constant.

Events provide ground for any logical inferences given by the deductive rules of the program, and may provide ground for inferences at 
future time steps via inductive rules.

\subsubsection{Persistence}

Events are only true at a single timestep.  It might seem that we could express a persistent predicate as a Datalog fact with a free variable 
for the time suffix.  The tuple would then be universally quantified over time:

\begin{Dedalus}
p(A, B)@N;
\end{Dedalus}

But clearly, because this must be interpreted as a rule head with an unbound variable, it produces an unsafe rule.  Instead, persistence is
expressed by an inductive rule that projects a tuple into the next timestep:

\begin{Dedalus}
p(A, B)@N+1 \(\leftarrow\)
  p(A, B)@N, 
  \(\lnot\)del\_p(A, B)@N;
\end{Dedalus}

The second subgoal allows us to model overwriteable storage: without it, a tuple will be trivially true at every future timestep if it becomes true
at any timestep.  Consider the following trace of events:

\begin{Dedalus}
p(1,2)@101;
p(1,3)@102;
p(1,?)@200;
del_p(1,2)@300;
p(1,?)@301;
\end{Dedalus}

It is easy to see that the results of the two queries are:


\begin{Dedalus}
p(1,2)@200;
p(1,3)@200;
p(1,3)@301;
\end{Dedalus}

\subsubsection{State Change}

Under this interpretation, a database update is an atomic (due to the adjacent timestamps)
pair of events with a deletion of the old value and assertion of the new, in the form:

$p(C_{1},C_{2},[...],C_{n})@I;$
\\
$del\_p(C_{1},C_{2},[...],C_{n})@I+1;$

For example:

\begin{Dedalus}
del\_p(1,2)@300; 
p(1, 4)@301;
\end{Dedalus}

\subsubsection{Sequences}

\begin{Dedalus}
seq(Agent, S + 1)@N+1 \(\leftarrow\)
  seq(Agent, S)@N, 
  event(Agent)@N; 
  
seq(Agent, S)@N+1 \(\leftarrow\) 
  seq(Agent, S)@N, 
  \(\lnot\) event(Agent)@N;
\end{Dedalus}

\subsection{Semantics}
\subsection{Post-hoc Interpretation}
\subsection{Continuous Interpretation}
