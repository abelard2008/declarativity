\section{State in Logic}

%%\jmh{I think the flavor of this section is to map familiar notions from state manipulation to purely logical syntactic constructs grounded in Datalog.  You should motivate 
%%at a high level why one would want to do such a thing.  The discussion below is then, in effect, a set of ``design patterns'' or ``typical uses''.  You might want to highlight 
%%the logical invariants that emerge from these \lang statements and assert that they capture the semantic intent of state manipulation.}


%%\jmh{Your definition is not self-contained, it makes reference to prose.  The prose is not crisp even as prose -- I don't know what ``later'' means, especially given that you said that @sync could produce any time.}


%%\linebreak
\begin{quote}
%
``Time is a device that was invented to keep everything from
happening at once."~\footnote{Graffiti on a wall at Cambridge
University~\cite{scheme}}
%
\end{quote} 

Given our definition of \slang, we now address the persistence and mutability
of data across time -- one of the two signature features of distributed systems
for which we provide a model-theoretic foundation.

The intuition behind \slang's \emph{successor} relation is that it models the
passage of (logical) time.  In our discussion, we will say that ground atoms
with lower timestamps occur ``before" atoms with higher ones.
%%Without specifying how and when each stratum
%%of \emph{successor} is evaluated, we can see that the syntactic restrictions of \lang ensure that 
%%we cannot move backwards infinitely often (specifically,
%%we may only move backwards via \emph{choice}, and every call to \emph{choice} may move us forward.  \paa{save for later, pretend there is no async?}).
%%we may only move forward in time.
The constraints we imposed on \slang rules restrict how deductions may be made
with respect to time.  First, rules may only refer to a single timestep in
their body, and {\em cannot join across timesteps}.  Second, rules may specify
deductions that occur concurrently with their ground facts or in the next
timestep--in \slang, we rule out induction ``backwards'' in time or
``skipping'' into the future.
%, or at some unspecified time: therefore inductions may only be specified in
%one direction only.

This notion of time allows us to consider the contents of the EDB -- and hence
a minimal model of the IDB -- with respect to an ``instant in time'': we simply
bind the time suffixes ($\Tau$) of all body predicates to a constant.  Because
this produces a sequence of models (one per timestep), it gives us an intuitive
and unambiguous way to declaratively express persistence and state changes
across timesteps.  In this section, we give examples of language constructs
that capture \wrm{state-ricuted????} motifs such as persistent relations,
deletion and update, sequences, and queues.

\subsection{Simple Persistence}

A fact in predicate $p$ at time $\Tau$ may provide ground for deductive rules
at time $\Tau$, as well as ground for deductive rules in timesteps greater than $\Tau$,
provided there exists a {\em simple persistence rule} -- a rule of the form:

\dedalus{p\pos($A_1$,$A_2$,[...],$A_n$)@next $\leftarrow$
p\pos($A_1$,$A_2$,[...],$A_n$);}

%%\rcs{why do we introduce p\pos?  It came from nowhere.  We need a rule p :- p\pos, right?}
%%\begin{definition}
%%A rule of the above form is known as a {\em simple persistence rule}.
%%\end{definition}

A simple persistence rule ensures that a $p$ fact true at time $i$ will be true
$\forall j \in \mathbb{Z} : j >= i$\footnote{It is interesting to note that we can
express this rule using a temporal logic assertion that makes use of the $\Box$
or ``henceforth'' operator:

$\forall A_1, \ldots, A_n \in C : p$\pos$(A_1, \ldots, A_n) \to \Box p$\pos$(A_1,
\ldots, A_n)$.

This assertion states that for any constants $A_1, \ldots, A_n$ in the Herbrand
Universe, $p$\pos$(A_1, \ldots, A_n)$ implies that henceforth, \linebreak
$p$\pos$(A_1, \ldots, A_n)$ will be true.\rcs{defined herbrand here?  this is the first use}}.

%\wrm{is there a point to this formula?  can't we just say ``a simple persistence
%rule naturally encapsulates the ``always'' operator from temporal logic.  maybe
%this should even be a footnote unless we're going to make a bigger deal about
%\lang and temporal logics?}
%\paa{umm, I just thought it was interesting.  feel free to strike it, footnote it, convert 
%it to plain english, etc}
\subsection{Mutable State}
\label{sec:mutable}

To model deletions and updates of a fact, it is necessary to break the induction
in a simple persistence rule.  Adding a {\em p\nega} subgoal to the body of a
simple persistence rule accomplishes this:

\begin{Dedalus}
p_pos(A_1,A_2,[...],A_n)@next \(\leftarrow\)
  p_pos(A_1,A_2,[...],A_n),
  \(\lnot\) p_neg(A_1,A_2,[...],A_n);
\end{Dedalus}

%%\begin{definition}
%
%%A rule of the above form is known as a {\em mutable persistence rule}.
%
%%\end{definition}

If, at any time $k$, we have a fact
\dedalus{p\nega($C_1$,$C_2$,[...],$C_n$)@k}, then we do not deduce a
\dedalus{p\pos($C_1$,$C_2$,[...],$C_n$)@k+1} fact.  By induction, we do not
deduce a \dedalus{p\pos($C_1$,$C_2$,[...],$C_n$)@l} for any $l > k$, unless
this \dedalus{p\pos} fact is re-derived at some timestep $l > k$ by another
rule.  This corresponds to the intuition that a persistent fact, once stated,
is true until it is retracted.  

%%\newtheorem{example}{Example}
\begin{example}
Consider the following \lang program and ground facts:

\begin{Dedalus}
p\pos(A, B) \(\leftarrow\) p(A, B);

p\pos(A, B)@next \(\leftarrow\) p\pos(A, B),\(\lnot\)p\nega(A, B);

p(1,2)@101;
p(1,3)@102;
p\nega(1,2)@300;
\end{Dedalus}

It is easy to see that the following are true: \dedalus{p(1,2)@200},
\dedalus{p(1,3)@200}, \dedalus{p(1,3)@300}.  However, \dedalus{p(1,2)@301} is
false because it was deleted at timestep \dedalus{300}.
\end{example}

Since mutable persistence occurs frequently in practice, we provide the {\em
persist} macro, which takes as arguments a predicate name and its arity, and
expands to the corresponding persistence rule.  For example, the above
\dedalus{p} persistence rule may be equivalently specified as
\dedalus{persist[p, 2]}.

Mutable persistence rules enable {\em updates}.  For some time $\Tau$, an
update is any pair of facts:

\begin{Dedalus}
p\nega(\(C_1\),\(C_2\),[...],\(C_n\))@\(\Tau\) 
p\pos(\(D_1\),\(D_2\),[...],\(D_n\))@\(\Tau+1\)
\end{Dedalus}

Intuitively, an update represents deleting an old value of a tuple and
inserting a new value.  Every update is {\em atomic across timesteps}, meaning
that the old value ceases to exist at the same timestep in which the new value
is derived -- timestep $\Tau+1$ in the above definition.

\wrm{BEGIN WARNING}
\subsection{Sequences}
One may represent a database sequence---an object that retains and monotonically increases a counter value---with a pair of inductive rules.  One rule increments the current counter value when some condition is 
true, while the other persists the value of the sequence when the condition is false.  We can capture the increase
of the sequence value without using arithmetic, because the infinite series of \emph{successor} has the monotonicity
property we require:

\begin{Dedalus}
seq(B)@next \(\leftarrow\) seq(A), successor(A,B), event(_).
  
seq(A)@next \(\leftarrow\) seq(A), \(\lnot\) event(_);
\end{Dedalus}

Note that these two rules produce only a single value of seq at each timestep, but they do so in a manner slightly different than our standard persistence template.


\subsection{Queues}


\jmh{Can't we whittle this down to the bone?  Why are we dealing with user and job attributes?  Why not drop user?}

While sequences are useful constructs for generating or imposing an ordering on tuples, programs will in some cases require that tuples
are processed in a particular order and with a particular punctuation.   To this end, we introduce a queue template, which employs 
inductive persistence and aggregate functions in rule heads to process tuples according to a data-dependent order, rather than as a set.

Aggregate functions simplify our discussion of queues.  Mumick and Shmueli observe correspondences in the expressivity of Datalog with stratified negation and stratified aggregation functions~\cite{mumickshmueli}.  Adding aggregation to our language does not affect its expressive power, but is useful for writing natural constructs for distributed computing including queues and ordering.  

In \lang we will allow
aggregate functions $\rho_1 - \rho_n$ to appear
in the head of a deductive rule of the form:

\dedalus{p(\(A_1\), \(\ldots\), \(A_n\), \(\rho_1\)(\(A_{n+1}\)), \(\ldots\), \(\rho_m\)(\(A_{n+m}\))) \(\leftarrow\)}
\linebreak\dedalus{\(q_1\)(\(A_1\), \(\ldots\), \(A_n\)), \(\ldots\), \(q_n\)(\(A_1\), \(\ldots\), \(A_n\));}

According to this rule, the predicate $p$ contains one row for each satisfying assignment of $A_1, \ldots, A_n$ -- akin to the distinct ``groups'' of SQL's ``GROUP BY'' notation.



Consider a predicate \dedalus{priority\_queue} that represents a series of tasks to be performed in some predefined order.  Its attributes are a string representing a user, a job, and an integer
indicating the priority of the job in the queue:

\begin{Dedalus}
priority\_queue(``bob'', ``bash'', 200)@123;
priority\_queue(``eve'', ``john'', 1)@123;
priority\_queue(``alice'', ``ssh'', 204)@123;
priority\_queue(``bob'', ``ssh'', 205)@123;
\end{Dedalus}

Note that all the time suffixes are the same.  
%Depending on the program that implements the balance update, several behaviors
%are possible.
Given this schema, we note that a program would likely want to process
\dedalus{priority\_queue} events individually in a data-dependent order, in
spite of their coincidence in logical time.  

%%It is difficult to express general
%%in-order tuple processing in Datalog, in part because the language does not
%%admit sequences.  \jmh{Huh?  I don't see the last clause there.  Maybe say simply that Datalog is set-oriented, but what we want here is precisely to impose an ordering on the elements of the set, which seems unnatural.  There's maybe a connection to expressibility and aggregation or arithmetic or something, but let's not try to sort that out for now.}
%above is really what we want to say, right? -wrm
%has so
%notion of order of evaluation (except the implicit ordering implied by
%stratification).

In the program below, we define a table \dedalus{m\_priority\_queue} that
serves as a queue to feed \dedalus{priority\_queue}.  The queue must persist
across timesteps because it may take multiple timesteps to drain it.  At each
timestep, for each value of \textbf{A}, a single tuple is projected into
\dedalus{priority\_queue} and deleted (atomic with the projection) from
\dedalus{m\_priority\_queue}, changing the value of the aggregate calculated
at the subsequent step:

\begin{Dedalus}
% persist m\_priority\_queue
m\_priority\_queue(A, B, C)@next \(\leftarrow\)
  m\_priority\_queue(A, B, C),
  notin del\_m\_priority\_queue(A, B, C);

% find the min priority
omin(A, min<C>) \(\leftarrow\)
  m\_priority\_queue(A, _, C);

% feed p in the next step 
% with the items of min priority
p(A, B, C)@next \(\leftarrow\)
  m\_priority\_queue(A, B, C),
  omin(A, C);

% delete from the next step 
% those items of min priority
del\_m\_priority\_queue(A, B, C) \(\leftarrow\)
  m\_priority\_queue(A, B, C),
  omin(A, C);
\end{Dedalus}

Under such a queueing discipline, deductive rules that depend on
\emph{priority\_queue} are constrained to consider only min-priority tuples at each timestep
per value of the variable \textbf{A}, thus implementing a per-user FIFO
discipline.  To enforce a global FIFO ordering over \emph{priority\_queue}, we
may redefine \emph{omin} and any dependent rules to exclude the \textbf{A}
attribute.

\paa{joe dislikes the below but I like it.  thoughts?}
A queue establishes a mapping between the local clock and the priority-ordering attribute of the input relation. By doing so, we are able to take
advantage of the natural ordering enforced by stratification over time, to enforce an ordering property over our input that is otherwise 
very difficult to express in a logic language.

\wrm{END WARNING}


